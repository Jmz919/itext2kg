{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, iText2KG is compatible with all language models supported by LangChain. \n",
    "\n",
    "To use iText2KG, you will need both a chat model and an embeddings model. \n",
    "\n",
    "For available chat models, refer to the options listed at: https://python.langchain.com/v0.2/docs/integrations/chat/. \n",
    "For embedding models, explore the choices at: https://python.langchain.com/v0.2/docs/integrations/text_embedding/. \n",
    "\n",
    "This notebook will show you how to run iText2KG using Mistral, Ollama, and OpenAI models. \n",
    "\n",
    "**Please ensure that you install the necessary package for each chat model before use.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same applies for OpenAI. \n",
    "\n",
    "please setup your model using the tutorial : https://python.langchain.com/v0.2/docs/integrations/chat/openai/\n",
    "The same for embedding model : https://python.langchain.com/v0.2/docs/integrations/text_embedding/openai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same applies for Ollama. \n",
    "\n",
    "please setup your model using the tutorial : https://python.langchain.com/v0.2/docs/integrations/chat/ollama/\n",
    "The same for embedding model : https://python.langchain.com/v0.2/docs/integrations/text_embedding/openai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iText2KG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use Case: We aim to connect an online job description with a generated CV using Knowledge Graphs. \n",
    "\n",
    "* The objective is to assess the candidate's suitability for the job offer. You can utilize different LLM or embedding models for each module of iText2KG. However, it is important to ensure that the dimensions of node and relation embeddings are consistent across models. If the embedding dimensions differ, cosine similarity may struggle to accurately measure vector distances for further matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 0}, page_content='Published as a conference paper at ICLR 2021\\nBERT OLOGY MEETS BIOLOGY : INTERPRETING\\nATTENTION IN PROTEIN LANGUAGE MODELS\\nJesse Vig1Ali Madani1Lav R. Varshney1,2Caiming Xiong1\\nRichard Socher1Nazneen Fatema Rajani1\\n1Salesforce Research,2University of Illinois at Urbana-Champaign\\n{jvig,amadani,cxiong,rsocher,nazneen.rajani}@salesforce.com\\nvarshney@illinois.edu\\nABSTRACT\\nTransformer architectures have proven to learn useful representations for pro-\\ntein classiﬁcation and generation tasks. However, these representations present\\nchallenges in interpretability. In this work, we demonstrate a set of methods for\\nanalyzing protein Transformer models through the lens of attention. We show that\\nattention: (1) captures the folding structure of proteins, connecting amino acids that\\nare far apart in the underlying sequence, but spatially close in the three-dimensional\\nstructure, (2) targets binding sites, a key functional component of proteins, and\\n(3) focuses on progressively more complex biophysical properties with increas-\\ning layer depth. We ﬁnd this behavior to be consistent across three Transformer\\narchitectures (BERT, ALBERT, XLNet) and two distinct protein datasets. We\\nalso present a three-dimensional visualization of the interaction between atten-\\ntion and protein structure. Code for visualization and analysis is available at\\nhttps://github.com/salesforce/provis .\\n1 I NTRODUCTION\\nThe study of proteins, the fundamental macromolecules governing biology and life itself, has led to\\nremarkable advances in understanding human health and the development of disease therapies. The\\ndecreasing cost of sequencing technology has enabled vast databases of naturally occurring proteins\\n(El-Gebali et al., 2019a), which are rich in information for developing powerful machine learning\\nmodels of protein sequences. For example, sequence models leveraging principles of co-evolution,\\nwhether modeling pairwise or higher-order interactions, have enabled prediction of structure or\\nfunction (Rollins et al., 2019).\\nProteins, as a sequence of amino acids, can be viewed precisely as a language and therefore modeled\\nusing neural architectures developed for natural language. In particular, the Transformer (Vaswani\\net al., 2017), which has revolutionized unsupervised learning for text, shows promise for similar\\nimpact on protein sequence modeling. However, the strong performance of the Transformer comes\\nat the cost of interpretability, and this lack of transparency can hide underlying problems such as\\nmodel bias and spurious correlations (Niven & Kao, 2019; Tan & Celis, 2019; Kurita et al., 2019). In\\nresponse, much NLP research now focuses on interpreting the Transformer, e.g., the subspecialty of\\n“BERTology” (Rogers et al., 2020), which speciﬁcally studies the BERT model (Devlin et al., 2019).\\nIn this work, we adapt and extend this line of interpretability research to protein sequences. We\\nanalyze Transformer protein models through the lens of attention, and present a set of interpretability\\nmethods that capture the unique functional and structural characteristics of proteins. We also compare\\nthe knowledge encoded in attention weights to that captured by hidden-state representations. Finally,\\nwe present a visualization of attention contextualized within three-dimensional protein structure.\\nOur analysis reveals that attention captures high-level structural properties of proteins, connecting\\namino acids that are spatially close in three-dimensional structure, but apart in the underlying sequence\\n(Figure 1a). We also ﬁnd that attention targets binding sites, a key functional component of proteins\\n(Figure 1b). Further, we show how attention is consistent with a classic measure of similarity between\\namino acids—the substitution matrix. Finally, we demonstrate that attention captures progressively\\nhigher-level representations of structure and function with increasing layer depth.\\n1arXiv:2006.15222v3  [cs.CL]  28 Mar 2021'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 1}, page_content='Published as a conference paper at ICLR 2021\\n(a) Attention in head 12-4, which targets amino\\nacid pairs that are close in physical space (see\\ninset subsequence 117D-157I) but lie apart in the\\nsequence. Example is a de novo designed TIM-\\nbarrel (5BVL) with characteristic symmetry.\\n(b) Attention in head 7-1, which targets binding\\nsites, a key functional component of proteins.\\nExample is HIV-1 protease (7HVP). The primary\\nlocation receiving attention is 27G, a binding site\\nfor protease inhibitor small-molecule drugs.\\nFigure 1: Examples of how specialized attention heads in a Transformer recover protein structure and\\nfunction, based solely on language model pre-training. Orange lines depict attention between amino\\nacids (line width proportional to attention weight; values below 0.1 hidden). Heads were selected\\nbased on correlation with ground-truth annotations of contact maps and binding sites. Visualizations\\nbased on the NGL Viewer (Rose et al., 2018; Rose & Hildebrand, 2015; Nguyen et al., 2017).\\nIn contrast to NLP, which aims to automate a capability that humans already have—understanding\\nnatural language—protein modeling also seeks to shed light on biological processes that are not fully\\nunderstood. Thus we also discuss how interpretability can aid scientiﬁc discovery.\\n2 B ACKGROUND : PROTEINS\\nIn this section we provide background on the biological concepts discussed in later sections.\\nAmino acids. Just as language is composed of words from a shared lexicon, every protein sequence\\nis formed from a vocabulary of amino acids, of which 20 are commonly observed. Amino acids may\\nbe denoted by their full name (e.g., Proline ), a 3-letter abbreviation ( Pro), or a single-letter code ( P).\\nSubstitution matrix. While word synonyms are encoded in a thesaurus, proteins that are similar in\\nstructure or function are captured in a substitution matrix , which scores pairs of amino acids on how\\nreadily they may be substituted for one another while maintaining protein viability. One common\\nsubstitution matrix is BLOSUM (Henikoff & Henikoff, 1992), which is derived from co-occurrence\\nstatistics of amino acids in aligned protein sequences.\\nProtein structure. Though a protein may be abstracted as a sequence of amino acids, it represents\\na physical entity with a well-deﬁned three-dimensional structure (Figure 1). Secondary structure\\ndescribes the local segments of proteins; two commonly observed types are the alpha helix andbeta\\nsheet .Tertiary structure encompasses the large-scale formations that determine the overall shape\\nand function of the protein. One way to characterize tertiary structure is by a contact map , which\\ndescribes the pairs of amino acids that are in contact (within 8 angstroms of one another) in the folded\\nprotein structure but lie apart (by at least 6 positions) in the underlying sequence (Rao et al., 2019).\\nBinding sites. Proteins may also be characterized by their functional properties. Binding sites are\\nprotein regions that bind with other molecules (proteins, natural ligands, and small-molecule drugs)\\nto carry out a speciﬁc function. For example, the HIV-1 protease is an enzyme responsible for a\\ncritical process in replication of HIV (Brik & Wong, 2003). It has a binding site, shown in Figure 1b,\\nthat is a target for drug development to ensure inhibition.\\nPost-translational modiﬁcations. After a protein is translated from RNA, it may undergo additional\\nmodiﬁcations, e.g. phosphorylation, which play a key role in protein structure and function.\\n2'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 2}, page_content='Published as a conference paper at ICLR 2021\\n3 M ETHODOLOGY\\nModel. We demonstrate our interpretability methods on ﬁve Transformer models that were pretrained\\nthrough language modeling of amino acid sequences. We primarily focus on the BERT-Base model\\nfrom TAPE (Rao et al., 2019), which was pretrained on Pfam, a dataset of 31M protein sequences (El-\\nGebali et al., 2019b). We refer to this model as TapeBert . We also analyze 4 pre-trained Transformer\\nmodels from ProtTrans (Elnaggar et al., 2020): ProtBert andProtBert-BFD , which are 30-layer,\\n16-head BERT models; ProtAlbert , a 12-layer, 64-head ALBERT (Lan et al., 2020) model; and\\nProtXLNet , a 30-layer, 16-head XLNet (Yang et al., 2019) model. ProtBert-BFD was pretrained on\\nBFD (Steinegger & Söding, 2018), a dataset of 2.1B protein sequences, while the other ProtTrans\\nmodels were pretrained on UniRef100 (Suzek et al., 2014), which includes 216M protein sequences.\\nA summary of these 5 models is presented in Appendix A.1.\\nHere we present an overview of BERT, with additional details on all models in Appendix A.2. BERT\\ninputs a sequence of amino acids x= (x1,...,xn)and applies a series of encoders. Each encoder\\nlayerℓoutputs a sequence of continuous embeddings (h(ℓ)\\n1,...,h(ℓ)\\nn)using a multi-headed attention\\nmechanism. Each attention head in a layer produces a set of attention weights αfor an input, where\\nαi,j> 0 is the attention from token ito tokenj, such that∑\\njαi,j= 1. Intuitively, attention weights\\ndeﬁne the inﬂuence of every token on the next layer’s representation for the current token. We denote\\na particular head by <layer>-<head_index> , e.g. head 3-7for the 3rd layer’s 7th head.\\nAttention analysis. We analyze how attention aligns with various protein properties. For properties\\nof token pairs, e.g. contact maps, we deﬁne an indicator function f(i,j)that returns 1 if the property\\nis present in token pair (i,j)(e.g., if amino acids iandjare in contact), and 0 otherwise. We\\nthen compute the proportion of high-attention token pairs ( αi,j>θ) where the property is present,\\naggregated over a dataset X:\\npα(f) =∑\\nx∈X|x|∑\\ni=1|x|∑\\nj=1f(i,j)· 1αi,j>θ/∑\\nx∈X|x|∑\\ni=1|x|∑\\nj=11αi,j>θ (1)\\nwhereθis a threshold to select for high-conﬁdence attention weights. We also present an alternative,\\ncontinuous version of this metric in Appendix B.1.\\nFor properties of individual tokens , e.g. binding sites, we deﬁne f(i,j)to return 1 if the property is\\npresent in token j(e.g. ifjis a binding site). In this case, pα(f)equals the proportion of attention\\nthat is directed tothe property (e.g. the proportion of attention focused on binding sites).\\nWhen applying these metrics, we include two types of checks to ensure that the results are not\\ndue to chance. First, we test that the proportion of attention that aligns with particular properties\\nis signiﬁcantly higher than the background frequency of these properties, taking into account the\\nBonferroni correction for multiple hypotheses corresponding to multiple attention heads. Second,\\nwe compare the results to a null model, which is an instance of the model with randomly shufﬂed\\nattention weights. We describe these methods in detail in Appendix B.2.\\nProbing tasks. We also perform probing tasks on the model, which test the knowledge contained\\nin model representations by using them as inputs to a classiﬁer that predicts a property of interest\\n(Veldhoen et al., 2016; Conneau et al., 2018; Adi et al., 2016). The performance of the probing\\nclassiﬁer serves as a measure of the knowledge of the property that is encoded in the representation.\\nWe run both embedding probes , which assess the knowledge encoded in the output embeddings of\\neach layer, and attention probes (Reif et al., 2019; Clark et al., 2019), which measure the knowledge\\ncontained in the attention weights for pairwise features. Details are provided in Appendix B.3.\\nDatasets. For our analyses of amino acids and contact maps, we use a curated dataset from TAPE'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 2}, page_content='contained in the attention weights for pairwise features. Details are provided in Appendix B.3.\\nDatasets. For our analyses of amino acids and contact maps, we use a curated dataset from TAPE\\nbased on ProteinNet (AlQuraishi, 2019; Fox et al., 2013; Berman et al., 2000; Moult et al., 2018),\\nwhich contains amino acid sequences annotated with spatial coordinates (used for the contact map\\nanalysis). For the analysis of secondary structure and binding sites we use the Secondary Structure\\ndataset (Rao et al., 2019; Berman et al., 2000; Moult et al., 2018; Klausen et al., 2019) from TAPE.\\nWe employed a taxonomy of secondary structure with three categories: Helix ,Strand , and Turn/Bend ,\\nwith the last two belonging to the higher-level beta sheet category (Sec. 2). We used this taxonomy to\\nstudy how the model understood structurally distinct regions of beta sheets. We obtained token-level\\nbinding site and protein modiﬁcation labels from the Protein Data Bank (Berman et al., 2000).\\nFor analyzing attention, we used a random subset of 5000 sequences from the training split of the\\n3'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 3}, page_content='Published as a conference paper at ICLR 2021\\n24681012\\nHead24681012Layer% Attention\\n025%Max\\n0%10%20%30%40%\\n(a) TapeBert\\n246810121416182022242628303234363840424446485052545658606264\\nHead24681012Layer% Attention\\n0 50%Max\\n0%15%30%45% (b) ProtAlbert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n0 50%Max\\n0%15%30%45%\\n(c) ProtBert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n0 50%Max\\n0%15%30%45%60% (d) ProtBert-BFD\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n025%Max\\n0%10%20%30%40% (e) ProtXLNet\\nFigure 2: Agreement between attention and contact maps across ﬁve pretrained Transformer models\\nfrom TAPE (a) and ProtTrans (b–e). The heatmaps show the proportion of high-conﬁdence attention\\nweights (αi,j>θ) from each head that connects pairs of amino acids that are in contact with one\\nanother. In TapeBert (a), for example, we can see that 45% of attention in head 12-4 (the 12th layer’s\\n4th head) maps to contacts. The bar plots show the maximum value from each layer. Note that the\\nvertical striping in ProtAlbert (b) is likely due to cross-layer parameter sharing (see Appendix A.3).\\nrespective datasets (note that none of the aforementioned annotations were used in model training).\\nFor the diagnostic classiﬁer, we used the respective training splits for training and the validation splits\\nfor evaluation. See Appendix B.4 for additional details.\\nExperimental details We exclude attention to the [SEP] delimiter token, as it has been shown to\\nbe a “no-op” attention token (Clark et al., 2019), as well as attention to the [CLS] token, which is\\nnot explicitly used in language modeling. We only include results for attention heads where at least\\n100 high-conﬁdence attention arcs are available for analysis. We set the attention threshold θto 0.3\\nto select for high-conﬁdence attention while retaining sufﬁcient data for analysis. We truncate all\\nprotein sequences to a length of 512 to reduce memory requirements.1\\nWe note that all of the above analyses are purely associative and do not attempt to establish a causal\\nlink between attention and model behavior (Vig et al., 2020; Grimsley et al., 2020), nor to explain\\nmodel predictions (Jain & Wallace, 2019; Wiegreffe & Pinter, 2019).\\n4 W HAT DOES ATTENTION UNDERSTAND ABOUT PROTEINS ?\\n4.1 P ROTEIN STRUCTURE\\nHere we explore the relationship between attention and tertiary structure, as characterized by contact\\nmaps (see Section 2). Secondary structure results are included in Appendix C.1.\\nAttention aligns strongly with contact maps in the deepest layers. Figure 2 shows how attention\\naligns with contact maps across the heads of the ﬁve models evaluated2, based on the metric deﬁned in\\nEquation 1. The most aligned heads are found in the deepest layers and focus up to 44.7% (TapeBert),\\n55.7% (ProtAlbert), 58.5% (ProtBert), 63.2% (ProtBert-BFD), and 44.5% (ProtXLNet) of attention\\non contacts, whereas the background frequency of contacts among all amino acid pairs in the dataset\\nis 1.3%. Figure 1a shows an example of the induced attention from the top head in TapeBert. We note\\nthat the model with the single most aligned head—ProtBert-BFD—is the largest model (same size as\\nProteinBert) at 420M parameters (Appendix A.1) and it was also the only model pre-trained on the\\n194% of sequences had length less than 512. Experiments performed on single 16GB Tesla V-100 GPU.\\n2Heads with fewer than 100 high-conﬁdence attention weights across the dataset are grayed out.\\n4'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 4}, page_content='Published as a conference paper at ICLR 2021\\n24681012\\nHead24681012Layer% Attention\\n0 50%Max\\n10%20%30%40%\\n(a) TapeBert\\n246810121416182022242628303234363840424446485052545658606264\\nHead24681012Layer% Attention\\n0 50%Max\\n0%15%30%45% (b) ProtAlbert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n0 50%Max\\n10%20%30%40%50%\\n(c) ProtBert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n025%Max\\n10%20%30%40% (d) ProtBert-BFD\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n010%Max\\n0%4%8%12% (e) ProtXLNet\\nFigure 3: Proportion of attention focused on binding sites across ﬁve pretrained models. The heatmaps\\nshow the proportion of high-conﬁdence attention ( αi,j>θ) from each head that is directed to binding\\nsites. In TapeBert (a), for example, we can see that 49% of attention in head 11-6 (the 11th layer’s\\n6th head) is directed to binding sites. The bar plots show the maximum value from each layer.\\nlargest dataset, BFD. It’s possible that both factors helped the model learn more structurally-aligned\\nattention patterns. Statistical signiﬁcance tests and null models are reported in Appendix C.2.\\nConsidering the models were trained on language modeling tasks without any spatial information,\\nthe presence of these structurally-aware attention heads is intriguing. One possible reason for this\\nemergent behavior is that contacts are more likely to biochemically interact with one another, creating\\nstatistical dependencies between the amino acids in contact. By focusing attention on the contacts of\\na masked position, the language models may acquire valuable context for token prediction.\\nWhile there seems to be a strong correlation between the attention head output and classically-deﬁned\\ncontacts, there are also differences. The models may have learned differing contextualized or nuanced\\nformulations that describe amino acid interactions. These learned interactions could then be used for\\nfurther discovery and investigation or repurposed for prediction tasks similar to how principles of\\ncoevolution enabled a powerful representation for structure prediction.\\n4.2 B INDING SITES AND POST-TRANSLATIONAL MODIFICATIONS\\nWe also analyze how attention interacts with binding sites and post-translational modiﬁcations\\n(PTMs), which both play a key role in protein function.\\nAttention targets binding sites throughout most layers of the models. Figure 3 shows the propor-\\ntion of attention focused on binding sites (Eq. 1) across the heads of the 5 models studied. Attention\\nto binding sites is most pronounced in the ProtAlbert model (Figure 3b), which has 22 heads that\\nfocus over 50% of attention on bindings sites, whereas the background frequency of binding sites in\\nthe dataset is 4.8%. The three BERT models (Figures 3a, 3c, and 3d) also attend strongly to binding\\nsites, with attention heads focusing up to 48.2%, 50.7%, and 45.6% of attention on binding sites,\\nrespectively. Figure 1b visualizes the attention in one strongly-aligned head from the TapeBert model.\\nStatistical signiﬁcance tests and a comparison to a null model are provided in Appendix C.3.\\nProtXLNet (Figure 3e) also targets binding sites, but not as strongly as the other models: the most\\naligned head focuses 15.1% of attention on binding sites, and the average head directs just 6.2% of\\nattention to binding sites, compared to 13.2%, 19.8%, 16.0%, and 15.1% for the ﬁrst four models\\nin Figure 3. It’s unclear whether this disparity is due to differences in architectures or pre-training\\nobjectives; for example, ProtXLNet uses a bidirectional auto-regressive pretraining method (see\\nAppendix A.2), whereas the other 4 models all use masked language modeling objectives.\\n5'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 5}, page_content='Published as a conference paper at ICLR 2021\\n0%10%20%30%Helix\\n0%10%20%Turn/Bend\\n0%10%20%Strand\\n0%10%20%Binding Site\\n123456789101112\\nLayer0%5%10%Contact\\nFigure 4: Each plot shows the percentage of\\nattention focused on the given property, av-\\neraged over all heads within each layer. The\\nplots, sorted by center of gravity (red dashed\\nline), show that heads in deeper layers focus\\nrelatively more attention on binding sites and\\ncontacts, whereas attention toward speciﬁc\\nsecondary structures is more even across lay-\\ners.\\n0.60.7Helix\\n0.10.2Turn/Bend\\n0.20.40.6Strand\\n0.100.120.140.16Binding Site\\n123456789101112\\nLayer0.050.100.15ContactEmbedding probe\\nAttention probeFigure 5: Performance of probing classiﬁers\\nby layer, sorted by task order in Figure 4.\\nThe embedding probes (orange) quantify the\\nknowledge of the given property that is en-\\ncoded in each layer’s output embeddings. The\\nattention probe (blue), show the amount of in-\\nformation encoded in attention weights for the\\n(pairwise) contact feature. Additional details\\nare provided in Appendix B.3.\\nWhy does attention target binding sites? In contrast to contact maps, which reveal relationships\\nwithin proteins, binding sites describe how a protein interacts with other molecules. These external\\ninteractions ultimately deﬁne the high-level function of the protein, and thus binding sites remain\\nconserved even when the sequence as a whole evolves (Kinjo & Nakamura, 2009). Further, structural\\nmotifs in binding sites are mainly restricted to speciﬁc families or superfamilies of proteins (Kinjo &\\nNakamura, 2009), and binding sites can reveal evolutionary relationships among proteins (Lee et al.,\\n2017). Thus binding sites may provide the model with a high-level characterization of the protein\\nthat is robust to individual sequence variation. By attending to these regions, the model can leverage\\nthis higher-level context when predicting masked tokens throughout the sequence.\\nAttention targets PTMs in a small number of heads. A small number of heads in each model con-\\ncentrate their attention very strongly on amino acids associated with post-translational modiﬁcations\\n(PTMs). For example, Head 11-6 in TapeBert focused 64% of attention on PTM positions, though\\nthese occur at only 0.8% of sequence positions in the dataset.3Similar to our discussion on binding\\nsites, PTMs are critical to protein function (Rubin & Rosen, 1975) and thereby are likely to exhibit\\nbehavior that is conserved across the sequence space. See Appendix C.4 for full results.\\n4.3 C ROSS -LAYER ANALYSIS\\nWe analyze how attention captures properties of varying complexity across different layers of\\nTapeBert, and compare this to a probing analysis of embeddings and attention weights (see Section 3).\\nAttention targets higher-level properties in deeper layers. As shown in Figure 4, deeper layers\\nfocus relatively more attention on binding sites and contacts (high-level concept), whereas secondary\\nstructure (low- to mid-level concept) is targeted more evenly across layers. The probing analysis\\nof attention (Figure 5, blue) similarly shows that knowledge of contact maps (a pairwise feature)\\n3This head also targets binding sites (Fig. 3a) but at a percentage of 49%.\\n6'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 6}, page_content='Published as a conference paper at ICLR 2021\\n24681012\\nHead24681012Layer% Attention\\n0 100%Max\\n0%20%40%60%80%\\n24681012\\nHead24681012Layer% Attention\\n0 20%Max\\n0%5%10%15%20%\\nFigure 6: Percentage of each head’s attention focused on amino acids Pro(left) and Phe(right).\\nACDEFGHIKLMNPQRSTVWYA\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nK\\nL\\nM\\nN\\nP\\nQ\\nR\\nS\\nT\\nV\\nW\\nY0.4\\n0.2\\n0.00.20.40.60.8\\nACDEFGHIKLMNPQRSTVWYA\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nK\\nL\\nM\\nN\\nP\\nQ\\nR\\nS\\nT\\nV\\nW\\nY4\\n3\\n2\\n1\\n01234\\nFigure 7: Pairwise attention similarity (left) vs. substitution matrix (right) (codes in App. C.5)\\nis encoded in attention weights primarily in the last 1-2 layers. These results are consistent with\\nprior work in NLP that suggests deeper layers in text-based Transformers attend to more complex\\nproperties (Vig & Belinkov, 2019) and encode higher-level representations (Raganato & Tiedemann,\\n2018; Peters et al., 2018; Tenney et al., 2019; Jawahar et al., 2019).\\nTheembedding probes (Figure 5, orange) also show that the model ﬁrst builds representations of\\nlocal secondary structure in lower layers before fully encoding binding sites and contact maps in\\ndeeper layers. However, this analysis also reveals stark differences in how knowledge of contact maps\\nis accrued in embeddings, which accumulate this knowledge gradually over many layers, compared\\nto attention weights, which acquire this knowledge only in the ﬁnal layers in this case. This example\\npoints out limitations of common layerwise probing approaches that only consider embeddings,\\nwhich, intuitively, represent what the model knows but not necessarily how it operationalizes that\\nknowledge.\\n4.4 A MINO ACIDS AND THE SUBSTITUTION MATRIX\\nIn addition to high-level structural and functional properties, we also performed a ﬁne-grained\\nanalysis of the interaction between attention and particular amino acids.\\nAttention heads specialize in particular amino acids. We computed the proportion of TapeBert’s\\nattention to each of the 20 standard amino acids, as shown in Figure 6 for two example amino acids.\\nFor 16 of the amino acids, there exists an attention head that focuses over 25% of attention on\\nthat amino acid, signiﬁcantly greater than the background frequencies of the corresponding amino\\nacids, which range from 1.3% to 9.4%. Similar behavior was observed for ProtBert, ProtBert-BFD,\\nProtAlbert, and ProtXLNet models, with 17, 15, 16, and 18 amino acids, respectively, receiving\\ngreater than 25% of the attention from at least one attention head. Detailed results for TapeBert\\nincluding statistical signiﬁcance tests and comparison to a null model are presented in Appendix C.5.\\nAttention is consistent with substitution relationships. A natural follow-up question from the\\nabove analysis is whether each head has “memorized” speciﬁc amino acids to target, or whether it\\nhas actually learned meaningful properties that correlate with particular amino acids. To test the\\nlatter hypothesis, we analyze whether amino acids with similar structural and functional properties\\nare attended to similarly across heads. Speciﬁcally, we compute the Pearson correlation between the\\ndistribution of attention across heads between all pairs of distinct amino acids, as shown in Figure 7\\n(left) for TapeBert. For example, the entry for Pro(P) and Phe(F) is the correlation between the\\ntwo heatmaps in Figure 6. We compare these scores to the BLOSUM62 substitution scores (Sec. 2)\\nin Figure 7 (right), and ﬁnd a Pearson correlation of 0.73, suggesting that attention is moderately\\n7'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 7}, page_content='Published as a conference paper at ICLR 2021\\nconsistent with substitution relationships. Similar correlations are observed for the ProtTrans models:\\n0.68 (ProtBert), 0.75 (ProtBert-BFD), 0.60 (ProtAlbert), and 0.71 (ProtXLNet). As a baseline, the\\nrandomized versions of these models (Appendix B.2) yielded correlations of -0.02 (TapeBert), 0.02\\n(ProtBert), -0.03 (ProtBert-BFD), -0.05 (ProtAlbert), and 0.21 (ProtXLNet).\\n5 R ELATED WORK\\n5.1 P ROTEIN LANGUAGE MODELS\\nDeep neural networks for protein language modeling have received broad interest. Early work\\napplied the Skip-gram model (Mikolov et al., 2013) to construct continuous embeddings from protein\\nsequences (Asgari & Mofrad, 2015). Sequence-only language models have since been trained through\\nautoregressive or autoencoding self-supervision objectives for discriminative and generative tasks,\\nfor example, using LSTMs or Transformer-based architectures (Alley et al., 2019; Bepler & Berger,\\n2019; Rao et al., 2019; Rives et al., 2019). TAPE created a benchmark of ﬁve tasks to assess protein\\nsequence models, and ProtTrans also released several large-scale pretrained protein Transformer\\nmodels (Elnaggar et al., 2020). Riesselman et al. (2019); Madani et al. (2020) trained autoregressive\\ngenerative models to predict the functional effect of mutations and generate natural-like proteins.\\nFrom an interpretability perspective, Rives et al. (2019) showed that the output embeddings from\\na pretrained Transformer can recapitulate structural and functional properties of proteins through\\nlearned linear transformations. Various works have analyzed output embeddings of protein models\\nthrough dimensionality reduction techniques such as PCA or t-SNE (Elnaggar et al., 2020; Biswas\\net al., 2020). In our work, we take an interpretability-ﬁrst perspective to focus on the internal model\\nrepresentations, speciﬁcally attention and intermediate hidden states, across multiple protein language\\nmodels. We also explore novel biological properties including binding sites and post-translational\\nmodiﬁcations.\\n5.2 I NTERPRETING MODELS IN NLP\\nThe rise of deep neural networks in ML has also led to much work on interpreting these so-called\\nblack-box models. This section reviews the NLP interpretability literature on the Transformer model,\\nwhich is directly comparable to our work on interpreting Transformer models of protein sequences.\\nInterpreting Transformers. The Transformer is a neural architecture that uses attention to ac-\\ncelerate learning (Vaswani et al., 2017). In NLP, transformers are the backbone of state-of-the-art\\npre-trained language models such as BERT (Devlin et al., 2019). BERTology focuses on interpreting\\nwhat the BERT model learns about language using a suite of probes and interventions (Rogers et al.,\\n2020). So-called diagnostic classiﬁers are used to interpret the outputs from BERT’s layers (Veldhoen\\net al., 2016). At a high level, mechanisms for interpreting BERT can be placed into three main\\ncategories: interpreting the learned embeddings (Ethayarajh, 2019; Wiedemann et al., 2019; Mickus\\net al., 2020; Adi et al., 2016; Conneau et al., 2018), BERT’s learned knowledge of syntax (Lin et al.,\\n2019; Liu et al., 2019; Tenney et al., 2019; Htut et al., 2019; Hewitt & Manning, 2019; Goldberg,\\n2019), and BERT’s learned knowledge of semantics (Tenney et al., 2019; Ettinger, 2020).\\nInterpreting attention speciﬁcally. Interpreting attention on textual sequences is a well-\\nestablished area of research (Wiegreffe & Pinter, 2019; Zhong et al., 2019; Brunner et al., 2020;\\nHewitt & Manning, 2019). Past work has been shown that attention correlates with syntactic and\\nsemantic relationships in natural language in some cases (Clark et al., 2019; Vig & Belinkov, 2019;\\nHtut et al., 2019). Depending on the task and model architecture, attention may have less or more\\nexplanatory power for model predictions (Jain & Wallace, 2019; Serrano & Smith, 2019; Pruthi et al.,'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 7}, page_content='Htut et al., 2019). Depending on the task and model architecture, attention may have less or more\\nexplanatory power for model predictions (Jain & Wallace, 2019; Serrano & Smith, 2019; Pruthi et al.,\\n2020; Moradi et al., 2019; Vashishth et al., 2019). Visualization techniques have been used to convey\\nthe structure and properties of attention in Transformers (Vaswani et al., 2017; Kovaleva et al., 2019;\\nHoover et al., 2020; Vig, 2019). Recent work has begun to analyze attention in Transformer models\\noutside of the domain of natural language (Schwaller et al., 2020; Payne et al., 2020).\\nOur work extends these methods to protein sequence models by considering particular biophysical\\nproperties and relationships. We also present a joint cross-layer probing analysis of attention weights\\nand layer embeddings. While past work in NLP has analyzed attention and embeddings across layers,\\nwe believe we are the ﬁrst to do so in any domain using a single, uniﬁed metric, which enables us to\\n8'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 8}, page_content='Published as a conference paper at ICLR 2021\\ndirectly compare the relative information content of the two representations. Finally, we present a\\nnovel tool for visualizing attention embedded in three-dimensional structure.\\n6 C ONCLUSIONS AND FUTURE WORK\\nThis paper builds on the synergy between NLP and computational biology by adapting and extending\\nNLP interpretability methods to protein sequence modeling. We show how a Transformer language\\nmodel recovers structural and functional properties of proteins and integrates this knowledge directly\\ninto its attention mechanism. While this paper focuses on reconciling attention with known properties\\nof proteins, one might also leverage attention to uncover novel relationships or more nuanced forms\\nof existing measures such as contact maps, as discussed in Section 4.1. In this way, language models\\nhave the potential to serve as tools for scientiﬁc discovery. But in order for learned representations\\nto be accessible to domain experts, they must be presented in an appropriate context to facilitate\\ndiscovery. Visualizing attention in the context of protein structure (Figure 1) is one attempt to do so.\\nWe believe there is the potential to develop such contextual visualizations of learned representations\\nin a range of scientiﬁc domains.\\nACKNOWLEDGMENTS\\nWe would like to thank Xi Victoria Lin, Stephan Zheng, Melvin Gruesbeck, and the anonymous\\nreviewers for their valuable feedback.\\nREFERENCES\\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained analysis\\nof sentence embeddings using auxiliary prediction tasks. arXiv:1608.04207 [cs.CL]., 2016.\\nEthan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church.\\nUniﬁed rational protein engineering with sequence-based deep representation learning. Nature\\nMethods , 16(12):1315–1322, 2019.\\nMohammed AlQuraishi. ProteinNet: a standardized data set for machine learning of protein structure.\\nBMC Bioinformatics , 20, 2019.\\nEhsaneddin Asgari and Mohammad RK Mofrad. Continuous distributed representation of biological\\nsequences for deep proteomics and genomics. PLOS One , 10(11), 2015.\\nTristan Bepler and Bonnie Berger. Learning protein sequence embeddings using information from\\nstructure. In International Conference on Learning Representations , 2019.\\nHelen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge Weissig,\\nIlya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic Acids Research , 28(1):\\n235–242, 2000.\\nSurojit Biswas, Grigory Khimulya, Ethan C. Alley, Kevin M. Esvelt, and George M. Church.\\nLow-n protein engineering with data-efﬁcient deep learning. bioRxiv , 2020. doi: 10.1101/\\n2020.01.23.917682. URL https://www.biorxiv.org/content/early/2020/08/\\n31/2020.01.23.917682 .\\nAshraf Brik and Chi-Huey Wong. HIV-1 protease: Mechanism and drug discovery. Organic &\\nBiomolecular Chemistry , 1(1):5–14, 2003.\\nGino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Watten-\\nhofer. On identiﬁability in Transformers. In International Conference on Learning Representations ,\\n2020. URL https://openreview.net/forum?id=BJg1f6EFDB .\\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look\\nat? An analysis of BERT’s attention. In BlackBoxNLP@ACL , 2019.\\n9'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 9}, page_content='Published as a conference paper at ICLR 2021\\nAlexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. What\\nyou can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties.\\nInProceedings of the 56th Annual Meeting of the Association for Computational Linguistics , pp.\\n2126–2136, 2018.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) , pp. 4171–4186, Minneapolis, Minnesota, 2019.\\nAssociation for Computational Linguistics.\\nSara El-Gebali, Jaina Mistry, Alex Bateman, Sean R. Eddy, Aurélien Luciani, Simon C. Potter,\\nMatloob Qureshi, Lorna J. Richardson, Gustavo A. Salazar, Alfredo Smart, Erik L. L. Sonnhammer,\\nLayla Hirsh, Lisanna Paladin, Damiano Piovesan, Silvio C. E. Tosatto, and Robert D. Finn. The\\nPfam protein families database in 2019. Nucleic Acids Research , 47(D1):D427–D432, January\\n2019a. doi: 10.1093/nar/gky995.\\nSara El-Gebali, Jaina Mistry, Alex Bateman, Sean R Eddy, Aurélien Luciani, Simon C Potter, Matloob\\nQureshi, Lorna J Richardson, Gustavo A Salazar, Alfredo Smart, Erik L L Sonnhammer, Layla\\nHirsh, Lisanna Paladin, Damiano Piovesan, Silvio C E Tosatto, and Robert D Finn. The Pfam\\nprotein families database in 2019. Nucleic Acids Research , 47(D1):D427–D432, 2019b. ISSN\\n0305-1048. doi: 10.1093/nar/gky995.\\nAhmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom\\nGibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and Burkhard\\nRost. ProtTrans: Towards cracking the language of life’s code through self-supervised deep\\nlearning and high performance computing. arXiv preprint arXiv:2007.06225 , 2020.\\nKawin Ethayarajh. How contextual are contextualized word representations? Comparing the geometry\\nof BERT, ELMo, and GPT-2 embeddings. In Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP) , pp. 55–65, Hong Kong, China, 2019. Association for\\nComputational Linguistics.\\nAllyson Ettinger. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for\\nlanguage models. Transactions of the Association for Computational Linguistics , 8:34–48, 2020.\\nNaomi K Fox, Steven E Brenner, and John-Marc Chandonia. SCOPe: Structural classiﬁcation of\\nproteins—extended, integrating scop and astral data and classiﬁcation of new structures. Nucleic\\nAcids Research , 42(D1):D304–D309, 2013.\\nYoav Goldberg. Assessing BERT’s syntactic abilities. arXiv preprint arXiv:1901.05287 , 2019.\\nChristopher Grimsley, Elijah Mayﬁeld, and Julia R.S. Bursten. Why attention is not explanation:\\nSurgical intervention and causal reasoning about neural models. In Proceedings of The 12th\\nLanguage Resources and Evaluation Conference , pp. 1780–1790, Marseille, France, May 2020.\\nEuropean Language Resources Association. ISBN 979-10-95546-34-4. URL https://www.\\naclweb.org/anthology/2020.lrec-1.220 .\\nS Henikoff and J G Henikoff. Amino acid substitution matrices from protein blocks. Proceedings of\\nthe National Academy of Sciences , 89(22):10915–10919, 1992. ISSN 0027-8424. doi: 10.1073/\\npnas.89.22.10915. URL https://www.pnas.org/content/89/22/10915 .\\nJohn Hewitt and Christopher D Manning. A structural probe for ﬁnding syntax in word representations.\\nInProceedings of the 2019 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) ,\\npp. 4129–4138, 2019.\\nBenjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exBERT: A Visual Analysis Tool\\nto Explore Learned Representations in Transformer Models. In Proceedings of the 58th Annual'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 9}, page_content='Benjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exBERT: A Visual Analysis Tool\\nto Explore Learned Representations in Transformer Models. In Proceedings of the 58th Annual\\nMeeting of the Association for Computational Linguistics: System Demonstrations , pp. 187–196.\\nAssociation for Computational Linguistics, 2020.\\n10'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 10}, page_content='Published as a conference paper at ICLR 2021\\nPhu Mon Htut, Jason Phang, Shikha Bordia, and Samuel R Bowman. Do attention heads in BERT\\ntrack syntactic dependencies? arXiv preprint arXiv:1911.12246 , 2019.\\nSarthak Jain and Byron C. Wallace. Attention is not Explanation. In Proceedings of the 2019\\nConference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long and Short Papers) , pp. 3543–3556, June 2019.\\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah. What does BERT learn about the structure of\\nlanguage? In ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics ,\\nFlorence, Italy, July 2019. URL https://hal.inria.fr/hal-02131630 .\\nAkira Kinjo and Haruki Nakamura. Comprehensive structural classiﬁcation of ligand-binding motifs\\nin proteins. Structure , 17(2), 2009.\\nMichael Schantz Klausen, Martin Closter Jespersen, Henrik Nielsen, Kamilla Kjaergaard Jensen,\\nVanessa Isabell Jurtz, Casper Kaae Soenderby, Morten Otto Alexander Sommer, Ole Winther,\\nMorten Nielsen, Bent Petersen, et al. NetSurfP-2.0: Improved prediction of protein structural\\nfeatures by integrated deep learning. Proteins: Structure, Function, and Bioinformatics , 2019.\\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets\\nof BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\\nIJCNLP) , pp. 4365–4374, Hong Kong, China, 2019. Association for Computational Linguistics.\\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black, and Yulia Tsvetkov. Measuring bias in\\ncontextualized word representations. In Proceedings of the First Workshop on Gender Bias in\\nNatural Language Processing , pp. 166–172, Florence, Italy, 2019. Association for Computational\\nLinguistics.\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\\nAlbert: A lite bert for self-supervised learning of language representations. In International\\nConference on Learning Representations , 2020.\\nJuyong Lee, Janez Konc, Dusanka Janezic, and Bernard Brooks. Global organization of a binding\\nsite network gives insight into evolution and structure-function relationships of proteins. Sci Rep , 7\\n(11652), 2017.\\nYongjie Lin, Yi Chern Tan, and Robert Frank. Open sesame: Getting inside BERT’s linguistic\\nknowledge. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP , pp. 241–253, Florence, Italy, 2019. Association for Computational\\nLinguistics.\\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic\\nknowledge and transferability of contextual representations. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) , pp. 1073–1094. Association for Computational\\nLinguistics, 2019.\\nAli Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R. Eguchi,\\nPo-Ssu Huang, and Richard Socher. Progen: Language modeling for protein generation. arXiv\\npreprint arXiv:2004.03497 , 2020.\\nTimothee Mickus, Mathieu Constant, Denis Paperno, and Kees Van Deemter. What do you mean,\\nBERT? Assessing BERT as a Distributional Semantics Model. Proceedings of the Society for\\nComputation in Linguistics , 3, 2020.\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations\\nof words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling,\\nZ. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems\\n26, pp. 3111–3119. Curran Associates, Inc., 2013.\\n11'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 11}, page_content=\"Published as a conference paper at ICLR 2021\\nPooya Moradi, Nishant Kambhatla, and Anoop Sarkar. Interrogating the explanatory power of\\nattention in neural machine translation. In Proceedings of the 3rd Workshop on Neural Generation\\nand Translation , pp. 221–230, Hong Kong, November 2019. Association for Computational\\nLinguistics. doi: 10.18653/v1/D19-5624. URL https://www.aclweb.org/anthology/\\nD19-5624 .\\nJohn Moult, Krzysztof Fidelis, Andriy Kryshtafovych, Torsten Schwede, and Anna Tramontano.\\nCritical assessment of methods of protein structure prediction (CASP)-Round XII. Proteins:\\nStructure, Function, and Bioinformatics , 86:7–15, 2018. ISSN 08873585. doi: 10.1002/prot.25415.\\nURLhttp://doi.wiley.com/10.1002/prot.25415 .\\nHai Nguyen, David A Case, and Alexander S Rose. NGLview–interactive molecular graphics for\\nJupyter notebooks. Bioinformatics , 34(7):1241–1242, 12 2017. ISSN 1367-4803. doi: 10.1093/\\nbioinformatics/btx789. URL https://doi.org/10.1093/bioinformatics/btx789 .\\nTimothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language\\narguments. In Proceedings of the 57th Annual Meeting of the Association for Computational\\nLinguistics , pp. 4658–4664, Florence, Italy, 2019. Association for Computational Linguistics.\\nJosh Payne, Mario Srouji, Dian Ang Yap, and Vineet Kosaraju. Bert learns (and teaches) chemistry.\\narXiv preprint arXiv:2007.16012 , 2020.\\nMatthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual\\nword embeddings: Architecture and representation. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing , pp. 1499–1509, Brussels, Belgium, October-\\nNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1179. URL\\nhttps://www.aclweb.org/anthology/D18-1179 .\\nDanish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C. Lipton. Learning to\\ndeceive with attention-based explanations. In Annual Conference of the Association for Computa-\\ntional Linguistics (ACL) , July 2020. URL https://arxiv.org/abs/1909.07913 .\\nAlessandro Raganato and Jörg Tiedemann. An analysis of encoder representations in Transformer-\\nbased machine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing\\nand Interpreting Neural Networks for NLP , pp. 287–297, Brussels, Belgium, November 2018.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/W18-5431. URL https://www.\\naclweb.org/anthology/W18-5431 .\\nRoshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel,\\nand Yun S Song. Evaluating protein transfer learning with TAPE. In Advances in Neural\\nInformation Processing Systems , 2019.\\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, and\\nBeen Kim. Visualizing and measuring the geometry of BERT. In H. Wallach, H. Larochelle,\\nA. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information\\nProcessing Systems , volume 32, pp. 8594–8603. Curran Associates, Inc., 2019.\\nAdam J Riesselman, Jung-Eun Shin, Aaron W Kollasch, Conor McMahon, Elana Simon, Chris\\nSander, Aashish Manglik, Andrew C Kruse, and Debora S Marks. Accelerating protein design\\nusing autoregressive generative models. bioRxiv , pp. 757252, 2019.\\nAlexander Rives, Siddharth Goyal, Joshua Meier, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry\\nMa, and Rob Fergus. Biological structure and function emerge from scaling unsupervised learning\\nto 250 million protein sequences. bioRxiv , pp. 622803, 2019.\\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in BERTology: What we know about\\nhow BERT works. Transactions of the Association for Computational Linguistics , 8:842–866,\\n2020.\\nNathan J Rollins, Kelly P Brock, Frank J Poelwijk, Michael A Stifﬂer, Nicholas P Gauthier, Chris\\nSander, and Debora S Marks. Inferring protein 3D structure from deep mutation scans. Nature\\nGenetics , 51(7):1170, 2019.\\n12\"),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 12}, page_content='Published as a conference paper at ICLR 2021\\nAlexander S. Rose and Peter W. Hildebrand. NGL Viewer: a web application for molecular vi-\\nsualization. Nucleic Acids Research , 43(W1):W576–W579, 04 2015. ISSN 0305-1048. doi:\\n10.1093/nar/gkv402. URL https://doi.org/10.1093/nar/gkv402 .\\nAlexander S Rose, Anthony R Bradley, Yana Valasatava, Jose M Duarte, Andreas Prli ´c, and Peter W\\nRose. NGL viewer: web-based molecular graphics for large complexes. Bioinformatics , 34(21):\\n3755–3758, 05 2018. ISSN 1367-4803. doi: 10.1093/bioinformatics/bty419. URL https:\\n//doi.org/10.1093/bioinformatics/bty419 .\\nCharles Rubin and Ora Rosen. Protein phosphorylation. Annual Review of Biochemistry , 44:831–887,\\n1975. URL https://doi.org/10.1146/annurev.bi.44.070175.004151 .\\nPhilippe Schwaller, Benjamin Hoover, Jean-Louis Reymond, Hendrik Strobelt, and Teodoro\\nLaino. Unsupervised attention-guided atom-mapping. ChemRxiv , 5 2020. doi: 10.26434/\\nchemrxiv.12298559.v1. URL https://chemrxiv.org/articles/Unsupervised_\\nAttention-Guided_Atom-Mapping/12298559 .\\nSoﬁa Serrano and Noah A. Smith. Is attention interpretable? In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics , pp. 2931–2951, Florence, Italy, July\\n2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1282. URL https:\\n//www.aclweb.org/anthology/P19-1282 .\\nMartin Steinegger and Johannes Söding. Clustering huge protein sequence sets in linear time. Nature\\nCommunications , 9(2542), 2018. doi: 10.1038/s41467-018-04964-5.\\nBaris E. Suzek, Yuqi Wang, Hongzhan Huang, Peter B. McGarvey, Cathy H. Wu, and the UniProt Con-\\nsortium. UniRef clusters: a comprehensive and scalable alternative for improving sequence\\nsimilarity searches. Bioinformatics , 31(6):926–932, 11 2014. ISSN 1367-4803. doi: 10.1093/\\nbioinformatics/btu739. URL https://doi.org/10.1093/bioinformatics/btu739 .\\nYi Chern Tan and L. Elisa Celis. Assessing social and intersectional biases in contextualized word\\nrepresentations. In Advances in Neural Information Processing Systems 32 , pp. 13230–13241.\\nCurran Associates, Inc., 2019.\\nIan Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In\\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp.\\n4593–4601, Florence, Italy, 2019. Association for Computational Linguistics.\\nShikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. Attention inter-\\npretability across NLP tasks. arXiv preprint arXiv:1909.11218 , 2019.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\\nProcessing Systems , pp. 5998–6008, 2017.\\nSara Veldhoen, Dieuwke Hupkes, and Willem H. Zuidema. Diagnostic classiﬁers revealing how\\nneural networks process hierarchical structure. In CoCo@NIPS , 2016.\\nJesse Vig. A multiscale visualization of attention in the Transformer model. In Proceedings of the\\n57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations ,\\npp. 37–42, Florence, Italy, 2019. Association for Computational Linguistics.\\nJesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a Transformer language\\nmodel. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP , pp. 63–76, Florence, Italy, 2019. Association for Computational\\nLinguistics.\\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and\\nStuart Shieber. Investigating gender bias in language models using causal mediation analysis. In\\nAdvances in Neural Information Processing Systems , volume 33, pp. 12388–12401, 2020.\\nGregor Wiedemann, Steffen Remus, Avi Chawla, and Chris Biemann. Does BERT make any\\nsense? Interpretable word sense disambiguation with contextualized embeddings. arXiv preprint\\narXiv:1909.10430 , 2019.\\n13'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 13}, page_content=\"Published as a conference paper at ICLR 2021\\nSarah Wiegreffe and Yuval Pinter. Attention is not not explanation. In Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint\\nConference on Natural Language Processing (EMNLP-IJCNLP) , pp. 11–20, November 2019.\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V\\nLe. XLNet: Generalized autoregressive pretraining for language understanding. In H. Wallach,\\nH. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural\\nInformation Processing Systems , volume 32, pp. 5753–5763. Curran Associates, Inc., 2019.\\nRuiqi Zhong, Steven Shao, and Kathleen McKeown. Fine-grained sentiment analysis with faithful\\nattention. arXiv preprint arXiv:1908.06870 , 2019.\\n14\"),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 14}, page_content='Published as a conference paper at ICLR 2021\\nA M ODEL OVERVIEW\\nA.1 P RE-TRAINED MODELS\\nTable 1 provides an overview of the ﬁve pre-trained Transformer models studied in this work. The\\nmodels originate from the TAPE and ProtTrans repositories, spanning three model architectures:\\nBERT, ALBERT, and XLNet.\\nTable 1: Summary of pre-trained models analyzed, including the source of the model, the type of\\nTransformer used, the number of layers and heads, the total number of model parameters, the source\\nof the pre-training dataset, and the number of protein sequences in the pre-training dataset.\\nSource Name Type Layers Heads Params Train Dataset # Seq\\nTAPE TapeBert BERT 12 12 94M Pfam 31M\\nProtTrans ProtBert BERT 30 16 420M Uniref100 216M\\nProtTrans ProtBert-BFD BERT 30 16 420M BFD 2.1B\\nProtTrans ProtAlbert ALBERT 12 64 224M Uniref100 216M\\nProtTrans ProtXLNet XLNet 30 16 409M Uniref100 216M\\nA.2 BERT T RANSFORMER ARCHITECTURE\\nStacked Encoder: BERT uses a stacked-encoder architecture, which inputs a sequence of tokens\\nx= (x1,...,xn)and applies position and token embeddings followed by a series of encoder\\nlayers. Each layer applies multi-head self-attention (see below) in combination with a feedforward\\nnetwork, layer normalization, and residual connections. The output of each layer ℓis a sequence of\\ncontextualized embeddings (h(ℓ)\\n1,...,h(ℓ)\\nn).\\nSelf-Attention: Given an input x= (x1,...,xn), the self-attention mechanism assigns to each\\ntoken pairi,jan attention weight αi,j>0where∑\\njαi,j= 1. Attention in BERT is bidirectional.\\nIn the multi-layer, multi-head setting, αis speciﬁc to a layer and head. The BERT-Base model has 12\\nlayers and 12 heads. Each attention head learns a distinct set of weights, resulting in 12 x 12 = 144\\ndistinct attention mechanisms in this case.\\nThe attention weights αi,jare computed from the scaled dot-product of the query vector ofiand the\\nkey vector ofj, followed by a softmax operation. The attention weights are then used to produce a\\nweighted sum of value vectors:\\nAttention (Q,K,V ) =softmax(QKT\\n√dk)\\nV (2)\\nusing query matrix Q, key matrix K, and value matrix V, wheredkis the dimension of K. In a\\nmulti-head setting, the queries, keys, and values are linearly projected htimes, and the attention\\noperation is performed in parallel for each representation, with the results concatenated.\\nA.3 O THER TRANSFORMER VARIANTS\\nALBERT: The architecture of ALBERT differs from BERT in two ways: (1) It shares parameters\\nacross layers, unlike BERT which learns distinct parameters for every layer and (2) It uses factorized\\nembeddings, which allows the input token embeddings to be of a different (smaller) size than the\\nhidden states. The original version of ALBERT designed for text also employed a sentence-order\\nprediction pretraining task, but this was not used on the models studied in this paper.\\nXLNet: Instead of the masked-language modeling pretraining objective use for BERT, XLNet uses\\na bidirectional auto-regressive pretraining method that considers all possible orderings of the input\\nfactorization. The architecture also adds a segment recurrence mechanism to process long sequences,\\nas well as a relative rather than absolute encoding scheme.\\n15'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 15}, page_content='Published as a conference paper at ICLR 2021\\nB A DDITIONAL EXPERIMENTAL DETAILS\\nB.1 A LTERNATIVE ATTENTION AGREEMENT METRIC\\nHere we present an alternative formulation to Eq. 1 based on an attention-weighted average. We\\ndeﬁne an indicator function f(i,j)for property fthat returns 1 if the property is present in token pair\\n(i,j)(i.e., if amino acids iandjare in contact), and zero otherwise. We then compute the proportion\\nof attention that matches with fover a dataset Xas follows:\\npα(f) =∑\\nx∈X|x|∑\\ni=1|x|∑\\nj=1f(i,j)αi,j(x)/∑\\nx∈X|x|∑\\ni=1|x|∑\\nj=1αi,j(x) (3)\\nwhereαi,j(x)denotes the attention from itojfor input sequence x.\\nB.2 S TATISTICAL SIGNIFICANCE TESTING AND NULL MODELS\\nWe perform statistical signiﬁcance tests to determine whether any results based on the metric deﬁned\\nin Equation 1 are due to chance. Given a property f, as deﬁned in Section 3, we perform a two-\\nproportion z-test comparing (1) the proportion of high-conﬁdence attention arcs ( αi,j>θ) for which\\nf(i,j) = 1 , and (2) the proportion of all possible pairs i,jfor whichf(i,j) = 1 . Note that the ﬁrst\\nproportion is exactly the metric pα(f)deﬁned in Equation 1 (e.g. the proportion of attention aligned\\nwith contact maps). The second proportion is simply the background frequency of the property (e.g.\\nthe background frequency of contacts). Since we extract the maximum scores over all of the heads in\\nthe model, we treat this as a case of multiple hypothesis testing and apply the Bonferroni correction,\\nwith the number of hypotheses mequal to the number of attention heads.\\nAs an additional check that the results did not occur by chance, we also report results on baseline\\n(null) models. We initially considered using two forms of null models: (1) a model with randomly\\ninitialized weights. and (2) a model trained on randomly shufﬂed sequences. However, in both cases,\\nnone of the sequences in the dataset yielded attention weights greater than the attention threshold θ.\\nThis suggests that the mere existence of the high-conﬁdence attention weights used in the analysis\\ncould not have occurred by chance, but it does not shed light on the particular analyses performed.\\nTherefore, we implemented an alternative randomization scheme in which we randomly shufﬂe\\nattention weights from the original models as a post-processing step. Speciﬁcally, we permute the\\nsequence of attention weights from each token for every attention head. To illustrate, let’s say that\\nthe original model produced attention weights of (0.3, 0.2, 0.1, 0.4, 0.0) from position iin protein\\nsequencexfrom headh, where|x|= 5. In the null model, the attention weights from position iin\\nsequencexin headhwould be a random permutation of those weights, e.g., (0.2, 0.0, 0.4, 0.3, 0.1).\\nNote that these are still valid attention weights as they would sum to 1 (since the original weights\\nwould sum to 1 by deﬁnition). We report results using this form of baseline model.\\nB.3 P ROBING METHODOLOGY\\nEmbedding probe. We probe the embedding vectors output from each layer using a linear probing\\nclassiﬁer. For token-level probing tasks (binding sites, secondary structure) we feed each token’s\\noutput vector directly to the classiﬁer. For token-pair probing tasks (contact map) we construct a\\npairwise feature vector by concatenating the elementwise differences and products of the two tokens’\\noutput vectors, following the TAPE4implementation.\\nWe use task-speciﬁc evaluation metrics for the probing classiﬁer: for secondary structure prediction,\\nwe measure F1 score; for contact prediction, we measure precision@ L/5, whereLis the length of\\nthe protein sequence, following standard practice (Moult et al., 2018); for binding site prediction,\\nwe measure precision@ L/20, since approximately one in twenty amino acids in each sequence is a\\nbinding site (4.8% in the dataset).\\nAttention probe. Just as the attention weight αi,jis deﬁned for a pair of amino acids (i,j), so is'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 15}, page_content='binding site (4.8% in the dataset).\\nAttention probe. Just as the attention weight αi,jis deﬁned for a pair of amino acids (i,j), so is\\nthe contact property f(i,j), which returns true if amino acids iandjare in contact. Treating the\\nattention weight as a feature of a token-pair (i,j), we can train a probing classiﬁer that predicts the\\n4https://github.com/songlab-cal/tape\\n16'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 16}, page_content='Published as a conference paper at ICLR 2021\\ncontact property based on this feature, thereby quantifying the attention mechanism’s knowledge of\\nthat property. In our multi-head setting, we treat the attention weights across all heads in a given layer\\nas a feature vector, and use a probing classiﬁer to assess the knowledge of a given property in the\\nattention weights across the entire layer. As with the embedding probe, we measure performance of\\nthe probing classiﬁer using precision@ L/5, whereLis the length of the protein sequence, following\\nstandard practice for contact prediction.\\nB.4 D ATASETS\\nWe used two protein sequence datasets from the TAPE repository for the analysis: the ProteinNet\\ndataset (AlQuraishi, 2019; Fox et al., 2013; Berman et al., 2000; Moult et al., 2018) and the Secondary\\nStructure dataset (Rao et al., 2019; Berman et al., 2000; Moult et al., 2018; Klausen et al., 2019). The\\nformer was used for analysis of amino acids and contact maps, and the latter was used for analysis of\\nsecondary structure. We additionally created a third dataset for binding site and post-translational\\nmodiﬁcation (PTM) analysis from the Secondary Structure dataset, which was augmented with\\nbinding site and PTM annotations obtained from the Protein Data Bank’s Web API.5We excluded\\nany sequences for which annotations were not available. The resulting dataset sizes are shown in\\nTable 2. For the analysis of attention, a random subset of 5000 sequences from the training split\\nof each dataset was used, as the analysis was purely evaluative. For training and evaluating the\\ndiagnostic classiﬁer, the full training and validation splits were used.\\nTable 2: Datasets used in analysis\\nDataset Train size Validation size\\nProteinNet 25299 224\\nSecondary Structure 8678 2170\\nBinding Sites / PTM 5734 1418\\nC A DDITIONAL RESULTS OF ATTENTION ANALYSIS\\nC.1 S ECONDARY STRUCTURE\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n15%30%45%60%\\n(a) TapeBert\\n246810121416182022242628303234363840424446485052545658606264\\nHead24681012Layer% Attention\\n050%Max\\n0%20%40%60% (b) ProtAlbert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n0%20%40%60%80%\\n(c) ProtBert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n20%40%60%80% (d) ProtBert-BFD\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n0%20%40%60% (e) ProtXLNet\\nFigure 8: Percentage of each head’s attention that is focused on Helix secondary structure.\\n5http://www.rcsb.org/pdb/software/rest.do\\n17'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 17}, page_content='Published as a conference paper at ICLR 2021\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n15%30%45%60%75%\\n(a) TapeBert\\n246810121416182022242628303234363840424446485052545658606264\\nHead24681012Layer% Attention\\n050%Max\\n0%20%40%60%80% (b) ProtAlbert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n0%20%40%60%80%\\n(c) ProtBert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n20%40%60%80% (d) ProtBert-BFD\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n0%15%30%45%60% (e) ProtXLNet\\nFigure 9: Percentage of each head’s attention that is focused on Strand secondary structure.\\n24681012\\nHead24681012Layer% Attention\\n0 50%Max\\n15%30%45%\\n(a) TapeBert\\n246810121416182022242628303234363840424446485052545658606264\\nHead24681012Layer% Attention\\n0 50%Max\\n0%15%30%45%60% (b) ProtAlbert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n0 50%Max\\n0%15%30%45%60%\\n(c) ProtBert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n20%40%60% (d) ProtBert-BFD\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n025%Max\\n0%10%20%30%40% (e) ProtXLNet\\nFigure 10: Percentage of each head’s attention that is focused on Turn/Bend secondary structure.\\n18'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 18}, page_content='Published as a conference paper at ICLR 2021\\nC.2 C ONTACT MAPS: STATISTICAL SIGNIFICANCE TESTS AND NULL MODELS\\n12-412-12 12-1112-28-512-79-76-119-812-5\\nTop heads02040Attention %\\n(a) TapeBert\\n11-3 12-3 10-3 1-119-510-5 9-31 8-31 3-31 9-13\\nTop heads0204060Attention %\\n (b) ProtAlbert\\n30-1429-44-419-74-8\\n30-10 19-1020-7 20-23-1\\nTop heads0204060Attention %\\n(c) ProtBert\\n30-9 30-8 29-429-1530-5 30-330-1210-9 4-11 29-7\\nTop heads0204060Attention %\\n (d) ProtBert-BFD\\n26-627-158-4 4-529-38-86-14 18-128-1025-2\\nTop heads02040Attention %\\n(e) ProtXLNet\\nFigure 11: Top 10 heads (denoted by <layer>-<head> ) for each model based on the proportion of\\nattention aligned with contact maps [95% conf. intervals]. The differences between the attention\\nproportions and the background frequency of contacts (orange dashed line) are statistically signiﬁcant\\n(p< 0.00001 ). Bonferroni correction applied for both conﬁdence intervals and tests (see App. B.2).\\n11-612-119-79-11 10-2 11-812-123-7 4-710-9\\nTop heads0246Attention %\\n(a) TapeBert-Random\\n12-201-16 1-47 1-11 2-55 1-40 1-25 7-55 3-55 6-55\\nTop heads051015Attention %\\n (b) ProtAlbert-Random\\n4-36-16 7-15 5-14 10-21-4\\n21-135-111-725-13\\nTop heads024Attention %\\n(c) ProtBert-Random\\n6-1612-12 11-108-164-5 5-31-15 11-9 10-2 4-16\\nTop heads0510Attention %\\n (d) ProtBert-BFD-Random\\n4-5 8-8 8-48-133-918-1 13-7 2-12 30-43-8\\nTop heads0246Attention %\\n(e) ProtXLNet-Random\\nFigure 12: Top-10 contact-aligned heads for null models. See Appendix B.2 for details.\\n19'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 19}, page_content='Published as a conference paper at ICLR 2021\\nC.3 B INDING SITES : STATISTICAL SIGNIFICANCE TESTS AND NULL MODEL\\n11-611-127-18-127-610-3 11-38-4 5-7 7-7\\nTop heads02040Attention %\\n(a) TapeBert\\n2-26 3-29 2-16 4-29 6-17 5-17 7-17 3-17 8-17 2-17\\nTop heads0204060Attention %\\n (b) ProtAlbert\\n24-721-1223-325-10 24-15 21-1426-2 4-1324-115-10\\nTop heads02040Attention %\\n(c) ProtBert\\n26-125-1425-4 26-226-1324-1 26-426-12 27-1523-1\\nTop heads02040Attention %\\n (d) ProtBert-BFD\\n11-169-912-3 14-6 16-913-12 19-13 17-1017-6 14-8\\nTop heads051015Attention %\\n(e) ProtXLNet\\nFigure 13: Top 10 heads (denoted by <layer>-<head> ) for each model based on the proportion of\\nattention focused on binding sites [95% conf. intervals]. Differences between attention proportions\\nand the background frequency of binding sites (orange dashed line) are all statistically signiﬁcant\\n(p< 0.00001 ). Bonferroni correction applied for both conﬁdence intervals and tests (see App. B.2).\\n11-3 11-811-12 12-113-612-7 11-1 6-119-4 9-5\\nTop heads0510Attention %\\n(a) TapeBert-Random\\n2-5911-60 12-33 11-33 11-18 11-199-42 2-6212-47 10-60\\nTop heads0204060Attention %\\n (b) ProtAlbert-Random\\n5-426-24-2\\n27-1527-6 25-6 23-322-136-36-11\\nTop heads0510Attention %\\n(c) ProtBert-Random\\n6-8\\n26-12 13-14 11-1325-2 24-111-1613-526-13 13-13\\nTop heads02040Attention %\\n (d) ProtBert-BFD-Random\\n22-8 22-317-10 19-13 18-13 14-1116-98-4\\n23-14 11-14\\nTop heads0510Attention %\\n(e) ProtXLNet-Random\\nFigure 14: Top-10 heads most focused on binding sites for null models. See Appendix B.2 for details.\\n20'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 20}, page_content='Published as a conference paper at ICLR 2021\\nC.4 P OST-TRANSLATIONAL MODIFICATIONS (PTM S)\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n15%30%45%60%\\n(a) TapeBert\\n246810121416182022242628303234363840424446485052545658606264\\nHead24681012Layer% Attention\\n025%Max\\n0%10%20%30%40% (b) ProtAlbert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n025%Max\\n0%8%16%24%32%\\n(c) ProtBert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n025%Max\\n0%10%20%30%40% (d) ProtBert-BFD\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n0 5%Max\\n0%2%3%4%6% (e) ProtXLNet\\nFigure 15: Percentage of each head’s attention that is focused on post-translational modiﬁcations.\\n21'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 21}, page_content='Published as a conference paper at ICLR 2021\\n11-69-7\\n11-1211-3 10-34-7 5-8 9-4 5-19-10\\nTop heads0204060Attention %\\n(a) TapeBert\\n2-28 1-11 2-19 2-3410-564-29 3-34 3-2910-176-17\\nTop heads02040Attention %\\n (b) ProtAlbert\\n26-225-1310-2 24-79-4\\n24-155-14 23-321-127-3\\nTop heads02040Attention %\\n(c) ProtBert\\n12-128-9\\n25-1426-1 24-127-11 26-14 26-1226-426-15\\nTop heads02040Attention %\\n (d) ProtBert-BFD\\n11-1613-712-16 13-1214-6 30-4 11-3 15-422-1411-5\\nTop heads0.02.55.07.5Attention %\\n(e) ProtXLNet\\nFigure 16: Top 10 heads (denoted by <layer>-<head> ) for each model based on the proportion of\\nattention focused on PTM positions [95% conf. intervals]. The differences between the attention\\nproportions and the background frequency of PTMs (orange dashed line) are statistically signiﬁcant\\n(p< 0.00001 ). Bonferroni correction applied for both conﬁdence intervals and tests (see App. B.2).\\n11-69-711-1 10-3 6-113-2 5-1 4-710-7 9-11\\nTop heads0246Attention %\\n(a) TapeBert-Random\\n1-57 1-40 3-55 4-55 1-55 1-25 1-36 5-55 1-11 7-55\\nTop heads01020Attention %\\n (b) ProtAlbert-Random\\n5-4\\n29-1429-2 16-5 29-91-425-627-13 26-1226-2\\nTop heads0510Attention %\\n(c) ProtBert-Random\\n6-8 8-9\\n12-1210-5 11-111-168-107-6\\n29-133-3\\nTop heads020Attention %\\n (d) ProtBert-BFD-Random\\n30-430-1513-714-12 22-14 21-1410-4 18-3 19-315-10\\nTop heads024Attention %\\n(e) ProtXLNet-Random\\nFigure 17: Top-10 heads most focused on PTMs for null models. See Appendix B.2 for details.\\n22'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 22}, page_content='Published as a conference paper at ICLR 2021\\nC.5 A MINO ACIDS\\n24681012\\nHead24681012Layer% Attention\\n0 25%Max\\n0%6%12%18%24%\\n(a) ALA\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n0%15%30%45%60% (b) ARG\\n24681012\\nHead24681012Layer% Attention\\n025%Max\\n0%10%20%30%40% (c) ASN\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n0%20%40%60%\\n(d) ASP\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n0%20%40%60%80% (e) CYS\\n24681012\\nHead24681012Layer% Attention\\n05%Max\\n0%2%3%4%6% (f) GLN\\n24681012\\nHead24681012Layer% Attention\\n010%Max\\n0%4%8%12%16%\\n(g) GLU\\n24681012\\nHead24681012Layer% Attention\\n0 100%Max\\n0%20%40%60%80% (h) GLY\\n24681012\\nHead24681012Layer% Attention\\n0 50%Max\\n0%15%30%45% (i) HIS\\n24681012\\nHead24681012Layer% Attention\\n0 25%Max\\n0%6%12%18%24%\\n(j) ILE\\n24681012\\nHead24681012Layer% Attention\\n025%Max\\n0%10%20%30%40% (k) LEU\\n24681012\\nHead24681012Layer% Attention\\n0 25%Max\\n0%6%12%18%24% (l) LYS\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n0%15%30%45%60%\\n(m) MET\\n24681012\\nHead24681012Layer% Attention\\n0 20%Max\\n0%5%10%15%20% (n) PHE\\n24681012\\nHead24681012Layer% Attention\\n0 100%Max\\n0%20%40%60%80% (o) PRO\\nFigure 18: Percentage of each head’s attention that is focused on the given amino acid, averaged over\\na dataset (TapeBert).\\n23'),\n",
       " Document(metadata={'source': '../datasets/scientific_articles/bertology.pdf', 'page': 23}, page_content='Published as a conference paper at ICLR 2021\\n24681012\\nHead24681012Layer% Attention\\n025%Max\\n0%8%16%24%32%\\n(a) SER\\n24681012\\nHead24681012Layer% Attention\\n010%Max\\n0%4%8%12%16% (b) THR\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n0%15%30%45%60% (c) TRP\\n24681012\\nHead24681012Layer% Attention\\n0 50%Max\\n0%15%30%45%\\n(d) TYR\\n24681012\\nHead24681012Layer% Attention\\n025%Max\\n0%8%16%24%32% (e) V AL\\nFigure 19: Percentage of each head’s attention that is focused on the given amino acid, averaged over\\na dataset (cont.)\\nTable 3: Amino acids and the corresponding maximally attentive heads in the standard and randomized\\nversions of TapeBert. The differences between the attention percentages for TapeBert and the\\nbackground frequencies of each amino acid are all statistically signiﬁcant ( p< 0.00001 ) taking into\\naccount the Bonferroni correction. See Appendix B.2 for details. The bolded numbers represent the\\nhigher of the two values between the standard and random models. In all cases except for Glutamine,\\nwhich was the amino acid with the lowest top attention proportion in the standard model (7.1), the\\nstandard TapeBert model has higher values than the randomized version.\\nTapeBert TapeBert-Random\\nAbbrev Code Name Background % Top Head Attn % Top Head Attn %\\nAla A Alanine 7.9 12-11 25.5 11-12 12.1\\nArg R Arginine 5.2 12-8 63.2 12-7 8.4\\nAsn N Asparagine 4.3 8-2 44.8 8-2 6.7\\nAsp D Aspartic acid 5.8 12-6 79.9 5-4 10.7\\nCys C Cysteine 1.3 11-6 83.2 11-6 9.3\\nGln Q Glutamine 3.8 11-7 7.1 12-1 9.2\\nGlu E Glutamic acid 6.9 11-7 16.2 11-4 11.8\\nGly G Glycine 7.1 2-11 98.1 11-8 14.6\\nHis H Histidine 2.7 9-10 56.7 11-6 5.4\\nIle I Isoleucine 5.6 11-10 27.0 9-5 10.6\\nLeu L Leucine 9.4 2-12 44.1 12-11 13.9\\nLys K Lysine 6.0 12-8 29.4 6-11 12.9\\nMet M Methionine 2.3 3-10 73.5 9-3 6.2\\nPhe F Phenylalanine 3.9 12-3 22.7 12-1 6.7\\nPro P Proline 4.6 1-11 98.3 10-6 7.6\\nSer S Serine 6.4 12-7 36.1 11-12 11.0\\nThr T Threonine 5.4 12-7 19.0 10-4 9.0\\nTrp W Tryptophan 1.3 11-4 68.1 9-2 3.0\\nTyr Y Tyrosine 3.4 12-3 51.6 12-11 6.6\\nVal V Valine 6.8 12-11 34.0 8-2 15.0\\n24')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "\n",
    "# loader = TextLoader(\"../datasets/scientific_articles/bertology.txt\")\n",
    "# docs = loader.load()\n",
    "# docs\n",
    "\n",
    "loader = PyPDFLoader(f\"../datasets/scientific_articles/bertology.pdf\")\n",
    "docs = loader.load_and_split()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itext2kg.documents_distiller import DocumentsDisiller, Article\n",
    "\n",
    "document_distiller = DocumentsDisiller(llm_model=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Published as a conference paper at ICLR 2021\\nBERT OLOGY MEETS BIOLOGY : INTERPRETING\\nATTENTION IN PROTEIN LANGUAGE MODELS\\nJesse Vig1Ali Madani1Lav R. Varshney1,2Caiming Xiong1\\nRichard Socher1Nazneen Fatema Rajani1\\n1Salesforce Research,2University of Illinois at Urbana-Champaign\\n[jvig,amadani,cxiong,rsocher,nazneen.rajani]@salesforce.com\\nvarshney@illinois.edu\\nABSTRACT\\nTransformer architectures have proven to learn useful representations for pro-\\ntein classiﬁcation and generation tasks. However, these representations present\\nchallenges in interpretability. In this work, we demonstrate a set of methods for\\nanalyzing protein Transformer models through the lens of attention. We show that\\nattention: (1) captures the folding structure of proteins, connecting amino acids that\\nare far apart in the underlying sequence, but spatially close in the three-dimensional\\nstructure, (2) targets binding sites, a key functional component of proteins, and\\n(3) focuses on progressively more complex biophysical properties with increas-\\ning layer depth. We ﬁnd this behavior to be consistent across three Transformer\\narchitectures (BERT, ALBERT, XLNet) and two distinct protein datasets. We\\nalso present a three-dimensional visualization of the interaction between atten-\\ntion and protein structure. Code for visualization and analysis is available at\\nhttps://github.com/salesforce/provis .\\n1 I NTRODUCTION\\nThe study of proteins, the fundamental macromolecules governing biology and life itself, has led to\\nremarkable advances in understanding human health and the development of disease therapies. The\\ndecreasing cost of sequencing technology has enabled vast databases of naturally occurring proteins\\n(El-Gebali et al., 2019a), which are rich in information for developing powerful machine learning\\nmodels of protein sequences. For example, sequence models leveraging principles of co-evolution,\\nwhether modeling pairwise or higher-order interactions, have enabled prediction of structure or\\nfunction (Rollins et al., 2019).\\nProteins, as a sequence of amino acids, can be viewed precisely as a language and therefore modeled\\nusing neural architectures developed for natural language. In particular, the Transformer (Vaswani\\net al., 2017), which has revolutionized unsupervised learning for text, shows promise for similar\\nimpact on protein sequence modeling. However, the strong performance of the Transformer comes\\nat the cost of interpretability, and this lack of transparency can hide underlying problems such as\\nmodel bias and spurious correlations (Niven & Kao, 2019; Tan & Celis, 2019; Kurita et al., 2019). In\\nresponse, much NLP research now focuses on interpreting the Transformer, e.g., the subspecialty of\\n“BERTology” (Rogers et al., 2020), which speciﬁcally studies the BERT model (Devlin et al., 2019).\\nIn this work, we adapt and extend this line of interpretability research to protein sequences. We\\nanalyze Transformer protein models through the lens of attention, and present a set of interpretability\\nmethods that capture the unique functional and structural characteristics of proteins. We also compare\\nthe knowledge encoded in attention weights to that captured by hidden-state representations. Finally,\\nwe present a visualization of attention contextualized within three-dimensional protein structure.\\nOur analysis reveals that attention captures high-level structural properties of proteins, connecting\\namino acids that are spatially close in three-dimensional structure, but apart in the underlying sequence\\n(Figure 1a). We also ﬁnd that attention targets binding sites, a key functional component of proteins\\n(Figure 1b). Further, we show how attention is consistent with a classic measure of similarity between\\namino acids—the substitution matrix. Finally, we demonstrate that attention captures progressively\\nhigher-level representations of structure and function with increasing layer depth.\\n1arXiv:2006.15222v3  [cs.CL]  28 Mar 2021',\n",
       " 'Published as a conference paper at ICLR 2021\\n(a) Attention in head 12-4, which targets amino\\nacid pairs that are close in physical space (see\\ninset subsequence 117D-157I) but lie apart in the\\nsequence. Example is a de novo designed TIM-\\nbarrel (5BVL) with characteristic symmetry.\\n(b) Attention in head 7-1, which targets binding\\nsites, a key functional component of proteins.\\nExample is HIV-1 protease (7HVP). The primary\\nlocation receiving attention is 27G, a binding site\\nfor protease inhibitor small-molecule drugs.\\nFigure 1: Examples of how specialized attention heads in a Transformer recover protein structure and\\nfunction, based solely on language model pre-training. Orange lines depict attention between amino\\nacids (line width proportional to attention weight; values below 0.1 hidden). Heads were selected\\nbased on correlation with ground-truth annotations of contact maps and binding sites. Visualizations\\nbased on the NGL Viewer (Rose et al., 2018; Rose & Hildebrand, 2015; Nguyen et al., 2017).\\nIn contrast to NLP, which aims to automate a capability that humans already have—understanding\\nnatural language—protein modeling also seeks to shed light on biological processes that are not fully\\nunderstood. Thus we also discuss how interpretability can aid scientiﬁc discovery.\\n2 B ACKGROUND : PROTEINS\\nIn this section we provide background on the biological concepts discussed in later sections.\\nAmino acids. Just as language is composed of words from a shared lexicon, every protein sequence\\nis formed from a vocabulary of amino acids, of which 20 are commonly observed. Amino acids may\\nbe denoted by their full name (e.g., Proline ), a 3-letter abbreviation ( Pro), or a single-letter code ( P).\\nSubstitution matrix. While word synonyms are encoded in a thesaurus, proteins that are similar in\\nstructure or function are captured in a substitution matrix , which scores pairs of amino acids on how\\nreadily they may be substituted for one another while maintaining protein viability. One common\\nsubstitution matrix is BLOSUM (Henikoff & Henikoff, 1992), which is derived from co-occurrence\\nstatistics of amino acids in aligned protein sequences.\\nProtein structure. Though a protein may be abstracted as a sequence of amino acids, it represents\\na physical entity with a well-deﬁned three-dimensional structure (Figure 1). Secondary structure\\ndescribes the local segments of proteins; two commonly observed types are the alpha helix andbeta\\nsheet .Tertiary structure encompasses the large-scale formations that determine the overall shape\\nand function of the protein. One way to characterize tertiary structure is by a contact map , which\\ndescribes the pairs of amino acids that are in contact (within 8 angstroms of one another) in the folded\\nprotein structure but lie apart (by at least 6 positions) in the underlying sequence (Rao et al., 2019).\\nBinding sites. Proteins may also be characterized by their functional properties. Binding sites are\\nprotein regions that bind with other molecules (proteins, natural ligands, and small-molecule drugs)\\nto carry out a speciﬁc function. For example, the HIV-1 protease is an enzyme responsible for a\\ncritical process in replication of HIV (Brik & Wong, 2003). It has a binding site, shown in Figure 1b,\\nthat is a target for drug development to ensure inhibition.\\nPost-translational modiﬁcations. After a protein is translated from RNA, it may undergo additional\\nmodiﬁcations, e.g. phosphorylation, which play a key role in protein structure and function.\\n2',\n",
       " 'Published as a conference paper at ICLR 2021\\n3 M ETHODOLOGY\\nModel. We demonstrate our interpretability methods on ﬁve Transformer models that were pretrained\\nthrough language modeling of amino acid sequences. We primarily focus on the BERT-Base model\\nfrom TAPE (Rao et al., 2019), which was pretrained on Pfam, a dataset of 31M protein sequences (El-\\nGebali et al., 2019b). We refer to this model as TapeBert . We also analyze 4 pre-trained Transformer\\nmodels from ProtTrans (Elnaggar et al., 2020): ProtBert andProtBert-BFD , which are 30-layer,\\n16-head BERT models; ProtAlbert , a 12-layer, 64-head ALBERT (Lan et al., 2020) model; and\\nProtXLNet , a 30-layer, 16-head XLNet (Yang et al., 2019) model. ProtBert-BFD was pretrained on\\nBFD (Steinegger & Söding, 2018), a dataset of 2.1B protein sequences, while the other ProtTrans\\nmodels were pretrained on UniRef100 (Suzek et al., 2014), which includes 216M protein sequences.\\nA summary of these 5 models is presented in Appendix A.1.\\nHere we present an overview of BERT, with additional details on all models in Appendix A.2. BERT\\ninputs a sequence of amino acids x= (x1,...,xn)and applies a series of encoders. Each encoder\\nlayerℓoutputs a sequence of continuous embeddings (h(ℓ)\\n1,...,h(ℓ)\\nn)using a multi-headed attention\\nmechanism. Each attention head in a layer produces a set of attention weights αfor an input, where\\nαi,j> 0 is the attention from token ito tokenj, such that∑\\njαi,j= 1. Intuitively, attention weights\\ndeﬁne the inﬂuence of every token on the next layer’s representation for the current token. We denote\\na particular head by <layer>-<head_index> , e.g. head 3-7for the 3rd layer’s 7th head.\\nAttention analysis. We analyze how attention aligns with various protein properties. For properties\\nof token pairs, e.g. contact maps, we deﬁne an indicator function f(i,j)that returns 1 if the property\\nis present in token pair (i,j)(e.g., if amino acids iandjare in contact), and 0 otherwise. We\\nthen compute the proportion of high-attention token pairs ( αi,j>θ) where the property is present,\\naggregated over a dataset X:\\npα(f) =∑\\nx∈X|x|∑\\ni=1|x|∑\\nj=1f(i,j)· 1αi,j>θ/∑\\nx∈X|x|∑\\ni=1|x|∑\\nj=11αi,j>θ (1)\\nwhereθis a threshold to select for high-conﬁdence attention weights. We also present an alternative,\\ncontinuous version of this metric in Appendix B.1.\\nFor properties of individual tokens , e.g. binding sites, we deﬁne f(i,j)to return 1 if the property is\\npresent in token j(e.g. ifjis a binding site). In this case, pα(f)equals the proportion of attention\\nthat is directed tothe property (e.g. the proportion of attention focused on binding sites).\\nWhen applying these metrics, we include two types of checks to ensure that the results are not\\ndue to chance. First, we test that the proportion of attention that aligns with particular properties\\nis signiﬁcantly higher than the background frequency of these properties, taking into account the\\nBonferroni correction for multiple hypotheses corresponding to multiple attention heads. Second,\\nwe compare the results to a null model, which is an instance of the model with randomly shufﬂed\\nattention weights. We describe these methods in detail in Appendix B.2.\\nProbing tasks. We also perform probing tasks on the model, which test the knowledge contained\\nin model representations by using them as inputs to a classiﬁer that predicts a property of interest\\n(Veldhoen et al., 2016; Conneau et al., 2018; Adi et al., 2016). The performance of the probing\\nclassiﬁer serves as a measure of the knowledge of the property that is encoded in the representation.\\nWe run both embedding probes , which assess the knowledge encoded in the output embeddings of\\neach layer, and attention probes (Reif et al., 2019; Clark et al., 2019), which measure the knowledge\\ncontained in the attention weights for pairwise features. Details are provided in Appendix B.3.\\nDatasets. For our analyses of amino acids and contact maps, we use a curated dataset from TAPE',\n",
       " 'contained in the attention weights for pairwise features. Details are provided in Appendix B.3.\\nDatasets. For our analyses of amino acids and contact maps, we use a curated dataset from TAPE\\nbased on ProteinNet (AlQuraishi, 2019; Fox et al., 2013; Berman et al., 2000; Moult et al., 2018),\\nwhich contains amino acid sequences annotated with spatial coordinates (used for the contact map\\nanalysis). For the analysis of secondary structure and binding sites we use the Secondary Structure\\ndataset (Rao et al., 2019; Berman et al., 2000; Moult et al., 2018; Klausen et al., 2019) from TAPE.\\nWe employed a taxonomy of secondary structure with three categories: Helix ,Strand , and Turn/Bend ,\\nwith the last two belonging to the higher-level beta sheet category (Sec. 2). We used this taxonomy to\\nstudy how the model understood structurally distinct regions of beta sheets. We obtained token-level\\nbinding site and protein modiﬁcation labels from the Protein Data Bank (Berman et al., 2000).\\nFor analyzing attention, we used a random subset of 5000 sequences from the training split of the\\n3',\n",
       " 'Published as a conference paper at ICLR 2021\\n24681012\\nHead24681012Layer% Attention\\n025%Max\\n0%10%20%30%40%\\n(a) TapeBert\\n246810121416182022242628303234363840424446485052545658606264\\nHead24681012Layer% Attention\\n0 50%Max\\n0%15%30%45% (b) ProtAlbert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n0 50%Max\\n0%15%30%45%\\n(c) ProtBert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n0 50%Max\\n0%15%30%45%60% (d) ProtBert-BFD\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n025%Max\\n0%10%20%30%40% (e) ProtXLNet\\nFigure 2: Agreement between attention and contact maps across ﬁve pretrained Transformer models\\nfrom TAPE (a) and ProtTrans (b–e). The heatmaps show the proportion of high-conﬁdence attention\\nweights (αi,j>θ) from each head that connects pairs of amino acids that are in contact with one\\nanother. In TapeBert (a), for example, we can see that 45% of attention in head 12-4 (the 12th layer’s\\n4th head) maps to contacts. The bar plots show the maximum value from each layer. Note that the\\nvertical striping in ProtAlbert (b) is likely due to cross-layer parameter sharing (see Appendix A.3).\\nrespective datasets (note that none of the aforementioned annotations were used in model training).\\nFor the diagnostic classiﬁer, we used the respective training splits for training and the validation splits\\nfor evaluation. See Appendix B.4 for additional details.\\nExperimental details We exclude attention to the [SEP] delimiter token, as it has been shown to\\nbe a “no-op” attention token (Clark et al., 2019), as well as attention to the [CLS] token, which is\\nnot explicitly used in language modeling. We only include results for attention heads where at least\\n100 high-conﬁdence attention arcs are available for analysis. We set the attention threshold θto 0.3\\nto select for high-conﬁdence attention while retaining sufﬁcient data for analysis. We truncate all\\nprotein sequences to a length of 512 to reduce memory requirements.1\\nWe note that all of the above analyses are purely associative and do not attempt to establish a causal\\nlink between attention and model behavior (Vig et al., 2020; Grimsley et al., 2020), nor to explain\\nmodel predictions (Jain & Wallace, 2019; Wiegreffe & Pinter, 2019).\\n4 W HAT DOES ATTENTION UNDERSTAND ABOUT PROTEINS ?\\n4.1 P ROTEIN STRUCTURE\\nHere we explore the relationship between attention and tertiary structure, as characterized by contact\\nmaps (see Section 2). Secondary structure results are included in Appendix C.1.\\nAttention aligns strongly with contact maps in the deepest layers. Figure 2 shows how attention\\naligns with contact maps across the heads of the ﬁve models evaluated2, based on the metric deﬁned in\\nEquation 1. The most aligned heads are found in the deepest layers and focus up to 44.7% (TapeBert),\\n55.7% (ProtAlbert), 58.5% (ProtBert), 63.2% (ProtBert-BFD), and 44.5% (ProtXLNet) of attention\\non contacts, whereas the background frequency of contacts among all amino acid pairs in the dataset\\nis 1.3%. Figure 1a shows an example of the induced attention from the top head in TapeBert. We note\\nthat the model with the single most aligned head—ProtBert-BFD—is the largest model (same size as\\nProteinBert) at 420M parameters (Appendix A.1) and it was also the only model pre-trained on the\\n194% of sequences had length less than 512. Experiments performed on single 16GB Tesla V-100 GPU.\\n2Heads with fewer than 100 high-conﬁdence attention weights across the dataset are grayed out.\\n4',\n",
       " 'Published as a conference paper at ICLR 2021\\n24681012\\nHead24681012Layer% Attention\\n0 50%Max\\n10%20%30%40%\\n(a) TapeBert\\n246810121416182022242628303234363840424446485052545658606264\\nHead24681012Layer% Attention\\n0 50%Max\\n0%15%30%45% (b) ProtAlbert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n0 50%Max\\n10%20%30%40%50%\\n(c) ProtBert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n025%Max\\n10%20%30%40% (d) ProtBert-BFD\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n010%Max\\n0%4%8%12% (e) ProtXLNet\\nFigure 3: Proportion of attention focused on binding sites across ﬁve pretrained models. The heatmaps\\nshow the proportion of high-conﬁdence attention ( αi,j>θ) from each head that is directed to binding\\nsites. In TapeBert (a), for example, we can see that 49% of attention in head 11-6 (the 11th layer’s\\n6th head) is directed to binding sites. The bar plots show the maximum value from each layer.\\nlargest dataset, BFD. It’s possible that both factors helped the model learn more structurally-aligned\\nattention patterns. Statistical signiﬁcance tests and null models are reported in Appendix C.2.\\nConsidering the models were trained on language modeling tasks without any spatial information,\\nthe presence of these structurally-aware attention heads is intriguing. One possible reason for this\\nemergent behavior is that contacts are more likely to biochemically interact with one another, creating\\nstatistical dependencies between the amino acids in contact. By focusing attention on the contacts of\\na masked position, the language models may acquire valuable context for token prediction.\\nWhile there seems to be a strong correlation between the attention head output and classically-deﬁned\\ncontacts, there are also differences. The models may have learned differing contextualized or nuanced\\nformulations that describe amino acid interactions. These learned interactions could then be used for\\nfurther discovery and investigation or repurposed for prediction tasks similar to how principles of\\ncoevolution enabled a powerful representation for structure prediction.\\n4.2 B INDING SITES AND POST-TRANSLATIONAL MODIFICATIONS\\nWe also analyze how attention interacts with binding sites and post-translational modiﬁcations\\n(PTMs), which both play a key role in protein function.\\nAttention targets binding sites throughout most layers of the models. Figure 3 shows the propor-\\ntion of attention focused on binding sites (Eq. 1) across the heads of the 5 models studied. Attention\\nto binding sites is most pronounced in the ProtAlbert model (Figure 3b), which has 22 heads that\\nfocus over 50% of attention on bindings sites, whereas the background frequency of binding sites in\\nthe dataset is 4.8%. The three BERT models (Figures 3a, 3c, and 3d) also attend strongly to binding\\nsites, with attention heads focusing up to 48.2%, 50.7%, and 45.6% of attention on binding sites,\\nrespectively. Figure 1b visualizes the attention in one strongly-aligned head from the TapeBert model.\\nStatistical signiﬁcance tests and a comparison to a null model are provided in Appendix C.3.\\nProtXLNet (Figure 3e) also targets binding sites, but not as strongly as the other models: the most\\naligned head focuses 15.1% of attention on binding sites, and the average head directs just 6.2% of\\nattention to binding sites, compared to 13.2%, 19.8%, 16.0%, and 15.1% for the ﬁrst four models\\nin Figure 3. It’s unclear whether this disparity is due to differences in architectures or pre-training\\nobjectives; for example, ProtXLNet uses a bidirectional auto-regressive pretraining method (see\\nAppendix A.2), whereas the other 4 models all use masked language modeling objectives.\\n5',\n",
       " 'Published as a conference paper at ICLR 2021\\n0%10%20%30%Helix\\n0%10%20%Turn/Bend\\n0%10%20%Strand\\n0%10%20%Binding Site\\n123456789101112\\nLayer0%5%10%Contact\\nFigure 4: Each plot shows the percentage of\\nattention focused on the given property, av-\\neraged over all heads within each layer. The\\nplots, sorted by center of gravity (red dashed\\nline), show that heads in deeper layers focus\\nrelatively more attention on binding sites and\\ncontacts, whereas attention toward speciﬁc\\nsecondary structures is more even across lay-\\ners.\\n0.60.7Helix\\n0.10.2Turn/Bend\\n0.20.40.6Strand\\n0.100.120.140.16Binding Site\\n123456789101112\\nLayer0.050.100.15ContactEmbedding probe\\nAttention probeFigure 5: Performance of probing classiﬁers\\nby layer, sorted by task order in Figure 4.\\nThe embedding probes (orange) quantify the\\nknowledge of the given property that is en-\\ncoded in each layer’s output embeddings. The\\nattention probe (blue), show the amount of in-\\nformation encoded in attention weights for the\\n(pairwise) contact feature. Additional details\\nare provided in Appendix B.3.\\nWhy does attention target binding sites? In contrast to contact maps, which reveal relationships\\nwithin proteins, binding sites describe how a protein interacts with other molecules. These external\\ninteractions ultimately deﬁne the high-level function of the protein, and thus binding sites remain\\nconserved even when the sequence as a whole evolves (Kinjo & Nakamura, 2009). Further, structural\\nmotifs in binding sites are mainly restricted to speciﬁc families or superfamilies of proteins (Kinjo &\\nNakamura, 2009), and binding sites can reveal evolutionary relationships among proteins (Lee et al.,\\n2017). Thus binding sites may provide the model with a high-level characterization of the protein\\nthat is robust to individual sequence variation. By attending to these regions, the model can leverage\\nthis higher-level context when predicting masked tokens throughout the sequence.\\nAttention targets PTMs in a small number of heads. A small number of heads in each model con-\\ncentrate their attention very strongly on amino acids associated with post-translational modiﬁcations\\n(PTMs). For example, Head 11-6 in TapeBert focused 64% of attention on PTM positions, though\\nthese occur at only 0.8% of sequence positions in the dataset.3Similar to our discussion on binding\\nsites, PTMs are critical to protein function (Rubin & Rosen, 1975) and thereby are likely to exhibit\\nbehavior that is conserved across the sequence space. See Appendix C.4 for full results.\\n4.3 C ROSS -LAYER ANALYSIS\\nWe analyze how attention captures properties of varying complexity across different layers of\\nTapeBert, and compare this to a probing analysis of embeddings and attention weights (see Section 3).\\nAttention targets higher-level properties in deeper layers. As shown in Figure 4, deeper layers\\nfocus relatively more attention on binding sites and contacts (high-level concept), whereas secondary\\nstructure (low- to mid-level concept) is targeted more evenly across layers. The probing analysis\\nof attention (Figure 5, blue) similarly shows that knowledge of contact maps (a pairwise feature)\\n3This head also targets binding sites (Fig. 3a) but at a percentage of 49%.\\n6',\n",
       " 'Published as a conference paper at ICLR 2021\\n24681012\\nHead24681012Layer% Attention\\n0 100%Max\\n0%20%40%60%80%\\n24681012\\nHead24681012Layer% Attention\\n0 20%Max\\n0%5%10%15%20%\\nFigure 6: Percentage of each head’s attention focused on amino acids Pro(left) and Phe(right).\\nACDEFGHIKLMNPQRSTVWYA\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nK\\nL\\nM\\nN\\nP\\nQ\\nR\\nS\\nT\\nV\\nW\\nY0.4\\n0.2\\n0.00.20.40.60.8\\nACDEFGHIKLMNPQRSTVWYA\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nK\\nL\\nM\\nN\\nP\\nQ\\nR\\nS\\nT\\nV\\nW\\nY4\\n3\\n2\\n1\\n01234\\nFigure 7: Pairwise attention similarity (left) vs. substitution matrix (right) (codes in App. C.5)\\nis encoded in attention weights primarily in the last 1-2 layers. These results are consistent with\\nprior work in NLP that suggests deeper layers in text-based Transformers attend to more complex\\nproperties (Vig & Belinkov, 2019) and encode higher-level representations (Raganato & Tiedemann,\\n2018; Peters et al., 2018; Tenney et al., 2019; Jawahar et al., 2019).\\nTheembedding probes (Figure 5, orange) also show that the model ﬁrst builds representations of\\nlocal secondary structure in lower layers before fully encoding binding sites and contact maps in\\ndeeper layers. However, this analysis also reveals stark differences in how knowledge of contact maps\\nis accrued in embeddings, which accumulate this knowledge gradually over many layers, compared\\nto attention weights, which acquire this knowledge only in the ﬁnal layers in this case. This example\\npoints out limitations of common layerwise probing approaches that only consider embeddings,\\nwhich, intuitively, represent what the model knows but not necessarily how it operationalizes that\\nknowledge.\\n4.4 A MINO ACIDS AND THE SUBSTITUTION MATRIX\\nIn addition to high-level structural and functional properties, we also performed a ﬁne-grained\\nanalysis of the interaction between attention and particular amino acids.\\nAttention heads specialize in particular amino acids. We computed the proportion of TapeBert’s\\nattention to each of the 20 standard amino acids, as shown in Figure 6 for two example amino acids.\\nFor 16 of the amino acids, there exists an attention head that focuses over 25% of attention on\\nthat amino acid, signiﬁcantly greater than the background frequencies of the corresponding amino\\nacids, which range from 1.3% to 9.4%. Similar behavior was observed for ProtBert, ProtBert-BFD,\\nProtAlbert, and ProtXLNet models, with 17, 15, 16, and 18 amino acids, respectively, receiving\\ngreater than 25% of the attention from at least one attention head. Detailed results for TapeBert\\nincluding statistical signiﬁcance tests and comparison to a null model are presented in Appendix C.5.\\nAttention is consistent with substitution relationships. A natural follow-up question from the\\nabove analysis is whether each head has “memorized” speciﬁc amino acids to target, or whether it\\nhas actually learned meaningful properties that correlate with particular amino acids. To test the\\nlatter hypothesis, we analyze whether amino acids with similar structural and functional properties\\nare attended to similarly across heads. Speciﬁcally, we compute the Pearson correlation between the\\ndistribution of attention across heads between all pairs of distinct amino acids, as shown in Figure 7\\n(left) for TapeBert. For example, the entry for Pro(P) and Phe(F) is the correlation between the\\ntwo heatmaps in Figure 6. We compare these scores to the BLOSUM62 substitution scores (Sec. 2)\\nin Figure 7 (right), and ﬁnd a Pearson correlation of 0.73, suggesting that attention is moderately\\n7',\n",
       " 'Published as a conference paper at ICLR 2021\\nconsistent with substitution relationships. Similar correlations are observed for the ProtTrans models:\\n0.68 (ProtBert), 0.75 (ProtBert-BFD), 0.60 (ProtAlbert), and 0.71 (ProtXLNet). As a baseline, the\\nrandomized versions of these models (Appendix B.2) yielded correlations of -0.02 (TapeBert), 0.02\\n(ProtBert), -0.03 (ProtBert-BFD), -0.05 (ProtAlbert), and 0.21 (ProtXLNet).\\n5 R ELATED WORK\\n5.1 P ROTEIN LANGUAGE MODELS\\nDeep neural networks for protein language modeling have received broad interest. Early work\\napplied the Skip-gram model (Mikolov et al., 2013) to construct continuous embeddings from protein\\nsequences (Asgari & Mofrad, 2015). Sequence-only language models have since been trained through\\nautoregressive or autoencoding self-supervision objectives for discriminative and generative tasks,\\nfor example, using LSTMs or Transformer-based architectures (Alley et al., 2019; Bepler & Berger,\\n2019; Rao et al., 2019; Rives et al., 2019). TAPE created a benchmark of ﬁve tasks to assess protein\\nsequence models, and ProtTrans also released several large-scale pretrained protein Transformer\\nmodels (Elnaggar et al., 2020). Riesselman et al. (2019); Madani et al. (2020) trained autoregressive\\ngenerative models to predict the functional effect of mutations and generate natural-like proteins.\\nFrom an interpretability perspective, Rives et al. (2019) showed that the output embeddings from\\na pretrained Transformer can recapitulate structural and functional properties of proteins through\\nlearned linear transformations. Various works have analyzed output embeddings of protein models\\nthrough dimensionality reduction techniques such as PCA or t-SNE (Elnaggar et al., 2020; Biswas\\net al., 2020). In our work, we take an interpretability-ﬁrst perspective to focus on the internal model\\nrepresentations, speciﬁcally attention and intermediate hidden states, across multiple protein language\\nmodels. We also explore novel biological properties including binding sites and post-translational\\nmodiﬁcations.\\n5.2 I NTERPRETING MODELS IN NLP\\nThe rise of deep neural networks in ML has also led to much work on interpreting these so-called\\nblack-box models. This section reviews the NLP interpretability literature on the Transformer model,\\nwhich is directly comparable to our work on interpreting Transformer models of protein sequences.\\nInterpreting Transformers. The Transformer is a neural architecture that uses attention to ac-\\ncelerate learning (Vaswani et al., 2017). In NLP, transformers are the backbone of state-of-the-art\\npre-trained language models such as BERT (Devlin et al., 2019). BERTology focuses on interpreting\\nwhat the BERT model learns about language using a suite of probes and interventions (Rogers et al.,\\n2020). So-called diagnostic classiﬁers are used to interpret the outputs from BERT’s layers (Veldhoen\\net al., 2016). At a high level, mechanisms for interpreting BERT can be placed into three main\\ncategories: interpreting the learned embeddings (Ethayarajh, 2019; Wiedemann et al., 2019; Mickus\\net al., 2020; Adi et al., 2016; Conneau et al., 2018), BERT’s learned knowledge of syntax (Lin et al.,\\n2019; Liu et al., 2019; Tenney et al., 2019; Htut et al., 2019; Hewitt & Manning, 2019; Goldberg,\\n2019), and BERT’s learned knowledge of semantics (Tenney et al., 2019; Ettinger, 2020).\\nInterpreting attention speciﬁcally. Interpreting attention on textual sequences is a well-\\nestablished area of research (Wiegreffe & Pinter, 2019; Zhong et al., 2019; Brunner et al., 2020;\\nHewitt & Manning, 2019). Past work has been shown that attention correlates with syntactic and\\nsemantic relationships in natural language in some cases (Clark et al., 2019; Vig & Belinkov, 2019;\\nHtut et al., 2019). Depending on the task and model architecture, attention may have less or more\\nexplanatory power for model predictions (Jain & Wallace, 2019; Serrano & Smith, 2019; Pruthi et al.,',\n",
       " 'Htut et al., 2019). Depending on the task and model architecture, attention may have less or more\\nexplanatory power for model predictions (Jain & Wallace, 2019; Serrano & Smith, 2019; Pruthi et al.,\\n2020; Moradi et al., 2019; Vashishth et al., 2019). Visualization techniques have been used to convey\\nthe structure and properties of attention in Transformers (Vaswani et al., 2017; Kovaleva et al., 2019;\\nHoover et al., 2020; Vig, 2019). Recent work has begun to analyze attention in Transformer models\\noutside of the domain of natural language (Schwaller et al., 2020; Payne et al., 2020).\\nOur work extends these methods to protein sequence models by considering particular biophysical\\nproperties and relationships. We also present a joint cross-layer probing analysis of attention weights\\nand layer embeddings. While past work in NLP has analyzed attention and embeddings across layers,\\nwe believe we are the ﬁrst to do so in any domain using a single, uniﬁed metric, which enables us to\\n8',\n",
       " 'Published as a conference paper at ICLR 2021\\ndirectly compare the relative information content of the two representations. Finally, we present a\\nnovel tool for visualizing attention embedded in three-dimensional structure.\\n6 C ONCLUSIONS AND FUTURE WORK\\nThis paper builds on the synergy between NLP and computational biology by adapting and extending\\nNLP interpretability methods to protein sequence modeling. We show how a Transformer language\\nmodel recovers structural and functional properties of proteins and integrates this knowledge directly\\ninto its attention mechanism. While this paper focuses on reconciling attention with known properties\\nof proteins, one might also leverage attention to uncover novel relationships or more nuanced forms\\nof existing measures such as contact maps, as discussed in Section 4.1. In this way, language models\\nhave the potential to serve as tools for scientiﬁc discovery. But in order for learned representations\\nto be accessible to domain experts, they must be presented in an appropriate context to facilitate\\ndiscovery. Visualizing attention in the context of protein structure (Figure 1) is one attempt to do so.\\nWe believe there is the potential to develop such contextual visualizations of learned representations\\nin a range of scientiﬁc domains.\\nACKNOWLEDGMENTS\\nWe would like to thank Xi Victoria Lin, Stephan Zheng, Melvin Gruesbeck, and the anonymous\\nreviewers for their valuable feedback.\\nREFERENCES\\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained analysis\\nof sentence embeddings using auxiliary prediction tasks. arXiv:1608.04207 [cs.CL]., 2016.\\nEthan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church.\\nUniﬁed rational protein engineering with sequence-based deep representation learning. Nature\\nMethods , 16(12):1315–1322, 2019.\\nMohammed AlQuraishi. ProteinNet: a standardized data set for machine learning of protein structure.\\nBMC Bioinformatics , 20, 2019.\\nEhsaneddin Asgari and Mohammad RK Mofrad. Continuous distributed representation of biological\\nsequences for deep proteomics and genomics. PLOS One , 10(11), 2015.\\nTristan Bepler and Bonnie Berger. Learning protein sequence embeddings using information from\\nstructure. In International Conference on Learning Representations , 2019.\\nHelen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge Weissig,\\nIlya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic Acids Research , 28(1):\\n235–242, 2000.\\nSurojit Biswas, Grigory Khimulya, Ethan C. Alley, Kevin M. Esvelt, and George M. Church.\\nLow-n protein engineering with data-efﬁcient deep learning. bioRxiv , 2020. doi: 10.1101/\\n2020.01.23.917682. URL https://www.biorxiv.org/content/early/2020/08/\\n31/2020.01.23.917682 .\\nAshraf Brik and Chi-Huey Wong. HIV-1 protease: Mechanism and drug discovery. Organic &\\nBiomolecular Chemistry , 1(1):5–14, 2003.\\nGino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Watten-\\nhofer. On identiﬁability in Transformers. In International Conference on Learning Representations ,\\n2020. URL https://openreview.net/forum?id=BJg1f6EFDB .\\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look\\nat? An analysis of BERT’s attention. In BlackBoxNLP@ACL , 2019.\\n9',\n",
       " 'Published as a conference paper at ICLR 2021\\nAlexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. What\\nyou can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties.\\nInProceedings of the 56th Annual Meeting of the Association for Computational Linguistics , pp.\\n2126–2136, 2018.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) , pp. 4171–4186, Minneapolis, Minnesota, 2019.\\nAssociation for Computational Linguistics.\\nSara El-Gebali, Jaina Mistry, Alex Bateman, Sean R. Eddy, Aurélien Luciani, Simon C. Potter,\\nMatloob Qureshi, Lorna J. Richardson, Gustavo A. Salazar, Alfredo Smart, Erik L. L. Sonnhammer,\\nLayla Hirsh, Lisanna Paladin, Damiano Piovesan, Silvio C. E. Tosatto, and Robert D. Finn. The\\nPfam protein families database in 2019. Nucleic Acids Research , 47(D1):D427–D432, January\\n2019a. doi: 10.1093/nar/gky995.\\nSara El-Gebali, Jaina Mistry, Alex Bateman, Sean R Eddy, Aurélien Luciani, Simon C Potter, Matloob\\nQureshi, Lorna J Richardson, Gustavo A Salazar, Alfredo Smart, Erik L L Sonnhammer, Layla\\nHirsh, Lisanna Paladin, Damiano Piovesan, Silvio C E Tosatto, and Robert D Finn. The Pfam\\nprotein families database in 2019. Nucleic Acids Research , 47(D1):D427–D432, 2019b. ISSN\\n0305-1048. doi: 10.1093/nar/gky995.\\nAhmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom\\nGibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and Burkhard\\nRost. ProtTrans: Towards cracking the language of life’s code through self-supervised deep\\nlearning and high performance computing. arXiv preprint arXiv:2007.06225 , 2020.\\nKawin Ethayarajh. How contextual are contextualized word representations? Comparing the geometry\\nof BERT, ELMo, and GPT-2 embeddings. In Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP) , pp. 55–65, Hong Kong, China, 2019. Association for\\nComputational Linguistics.\\nAllyson Ettinger. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for\\nlanguage models. Transactions of the Association for Computational Linguistics , 8:34–48, 2020.\\nNaomi K Fox, Steven E Brenner, and John-Marc Chandonia. SCOPe: Structural classiﬁcation of\\nproteins—extended, integrating scop and astral data and classiﬁcation of new structures. Nucleic\\nAcids Research , 42(D1):D304–D309, 2013.\\nYoav Goldberg. Assessing BERT’s syntactic abilities. arXiv preprint arXiv:1901.05287 , 2019.\\nChristopher Grimsley, Elijah Mayﬁeld, and Julia R.S. Bursten. Why attention is not explanation:\\nSurgical intervention and causal reasoning about neural models. In Proceedings of The 12th\\nLanguage Resources and Evaluation Conference , pp. 1780–1790, Marseille, France, May 2020.\\nEuropean Language Resources Association. ISBN 979-10-95546-34-4. URL https://www.\\naclweb.org/anthology/2020.lrec-1.220 .\\nS Henikoff and J G Henikoff. Amino acid substitution matrices from protein blocks. Proceedings of\\nthe National Academy of Sciences , 89(22):10915–10919, 1992. ISSN 0027-8424. doi: 10.1073/\\npnas.89.22.10915. URL https://www.pnas.org/content/89/22/10915 .\\nJohn Hewitt and Christopher D Manning. A structural probe for ﬁnding syntax in word representations.\\nInProceedings of the 2019 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) ,\\npp. 4129–4138, 2019.\\nBenjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exBERT: A Visual Analysis Tool\\nto Explore Learned Representations in Transformer Models. In Proceedings of the 58th Annual',\n",
       " 'Benjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exBERT: A Visual Analysis Tool\\nto Explore Learned Representations in Transformer Models. In Proceedings of the 58th Annual\\nMeeting of the Association for Computational Linguistics: System Demonstrations , pp. 187–196.\\nAssociation for Computational Linguistics, 2020.\\n10',\n",
       " 'Published as a conference paper at ICLR 2021\\nPhu Mon Htut, Jason Phang, Shikha Bordia, and Samuel R Bowman. Do attention heads in BERT\\ntrack syntactic dependencies? arXiv preprint arXiv:1911.12246 , 2019.\\nSarthak Jain and Byron C. Wallace. Attention is not Explanation. In Proceedings of the 2019\\nConference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long and Short Papers) , pp. 3543–3556, June 2019.\\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah. What does BERT learn about the structure of\\nlanguage? In ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics ,\\nFlorence, Italy, July 2019. URL https://hal.inria.fr/hal-02131630 .\\nAkira Kinjo and Haruki Nakamura. Comprehensive structural classiﬁcation of ligand-binding motifs\\nin proteins. Structure , 17(2), 2009.\\nMichael Schantz Klausen, Martin Closter Jespersen, Henrik Nielsen, Kamilla Kjaergaard Jensen,\\nVanessa Isabell Jurtz, Casper Kaae Soenderby, Morten Otto Alexander Sommer, Ole Winther,\\nMorten Nielsen, Bent Petersen, et al. NetSurfP-2.0: Improved prediction of protein structural\\nfeatures by integrated deep learning. Proteins: Structure, Function, and Bioinformatics , 2019.\\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets\\nof BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\\nIJCNLP) , pp. 4365–4374, Hong Kong, China, 2019. Association for Computational Linguistics.\\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black, and Yulia Tsvetkov. Measuring bias in\\ncontextualized word representations. In Proceedings of the First Workshop on Gender Bias in\\nNatural Language Processing , pp. 166–172, Florence, Italy, 2019. Association for Computational\\nLinguistics.\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\\nAlbert: A lite bert for self-supervised learning of language representations. In International\\nConference on Learning Representations , 2020.\\nJuyong Lee, Janez Konc, Dusanka Janezic, and Bernard Brooks. Global organization of a binding\\nsite network gives insight into evolution and structure-function relationships of proteins. Sci Rep , 7\\n(11652), 2017.\\nYongjie Lin, Yi Chern Tan, and Robert Frank. Open sesame: Getting inside BERT’s linguistic\\nknowledge. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP , pp. 241–253, Florence, Italy, 2019. Association for Computational\\nLinguistics.\\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic\\nknowledge and transferability of contextual representations. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) , pp. 1073–1094. Association for Computational\\nLinguistics, 2019.\\nAli Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R. Eguchi,\\nPo-Ssu Huang, and Richard Socher. Progen: Language modeling for protein generation. arXiv\\npreprint arXiv:2004.03497 , 2020.\\nTimothee Mickus, Mathieu Constant, Denis Paperno, and Kees Van Deemter. What do you mean,\\nBERT? Assessing BERT as a Distributional Semantics Model. Proceedings of the Society for\\nComputation in Linguistics , 3, 2020.\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations\\nof words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling,\\nZ. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems\\n26, pp. 3111–3119. Curran Associates, Inc., 2013.\\n11',\n",
       " \"Published as a conference paper at ICLR 2021\\nPooya Moradi, Nishant Kambhatla, and Anoop Sarkar. Interrogating the explanatory power of\\nattention in neural machine translation. In Proceedings of the 3rd Workshop on Neural Generation\\nand Translation , pp. 221–230, Hong Kong, November 2019. Association for Computational\\nLinguistics. doi: 10.18653/v1/D19-5624. URL https://www.aclweb.org/anthology/\\nD19-5624 .\\nJohn Moult, Krzysztof Fidelis, Andriy Kryshtafovych, Torsten Schwede, and Anna Tramontano.\\nCritical assessment of methods of protein structure prediction (CASP)-Round XII. Proteins:\\nStructure, Function, and Bioinformatics , 86:7–15, 2018. ISSN 08873585. doi: 10.1002/prot.25415.\\nURLhttp://doi.wiley.com/10.1002/prot.25415 .\\nHai Nguyen, David A Case, and Alexander S Rose. NGLview–interactive molecular graphics for\\nJupyter notebooks. Bioinformatics , 34(7):1241–1242, 12 2017. ISSN 1367-4803. doi: 10.1093/\\nbioinformatics/btx789. URL https://doi.org/10.1093/bioinformatics/btx789 .\\nTimothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language\\narguments. In Proceedings of the 57th Annual Meeting of the Association for Computational\\nLinguistics , pp. 4658–4664, Florence, Italy, 2019. Association for Computational Linguistics.\\nJosh Payne, Mario Srouji, Dian Ang Yap, and Vineet Kosaraju. Bert learns (and teaches) chemistry.\\narXiv preprint arXiv:2007.16012 , 2020.\\nMatthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual\\nword embeddings: Architecture and representation. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing , pp. 1499–1509, Brussels, Belgium, October-\\nNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1179. URL\\nhttps://www.aclweb.org/anthology/D18-1179 .\\nDanish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C. Lipton. Learning to\\ndeceive with attention-based explanations. In Annual Conference of the Association for Computa-\\ntional Linguistics (ACL) , July 2020. URL https://arxiv.org/abs/1909.07913 .\\nAlessandro Raganato and Jörg Tiedemann. An analysis of encoder representations in Transformer-\\nbased machine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing\\nand Interpreting Neural Networks for NLP , pp. 287–297, Brussels, Belgium, November 2018.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/W18-5431. URL https://www.\\naclweb.org/anthology/W18-5431 .\\nRoshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel,\\nand Yun S Song. Evaluating protein transfer learning with TAPE. In Advances in Neural\\nInformation Processing Systems , 2019.\\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, and\\nBeen Kim. Visualizing and measuring the geometry of BERT. In H. Wallach, H. Larochelle,\\nA. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information\\nProcessing Systems , volume 32, pp. 8594–8603. Curran Associates, Inc., 2019.\\nAdam J Riesselman, Jung-Eun Shin, Aaron W Kollasch, Conor McMahon, Elana Simon, Chris\\nSander, Aashish Manglik, Andrew C Kruse, and Debora S Marks. Accelerating protein design\\nusing autoregressive generative models. bioRxiv , pp. 757252, 2019.\\nAlexander Rives, Siddharth Goyal, Joshua Meier, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry\\nMa, and Rob Fergus. Biological structure and function emerge from scaling unsupervised learning\\nto 250 million protein sequences. bioRxiv , pp. 622803, 2019.\\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in BERTology: What we know about\\nhow BERT works. Transactions of the Association for Computational Linguistics , 8:842–866,\\n2020.\\nNathan J Rollins, Kelly P Brock, Frank J Poelwijk, Michael A Stifﬂer, Nicholas P Gauthier, Chris\\nSander, and Debora S Marks. Inferring protein 3D structure from deep mutation scans. Nature\\nGenetics , 51(7):1170, 2019.\\n12\",\n",
       " 'Published as a conference paper at ICLR 2021\\nAlexander S. Rose and Peter W. Hildebrand. NGL Viewer: a web application for molecular vi-\\nsualization. Nucleic Acids Research , 43(W1):W576–W579, 04 2015. ISSN 0305-1048. doi:\\n10.1093/nar/gkv402. URL https://doi.org/10.1093/nar/gkv402 .\\nAlexander S Rose, Anthony R Bradley, Yana Valasatava, Jose M Duarte, Andreas Prli ´c, and Peter W\\nRose. NGL viewer: web-based molecular graphics for large complexes. Bioinformatics , 34(21):\\n3755–3758, 05 2018. ISSN 1367-4803. doi: 10.1093/bioinformatics/bty419. URL https:\\n//doi.org/10.1093/bioinformatics/bty419 .\\nCharles Rubin and Ora Rosen. Protein phosphorylation. Annual Review of Biochemistry , 44:831–887,\\n1975. URL https://doi.org/10.1146/annurev.bi.44.070175.004151 .\\nPhilippe Schwaller, Benjamin Hoover, Jean-Louis Reymond, Hendrik Strobelt, and Teodoro\\nLaino. Unsupervised attention-guided atom-mapping. ChemRxiv , 5 2020. doi: 10.26434/\\nchemrxiv.12298559.v1. URL https://chemrxiv.org/articles/Unsupervised_\\nAttention-Guided_Atom-Mapping/12298559 .\\nSoﬁa Serrano and Noah A. Smith. Is attention interpretable? In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics , pp. 2931–2951, Florence, Italy, July\\n2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1282. URL https:\\n//www.aclweb.org/anthology/P19-1282 .\\nMartin Steinegger and Johannes Söding. Clustering huge protein sequence sets in linear time. Nature\\nCommunications , 9(2542), 2018. doi: 10.1038/s41467-018-04964-5.\\nBaris E. Suzek, Yuqi Wang, Hongzhan Huang, Peter B. McGarvey, Cathy H. Wu, and the UniProt Con-\\nsortium. UniRef clusters: a comprehensive and scalable alternative for improving sequence\\nsimilarity searches. Bioinformatics , 31(6):926–932, 11 2014. ISSN 1367-4803. doi: 10.1093/\\nbioinformatics/btu739. URL https://doi.org/10.1093/bioinformatics/btu739 .\\nYi Chern Tan and L. Elisa Celis. Assessing social and intersectional biases in contextualized word\\nrepresentations. In Advances in Neural Information Processing Systems 32 , pp. 13230–13241.\\nCurran Associates, Inc., 2019.\\nIan Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In\\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp.\\n4593–4601, Florence, Italy, 2019. Association for Computational Linguistics.\\nShikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. Attention inter-\\npretability across NLP tasks. arXiv preprint arXiv:1909.11218 , 2019.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\\nProcessing Systems , pp. 5998–6008, 2017.\\nSara Veldhoen, Dieuwke Hupkes, and Willem H. Zuidema. Diagnostic classiﬁers revealing how\\nneural networks process hierarchical structure. In CoCo@NIPS , 2016.\\nJesse Vig. A multiscale visualization of attention in the Transformer model. In Proceedings of the\\n57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations ,\\npp. 37–42, Florence, Italy, 2019. Association for Computational Linguistics.\\nJesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a Transformer language\\nmodel. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP , pp. 63–76, Florence, Italy, 2019. Association for Computational\\nLinguistics.\\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and\\nStuart Shieber. Investigating gender bias in language models using causal mediation analysis. In\\nAdvances in Neural Information Processing Systems , volume 33, pp. 12388–12401, 2020.\\nGregor Wiedemann, Steffen Remus, Avi Chawla, and Chris Biemann. Does BERT make any\\nsense? Interpretable word sense disambiguation with contextualized embeddings. arXiv preprint\\narXiv:1909.10430 , 2019.\\n13',\n",
       " \"Published as a conference paper at ICLR 2021\\nSarah Wiegreffe and Yuval Pinter. Attention is not not explanation. In Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint\\nConference on Natural Language Processing (EMNLP-IJCNLP) , pp. 11–20, November 2019.\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V\\nLe. XLNet: Generalized autoregressive pretraining for language understanding. In H. Wallach,\\nH. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural\\nInformation Processing Systems , volume 32, pp. 5753–5763. Curran Associates, Inc., 2019.\\nRuiqi Zhong, Steven Shao, and Kathleen McKeown. Fine-grained sentiment analysis with faithful\\nattention. arXiv preprint arXiv:1908.06870 , 2019.\\n14\",\n",
       " 'Published as a conference paper at ICLR 2021\\nA M ODEL OVERVIEW\\nA.1 P RE-TRAINED MODELS\\nTable 1 provides an overview of the ﬁve pre-trained Transformer models studied in this work. The\\nmodels originate from the TAPE and ProtTrans repositories, spanning three model architectures:\\nBERT, ALBERT, and XLNet.\\nTable 1: Summary of pre-trained models analyzed, including the source of the model, the type of\\nTransformer used, the number of layers and heads, the total number of model parameters, the source\\nof the pre-training dataset, and the number of protein sequences in the pre-training dataset.\\nSource Name Type Layers Heads Params Train Dataset # Seq\\nTAPE TapeBert BERT 12 12 94M Pfam 31M\\nProtTrans ProtBert BERT 30 16 420M Uniref100 216M\\nProtTrans ProtBert-BFD BERT 30 16 420M BFD 2.1B\\nProtTrans ProtAlbert ALBERT 12 64 224M Uniref100 216M\\nProtTrans ProtXLNet XLNet 30 16 409M Uniref100 216M\\nA.2 BERT T RANSFORMER ARCHITECTURE\\nStacked Encoder: BERT uses a stacked-encoder architecture, which inputs a sequence of tokens\\nx= (x1,...,xn)and applies position and token embeddings followed by a series of encoder\\nlayers. Each layer applies multi-head self-attention (see below) in combination with a feedforward\\nnetwork, layer normalization, and residual connections. The output of each layer ℓis a sequence of\\ncontextualized embeddings (h(ℓ)\\n1,...,h(ℓ)\\nn).\\nSelf-Attention: Given an input x= (x1,...,xn), the self-attention mechanism assigns to each\\ntoken pairi,jan attention weight αi,j>0where∑\\njαi,j= 1. Attention in BERT is bidirectional.\\nIn the multi-layer, multi-head setting, αis speciﬁc to a layer and head. The BERT-Base model has 12\\nlayers and 12 heads. Each attention head learns a distinct set of weights, resulting in 12 x 12 = 144\\ndistinct attention mechanisms in this case.\\nThe attention weights αi,jare computed from the scaled dot-product of the query vector ofiand the\\nkey vector ofj, followed by a softmax operation. The attention weights are then used to produce a\\nweighted sum of value vectors:\\nAttention (Q,K,V ) =softmax(QKT\\n√dk)\\nV (2)\\nusing query matrix Q, key matrix K, and value matrix V, wheredkis the dimension of K. In a\\nmulti-head setting, the queries, keys, and values are linearly projected htimes, and the attention\\noperation is performed in parallel for each representation, with the results concatenated.\\nA.3 O THER TRANSFORMER VARIANTS\\nALBERT: The architecture of ALBERT differs from BERT in two ways: (1) It shares parameters\\nacross layers, unlike BERT which learns distinct parameters for every layer and (2) It uses factorized\\nembeddings, which allows the input token embeddings to be of a different (smaller) size than the\\nhidden states. The original version of ALBERT designed for text also employed a sentence-order\\nprediction pretraining task, but this was not used on the models studied in this paper.\\nXLNet: Instead of the masked-language modeling pretraining objective use for BERT, XLNet uses\\na bidirectional auto-regressive pretraining method that considers all possible orderings of the input\\nfactorization. The architecture also adds a segment recurrence mechanism to process long sequences,\\nas well as a relative rather than absolute encoding scheme.\\n15',\n",
       " 'Published as a conference paper at ICLR 2021\\nB A DDITIONAL EXPERIMENTAL DETAILS\\nB.1 A LTERNATIVE ATTENTION AGREEMENT METRIC\\nHere we present an alternative formulation to Eq. 1 based on an attention-weighted average. We\\ndeﬁne an indicator function f(i,j)for property fthat returns 1 if the property is present in token pair\\n(i,j)(i.e., if amino acids iandjare in contact), and zero otherwise. We then compute the proportion\\nof attention that matches with fover a dataset Xas follows:\\npα(f) =∑\\nx∈X|x|∑\\ni=1|x|∑\\nj=1f(i,j)αi,j(x)/∑\\nx∈X|x|∑\\ni=1|x|∑\\nj=1αi,j(x) (3)\\nwhereαi,j(x)denotes the attention from itojfor input sequence x.\\nB.2 S TATISTICAL SIGNIFICANCE TESTING AND NULL MODELS\\nWe perform statistical signiﬁcance tests to determine whether any results based on the metric deﬁned\\nin Equation 1 are due to chance. Given a property f, as deﬁned in Section 3, we perform a two-\\nproportion z-test comparing (1) the proportion of high-conﬁdence attention arcs ( αi,j>θ) for which\\nf(i,j) = 1 , and (2) the proportion of all possible pairs i,jfor whichf(i,j) = 1 . Note that the ﬁrst\\nproportion is exactly the metric pα(f)deﬁned in Equation 1 (e.g. the proportion of attention aligned\\nwith contact maps). The second proportion is simply the background frequency of the property (e.g.\\nthe background frequency of contacts). Since we extract the maximum scores over all of the heads in\\nthe model, we treat this as a case of multiple hypothesis testing and apply the Bonferroni correction,\\nwith the number of hypotheses mequal to the number of attention heads.\\nAs an additional check that the results did not occur by chance, we also report results on baseline\\n(null) models. We initially considered using two forms of null models: (1) a model with randomly\\ninitialized weights. and (2) a model trained on randomly shufﬂed sequences. However, in both cases,\\nnone of the sequences in the dataset yielded attention weights greater than the attention threshold θ.\\nThis suggests that the mere existence of the high-conﬁdence attention weights used in the analysis\\ncould not have occurred by chance, but it does not shed light on the particular analyses performed.\\nTherefore, we implemented an alternative randomization scheme in which we randomly shufﬂe\\nattention weights from the original models as a post-processing step. Speciﬁcally, we permute the\\nsequence of attention weights from each token for every attention head. To illustrate, let’s say that\\nthe original model produced attention weights of (0.3, 0.2, 0.1, 0.4, 0.0) from position iin protein\\nsequencexfrom headh, where|x|= 5. In the null model, the attention weights from position iin\\nsequencexin headhwould be a random permutation of those weights, e.g., (0.2, 0.0, 0.4, 0.3, 0.1).\\nNote that these are still valid attention weights as they would sum to 1 (since the original weights\\nwould sum to 1 by deﬁnition). We report results using this form of baseline model.\\nB.3 P ROBING METHODOLOGY\\nEmbedding probe. We probe the embedding vectors output from each layer using a linear probing\\nclassiﬁer. For token-level probing tasks (binding sites, secondary structure) we feed each token’s\\noutput vector directly to the classiﬁer. For token-pair probing tasks (contact map) we construct a\\npairwise feature vector by concatenating the elementwise differences and products of the two tokens’\\noutput vectors, following the TAPE4implementation.\\nWe use task-speciﬁc evaluation metrics for the probing classiﬁer: for secondary structure prediction,\\nwe measure F1 score; for contact prediction, we measure precision@ L/5, whereLis the length of\\nthe protein sequence, following standard practice (Moult et al., 2018); for binding site prediction,\\nwe measure precision@ L/20, since approximately one in twenty amino acids in each sequence is a\\nbinding site (4.8% in the dataset).\\nAttention probe. Just as the attention weight αi,jis deﬁned for a pair of amino acids (i,j), so is',\n",
       " 'binding site (4.8% in the dataset).\\nAttention probe. Just as the attention weight αi,jis deﬁned for a pair of amino acids (i,j), so is\\nthe contact property f(i,j), which returns true if amino acids iandjare in contact. Treating the\\nattention weight as a feature of a token-pair (i,j), we can train a probing classiﬁer that predicts the\\n4https://github.com/songlab-cal/tape\\n16',\n",
       " 'Published as a conference paper at ICLR 2021\\ncontact property based on this feature, thereby quantifying the attention mechanism’s knowledge of\\nthat property. In our multi-head setting, we treat the attention weights across all heads in a given layer\\nas a feature vector, and use a probing classiﬁer to assess the knowledge of a given property in the\\nattention weights across the entire layer. As with the embedding probe, we measure performance of\\nthe probing classiﬁer using precision@ L/5, whereLis the length of the protein sequence, following\\nstandard practice for contact prediction.\\nB.4 D ATASETS\\nWe used two protein sequence datasets from the TAPE repository for the analysis: the ProteinNet\\ndataset (AlQuraishi, 2019; Fox et al., 2013; Berman et al., 2000; Moult et al., 2018) and the Secondary\\nStructure dataset (Rao et al., 2019; Berman et al., 2000; Moult et al., 2018; Klausen et al., 2019). The\\nformer was used for analysis of amino acids and contact maps, and the latter was used for analysis of\\nsecondary structure. We additionally created a third dataset for binding site and post-translational\\nmodiﬁcation (PTM) analysis from the Secondary Structure dataset, which was augmented with\\nbinding site and PTM annotations obtained from the Protein Data Bank’s Web API.5We excluded\\nany sequences for which annotations were not available. The resulting dataset sizes are shown in\\nTable 2. For the analysis of attention, a random subset of 5000 sequences from the training split\\nof each dataset was used, as the analysis was purely evaluative. For training and evaluating the\\ndiagnostic classiﬁer, the full training and validation splits were used.\\nTable 2: Datasets used in analysis\\nDataset Train size Validation size\\nProteinNet 25299 224\\nSecondary Structure 8678 2170\\nBinding Sites / PTM 5734 1418\\nC A DDITIONAL RESULTS OF ATTENTION ANALYSIS\\nC.1 S ECONDARY STRUCTURE\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n15%30%45%60%\\n(a) TapeBert\\n246810121416182022242628303234363840424446485052545658606264\\nHead24681012Layer% Attention\\n050%Max\\n0%20%40%60% (b) ProtAlbert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n0%20%40%60%80%\\n(c) ProtBert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n20%40%60%80% (d) ProtBert-BFD\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n0%20%40%60% (e) ProtXLNet\\nFigure 8: Percentage of each head’s attention that is focused on Helix secondary structure.\\n5http://www.rcsb.org/pdb/software/rest.do\\n17',\n",
       " 'Published as a conference paper at ICLR 2021\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n15%30%45%60%75%\\n(a) TapeBert\\n246810121416182022242628303234363840424446485052545658606264\\nHead24681012Layer% Attention\\n050%Max\\n0%20%40%60%80% (b) ProtAlbert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n0%20%40%60%80%\\n(c) ProtBert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n20%40%60%80% (d) ProtBert-BFD\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n0%15%30%45%60% (e) ProtXLNet\\nFigure 9: Percentage of each head’s attention that is focused on Strand secondary structure.\\n24681012\\nHead24681012Layer% Attention\\n0 50%Max\\n15%30%45%\\n(a) TapeBert\\n246810121416182022242628303234363840424446485052545658606264\\nHead24681012Layer% Attention\\n0 50%Max\\n0%15%30%45%60% (b) ProtAlbert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n0 50%Max\\n0%15%30%45%60%\\n(c) ProtBert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n050%Max\\n20%40%60% (d) ProtBert-BFD\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n025%Max\\n0%10%20%30%40% (e) ProtXLNet\\nFigure 10: Percentage of each head’s attention that is focused on Turn/Bend secondary structure.\\n18',\n",
       " 'Published as a conference paper at ICLR 2021\\nC.2 C ONTACT MAPS: STATISTICAL SIGNIFICANCE TESTS AND NULL MODELS\\n12-412-12 12-1112-28-512-79-76-119-812-5\\nTop heads02040Attention %\\n(a) TapeBert\\n11-3 12-3 10-3 1-119-510-5 9-31 8-31 3-31 9-13\\nTop heads0204060Attention %\\n (b) ProtAlbert\\n30-1429-44-419-74-8\\n30-10 19-1020-7 20-23-1\\nTop heads0204060Attention %\\n(c) ProtBert\\n30-9 30-8 29-429-1530-5 30-330-1210-9 4-11 29-7\\nTop heads0204060Attention %\\n (d) ProtBert-BFD\\n26-627-158-4 4-529-38-86-14 18-128-1025-2\\nTop heads02040Attention %\\n(e) ProtXLNet\\nFigure 11: Top 10 heads (denoted by <layer>-<head> ) for each model based on the proportion of\\nattention aligned with contact maps [95% conf. intervals]. The differences between the attention\\nproportions and the background frequency of contacts (orange dashed line) are statistically signiﬁcant\\n(p< 0.00001 ). Bonferroni correction applied for both conﬁdence intervals and tests (see App. B.2).\\n11-612-119-79-11 10-2 11-812-123-7 4-710-9\\nTop heads0246Attention %\\n(a) TapeBert-Random\\n12-201-16 1-47 1-11 2-55 1-40 1-25 7-55 3-55 6-55\\nTop heads051015Attention %\\n (b) ProtAlbert-Random\\n4-36-16 7-15 5-14 10-21-4\\n21-135-111-725-13\\nTop heads024Attention %\\n(c) ProtBert-Random\\n6-1612-12 11-108-164-5 5-31-15 11-9 10-2 4-16\\nTop heads0510Attention %\\n (d) ProtBert-BFD-Random\\n4-5 8-8 8-48-133-918-1 13-7 2-12 30-43-8\\nTop heads0246Attention %\\n(e) ProtXLNet-Random\\nFigure 12: Top-10 contact-aligned heads for null models. See Appendix B.2 for details.\\n19',\n",
       " 'Published as a conference paper at ICLR 2021\\nC.3 B INDING SITES : STATISTICAL SIGNIFICANCE TESTS AND NULL MODEL\\n11-611-127-18-127-610-3 11-38-4 5-7 7-7\\nTop heads02040Attention %\\n(a) TapeBert\\n2-26 3-29 2-16 4-29 6-17 5-17 7-17 3-17 8-17 2-17\\nTop heads0204060Attention %\\n (b) ProtAlbert\\n24-721-1223-325-10 24-15 21-1426-2 4-1324-115-10\\nTop heads02040Attention %\\n(c) ProtBert\\n26-125-1425-4 26-226-1324-1 26-426-12 27-1523-1\\nTop heads02040Attention %\\n (d) ProtBert-BFD\\n11-169-912-3 14-6 16-913-12 19-13 17-1017-6 14-8\\nTop heads051015Attention %\\n(e) ProtXLNet\\nFigure 13: Top 10 heads (denoted by <layer>-<head> ) for each model based on the proportion of\\nattention focused on binding sites [95% conf. intervals]. Differences between attention proportions\\nand the background frequency of binding sites (orange dashed line) are all statistically signiﬁcant\\n(p< 0.00001 ). Bonferroni correction applied for both conﬁdence intervals and tests (see App. B.2).\\n11-3 11-811-12 12-113-612-7 11-1 6-119-4 9-5\\nTop heads0510Attention %\\n(a) TapeBert-Random\\n2-5911-60 12-33 11-33 11-18 11-199-42 2-6212-47 10-60\\nTop heads0204060Attention %\\n (b) ProtAlbert-Random\\n5-426-24-2\\n27-1527-6 25-6 23-322-136-36-11\\nTop heads0510Attention %\\n(c) ProtBert-Random\\n6-8\\n26-12 13-14 11-1325-2 24-111-1613-526-13 13-13\\nTop heads02040Attention %\\n (d) ProtBert-BFD-Random\\n22-8 22-317-10 19-13 18-13 14-1116-98-4\\n23-14 11-14\\nTop heads0510Attention %\\n(e) ProtXLNet-Random\\nFigure 14: Top-10 heads most focused on binding sites for null models. See Appendix B.2 for details.\\n20',\n",
       " 'Published as a conference paper at ICLR 2021\\nC.4 P OST-TRANSLATIONAL MODIFICATIONS (PTM S)\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n15%30%45%60%\\n(a) TapeBert\\n246810121416182022242628303234363840424446485052545658606264\\nHead24681012Layer% Attention\\n025%Max\\n0%10%20%30%40% (b) ProtAlbert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n025%Max\\n0%8%16%24%32%\\n(c) ProtBert\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n025%Max\\n0%10%20%30%40% (d) ProtBert-BFD\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nHead24681012141618202224262830Layer% Attention\\n0 5%Max\\n0%2%3%4%6% (e) ProtXLNet\\nFigure 15: Percentage of each head’s attention that is focused on post-translational modiﬁcations.\\n21',\n",
       " 'Published as a conference paper at ICLR 2021\\n11-69-7\\n11-1211-3 10-34-7 5-8 9-4 5-19-10\\nTop heads0204060Attention %\\n(a) TapeBert\\n2-28 1-11 2-19 2-3410-564-29 3-34 3-2910-176-17\\nTop heads02040Attention %\\n (b) ProtAlbert\\n26-225-1310-2 24-79-4\\n24-155-14 23-321-127-3\\nTop heads02040Attention %\\n(c) ProtBert\\n12-128-9\\n25-1426-1 24-127-11 26-14 26-1226-426-15\\nTop heads02040Attention %\\n (d) ProtBert-BFD\\n11-1613-712-16 13-1214-6 30-4 11-3 15-422-1411-5\\nTop heads0.02.55.07.5Attention %\\n(e) ProtXLNet\\nFigure 16: Top 10 heads (denoted by <layer>-<head> ) for each model based on the proportion of\\nattention focused on PTM positions [95% conf. intervals]. The differences between the attention\\nproportions and the background frequency of PTMs (orange dashed line) are statistically signiﬁcant\\n(p< 0.00001 ). Bonferroni correction applied for both conﬁdence intervals and tests (see App. B.2).\\n11-69-711-1 10-3 6-113-2 5-1 4-710-7 9-11\\nTop heads0246Attention %\\n(a) TapeBert-Random\\n1-57 1-40 3-55 4-55 1-55 1-25 1-36 5-55 1-11 7-55\\nTop heads01020Attention %\\n (b) ProtAlbert-Random\\n5-4\\n29-1429-2 16-5 29-91-425-627-13 26-1226-2\\nTop heads0510Attention %\\n(c) ProtBert-Random\\n6-8 8-9\\n12-1210-5 11-111-168-107-6\\n29-133-3\\nTop heads020Attention %\\n (d) ProtBert-BFD-Random\\n30-430-1513-714-12 22-14 21-1410-4 18-3 19-315-10\\nTop heads024Attention %\\n(e) ProtXLNet-Random\\nFigure 17: Top-10 heads most focused on PTMs for null models. See Appendix B.2 for details.\\n22',\n",
       " 'Published as a conference paper at ICLR 2021\\nC.5 A MINO ACIDS\\n24681012\\nHead24681012Layer% Attention\\n0 25%Max\\n0%6%12%18%24%\\n(a) ALA\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n0%15%30%45%60% (b) ARG\\n24681012\\nHead24681012Layer% Attention\\n025%Max\\n0%10%20%30%40% (c) ASN\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n0%20%40%60%\\n(d) ASP\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n0%20%40%60%80% (e) CYS\\n24681012\\nHead24681012Layer% Attention\\n05%Max\\n0%2%3%4%6% (f) GLN\\n24681012\\nHead24681012Layer% Attention\\n010%Max\\n0%4%8%12%16%\\n(g) GLU\\n24681012\\nHead24681012Layer% Attention\\n0 100%Max\\n0%20%40%60%80% (h) GLY\\n24681012\\nHead24681012Layer% Attention\\n0 50%Max\\n0%15%30%45% (i) HIS\\n24681012\\nHead24681012Layer% Attention\\n0 25%Max\\n0%6%12%18%24%\\n(j) ILE\\n24681012\\nHead24681012Layer% Attention\\n025%Max\\n0%10%20%30%40% (k) LEU\\n24681012\\nHead24681012Layer% Attention\\n0 25%Max\\n0%6%12%18%24% (l) LYS\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n0%15%30%45%60%\\n(m) MET\\n24681012\\nHead24681012Layer% Attention\\n0 20%Max\\n0%5%10%15%20% (n) PHE\\n24681012\\nHead24681012Layer% Attention\\n0 100%Max\\n0%20%40%60%80% (o) PRO\\nFigure 18: Percentage of each head’s attention that is focused on the given amino acid, averaged over\\na dataset (TapeBert).\\n23',\n",
       " 'Published as a conference paper at ICLR 2021\\n24681012\\nHead24681012Layer% Attention\\n025%Max\\n0%8%16%24%32%\\n(a) SER\\n24681012\\nHead24681012Layer% Attention\\n010%Max\\n0%4%8%12%16% (b) THR\\n24681012\\nHead24681012Layer% Attention\\n050%Max\\n0%15%30%45%60% (c) TRP\\n24681012\\nHead24681012Layer% Attention\\n0 50%Max\\n0%15%30%45%\\n(d) TYR\\n24681012\\nHead24681012Layer% Attention\\n025%Max\\n0%8%16%24%32% (e) V AL\\nFigure 19: Percentage of each head’s attention that is focused on the given amino acid, averaged over\\na dataset (cont.)\\nTable 3: Amino acids and the corresponding maximally attentive heads in the standard and randomized\\nversions of TapeBert. The differences between the attention percentages for TapeBert and the\\nbackground frequencies of each amino acid are all statistically signiﬁcant ( p< 0.00001 ) taking into\\naccount the Bonferroni correction. See Appendix B.2 for details. The bolded numbers represent the\\nhigher of the two values between the standard and random models. In all cases except for Glutamine,\\nwhich was the amino acid with the lowest top attention proportion in the standard model (7.1), the\\nstandard TapeBert model has higher values than the randomized version.\\nTapeBert TapeBert-Random\\nAbbrev Code Name Background % Top Head Attn % Top Head Attn %\\nAla A Alanine 7.9 12-11 25.5 11-12 12.1\\nArg R Arginine 5.2 12-8 63.2 12-7 8.4\\nAsn N Asparagine 4.3 8-2 44.8 8-2 6.7\\nAsp D Aspartic acid 5.8 12-6 79.9 5-4 10.7\\nCys C Cysteine 1.3 11-6 83.2 11-6 9.3\\nGln Q Glutamine 3.8 11-7 7.1 12-1 9.2\\nGlu E Glutamic acid 6.9 11-7 16.2 11-4 11.8\\nGly G Glycine 7.1 2-11 98.1 11-8 14.6\\nHis H Histidine 2.7 9-10 56.7 11-6 5.4\\nIle I Isoleucine 5.6 11-10 27.0 9-5 10.6\\nLeu L Leucine 9.4 2-12 44.1 12-11 13.9\\nLys K Lysine 6.0 12-8 29.4 6-11 12.9\\nMet M Methionine 2.3 3-10 73.5 9-3 6.2\\nPhe F Phenylalanine 3.9 12-3 22.7 12-1 6.7\\nPro P Proline 4.6 1-11 98.3 10-6 7.6\\nSer S Serine 6.4 12-7 36.1 11-12 11.0\\nThr T Threonine 5.4 12-7 19.0 10-4 9.0\\nTrp W Tryptophan 1.3 11-4 68.1 9-2 3.0\\nTyr Y Tyrosine 3.4 12-3 51.6 12-11 6.6\\nVal V Valine 6.8 12-11 34.0 8-2 15.0\\n24']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [doc.page_content.replace(\"{\", '[').replace(\"}\", \"]\") for doc in docs]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'$defs': {'Author': {'properties': {'name': {'description': 'The name of the author', 'title': 'Name', 'type': 'string'}, 'affiliation': {'description': 'The affiliation of the author', 'title': 'Affiliation', 'type': 'string'}}, 'required': ['name', 'affiliation'], 'title': 'Author', 'type': 'object'}}, 'properties': {'title': {'description': 'The title of the scientific article', 'title': 'Title', 'type': 'string'}, 'authors': {'description': \"The list of the article's authors and their affiliation\", 'items': {'$ref': '#/$defs/Author'}, 'title': 'Authors', 'type': 'array'}, 'abstract': {'description': \"The article's abstract\", 'title': 'Abstract', 'type': 'string'}, 'key_findings': {'description': 'The key findings of the article', 'title': 'Key Findings', 'type': 'string'}, 'limitation_of_sota': {'description': 'limitation of the existing work', 'title': 'Limitation Of Sota', 'type': 'string'}, 'proposed_solution': {'description': 'the proposed solution in details', 'title': 'Proposed Solution', 'type': 'string'}, 'paper_limitations': {'description': 'The limitations of the proposed solution of the paper', 'title': 'Paper Limitations', 'type': 'string'}}, 'required': ['title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations']}, {'title': 'Protein Modeling with Transformers', 'authors': [{'$ref': '#/$defs/Author'}], 'abstract': 'This paper presents a novel approach to protein modeling using transformers. The authors demonstrate that specialized attention heads in a transformer can recover protein structure and function, based solely on language model pre-training.', 'key_findings': 'The key findings of this study are:', 'limitation_of_sota': '', 'proposed_solution': 'The proposed solution is to use specialized attention heads in a transformer to recover protein structure and function.', 'paper_limitations': ''}, {'$defs': {'Author': {'properties': {'name': {'description': 'The name of the author', 'title': 'Name', 'type': 'string'}, 'affiliation': {'description': 'The affiliation of the author', 'title': 'Affiliation', 'type': 'string'}}, 'required': ['name', 'affiliation'], 'title': 'Author', 'type': 'object'}}, 'properties': {'title': {'description': 'The title of the scientific article', 'title': 'Title', 'type': 'string'}, 'authors': {'description': \"The list of the article's authors and their affiliation\", 'items': {'$ref': '#/$defs/Author'}, 'title': 'Authors', 'type': 'array'}, 'abstract': {'description': \"The article's abstract\", 'title': 'Abstract', 'type': 'string'}, 'key_findings': {'description': 'The key findings of the article', 'title': 'Key Findings', 'type': 'string'}, 'limitation_of_sota': {'description': 'limitation of the existing work', 'title': 'Limitation Of Sota', 'type': 'string'}, 'proposed_solution': {'description': 'the proposed solution in details', 'title': 'Proposed Solution', 'type': 'string'}, 'paper_limitations': {'description': 'The limitations of the proposed solution of the paper', 'title': 'Paper Limitations', 'type': 'string'}}, 'required': ['title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations']}, {'title': 'Context for attention weights', 'authors': [{'$ref': '#/$defs/Author'}, {'$ref': '#/$defs/Author'}], 'abstract': '', 'key_findings': '', 'limitation_of_sota': '', 'proposed_solution': '', 'paper_limitations': ''}, {'title': 'What Does Attention Understand About Proteins?', 'authors': [{'$ref': '#/$defs/Author', 'name': \"Your Name (assuming you're the author)\", 'affiliation': 'Your Affiliation'}], 'abstract': '', 'key_findings': 'Attention aligns strongly with contact maps in the deepest layers.', 'limitation_of_sota': '', 'proposed_solution': '', 'paper_limitations': ''}, {'title': 'Attention to Binding Sites and Post-Translational Modifications', 'authors': [{'$ref': '#/$defs/Author', 'name': 'Unknown Author', 'affiliation': 'Unknown Affiliation'}], 'abstract': '', 'key_findings': 'The study found that attention heads in five pre-trained models (TapeBert, ProtAlbert, ProtBert, ProtBert-BFD, and ProtXLNet) focus strongly on binding sites.', 'limitation_of_sota': 'The existing work does not provide a comprehensive analysis of attention to binding sites.', 'proposed_solution': 'The study proposes that the pre-trained models have learned structurally-aware attention patterns by focusing on contacts between amino acids, which can be used for further discovery and investigation or repurposed for prediction tasks.', 'paper_limitations': ''}, {'title': 'Why does attention target binding sites?', 'authors': [{'$ref': '#/$defs/Author', 'name': 'Unknown Author', 'affiliation': 'Unknown Affiliation'}], 'abstract': '', 'key_findings': 'Attention targets binding sites because they describe how a protein interacts with other molecules, and these external interactions ultimately define the high-level function of the protein.', 'limitation_of_sota': '', 'proposed_solution': 'The proposed solution is not applicable to this problem.', 'paper_limitations': ''}, {'$defs': {'Author': {'properties': {'name': {'description': 'The name of the author', 'title': 'Name', 'type': 'string'}, 'affiliation': {'description': 'The affiliation of the author', 'title': 'Affiliation', 'type': 'string'}}, 'required': ['name', 'affiliation'], 'title': 'Author', 'type': 'object'}}, 'properties': {'title': {'description': 'The title of the scientific article', 'title': 'Title', 'type': 'string'}, 'authors': {'description': \"The list of the article's authors and their affiliation\", 'items': {'$ref': '#/$defs/Author'}, 'title': 'Authors', 'type': 'array'}, 'abstract': {'description': \"The article's abstract\", 'title': 'Abstract', 'type': 'string'}, 'key_findings': {'description': 'The key findings of the article', 'title': 'Key Findings', 'type': 'string'}, 'limitation_of_sota': {'description': 'limitation of the existing work', 'title': 'Limitation Of Sota', 'type': 'string'}, 'proposed_solution': {'description': 'the proposed solution in details', 'title': 'Proposed Solution', 'type': 'string'}, 'paper_limitations': {'description': 'The limitations of the proposed solution of the paper', 'title': 'Paper Limitations', 'type': 'string'}}, 'required': ['title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations'], 'title': 'Scientific Article', 'type': 'object'}, {'$defs': {'Author': {'properties': {'name': {'description': 'The name of the author', 'title': 'Name', 'type': 'string'}, 'affiliation': {'description': 'The affiliation of the author', 'title': 'Affiliation', 'type': 'string'}}, 'required': ['name', 'affiliation'], 'title': 'Author', 'type': 'object'}}, 'properties': {'title': {'description': 'The title of the scientific article', 'title': 'Title', 'type': 'string'}, 'authors': {'description': \"The list of the article's authors and their affiliation\", 'items': {'$ref': '#/$defs/Author'}, 'title': 'Authors', 'type': 'array'}, 'abstract': {'description': \"The article's abstract\", 'title': 'Abstract', 'type': 'string'}, 'key_findings': {'description': 'The key findings of the article', 'title': 'Key Findings', 'type': 'string'}, 'limitation_of_sota': {'description': 'limitation of the existing work', 'title': 'Limitation Of Sota', 'type': 'string'}, 'proposed_solution': {'description': 'the proposed solution in details', 'title': 'Proposed Solution', 'type': 'string'}, 'paper_limitations': {'description': 'The limitations of the proposed solution of the paper', 'title': 'Paper Limitations', 'type': 'string'}}, 'required': ['title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations']}, {'title': 'Our work extends these methods to protein sequence models by considering particular biophysical properties and relationships.', 'authors': [{'$ref': '#/$defs/Author', 'name': '', 'affiliation': ''}], 'abstract': '', 'key_findings': 'Our work extends these methods to protein sequence models by considering particular biophysical properties and relationships. We also present a joint cross-layer probing analysis of attention weights and layer embeddings.', 'limitation_of_sota': 'Depending on the task and model architecture, attention may have less or more explanatory power for model predictions (Jain & Wallace, 2019; Serrano & Smith, 2019; Pruthi et al., 2020; Moradi et al., 2019; Vashishth et al., 2019).', 'proposed_solution': 'We extend these methods to protein sequence models by considering particular biophysical properties and relationships.', 'paper_limitations': ''}, {'title': 'Reconciling Attention with Known Properties of Proteins', 'authors': [{'name': 'Xi Victoria Lin, Stephan Zheng, Melvin Gruesbeck, and anonymous reviewers', 'affiliation': ''}], 'abstract': '', 'key_findings': 'This paper builds on the synergy between NLP and computational biology by adapting and extending NLP interpretability methods to protein sequence modeling.', 'limitation_of_sota': '', 'proposed_solution': 'A Transformer language model recovers structural and functional properties of proteins and integrates this knowledge directly into its attention mechanism.', 'paper_limitations': ''}, {'title': 'What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties.', 'authors': [{'name': 'Alexis Conneau', 'affiliation': ''}, {'name': 'German Kruszewski', 'affiliation': ''}, {'name': 'Guillaume Lample', 'affiliation': ''}, {'name': 'Loïc Barrault', 'affiliation': ''}, {'name': 'Marco Baroni', 'affiliation': ''}], 'abstract': '', 'key_findings': '', 'limitation_of_sota': '', 'proposed_solution': '', 'paper_limitations': ''}, {'title': 'exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models', 'authors': [{'name': 'Benjamin Hoover', 'affiliation': ''}, {'name': 'Hendrik Strobelt', 'affiliation': ''}, {'name': 'Sebastian Gehrmann', 'affiliation': ''}], 'abstract': '', 'key_findings': '', 'limitation_of_sota': '', 'proposed_solution': '', 'paper_limitations': ''}, {'title': 'Do attention heads in BERT track syntactic dependencies?', 'authors': [{'$ref': '#/$defs/Author'}, {'$ref': '#/$defs/Author'}, {'$ref': '#/$defs/Author'}, {'$ref': '#/$defs/Author'}], 'abstract': '', 'key_findings': '', 'limitation_of_sota': '', 'proposed_solution': 'The paper proposes to investigate whether attention heads in BERT can track syntactic dependencies.', 'paper_limitations': ''}, {'title': 'Interrogating the explanatory power of attention in neural machine translation', 'authors': [{'name': 'Pooya Moradi', 'affiliation': ''}, {'name': 'Nishant Kambhatla', 'affiliation': ''}, {'name': 'Anoop Sarkar', 'affiliation': ''}], 'abstract': '', 'key_findings': '', 'limitation_of_sota': 'The existing work on attention mechanisms in neural machine translation has limitations, such as not fully understanding the explanatory power of attention.', 'proposed_solution': 'The proposed solution is to interrogate the explanatory power of attention in neural machine translation using a specific approach.', 'paper_limitations': ''}, {'title': 'NGL Viewer: a web application for molecular visualization', 'authors': [{'$ref': '#/$defs/Author', 'name': 'Alexander S. Rose', 'affiliation': ''}, {'$ref': '#/$defs/Author', 'name': 'Peter W. Hildebrand', 'affiliation': ''}], 'abstract': '', 'key_findings': '', 'limitation_of_sota': '', 'proposed_solution': '', 'paper_limitations': ''}, {'title': 'Attention is not not explanation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 11–20, November 2019.', 'authors': [{'name': 'Sarah Wiegreffe', 'affiliation': ''}, {'name': 'Yuval Pinter', 'affiliation': ''}], 'abstract': '', 'key_findings': '', 'limitation_of_sota': '', 'proposed_solution': '', 'paper_limitations': ''}, {'$defs': {'Author': {'properties': {'name': {'description': 'The name of the author', 'title': 'Name', 'type': 'string'}, 'affiliation': {'description': 'The affiliation of the author', 'title': 'Affiliation', 'type': 'string'}}, 'required': ['name', 'affiliation'], 'title': 'Author', 'type': 'object'}}, 'properties': {'title': {'description': 'The title of the scientific article', 'title': 'Title', 'type': 'string'}, 'authors': {'description': \"The list of the article's authors and their affiliation\", 'items': {'$ref': '#/$defs/Author'}, 'title': 'Authors', 'type': 'array'}, 'abstract': {'description': \"The article's abstract\", 'title': 'Abstract', 'type': 'string'}, 'key_findings': {'description': 'The key findings of the article', 'title': 'Key Findings', 'type': 'string'}, 'limitation_of_sota': {'description': 'limitation of the existing work', 'title': 'Limitation Of Sota', 'type': 'string'}, 'proposed_solution': {'description': 'the proposed solution in details', 'title': 'Proposed Solution', 'type': 'string'}, 'paper_limitations': {'description': 'The limitations of the proposed solution of the paper', 'title': 'Paper Limitations', 'type': 'string'}}, 'required': ['title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations']}, {'$defs': {'Author': {'properties': {'name': {'description': 'The name of the author', 'title': 'Name', 'type': 'string'}, 'affiliation': {'description': 'The affiliation of the author', 'title': 'Affiliation', 'type': 'string'}}, 'required': ['name', 'affiliation'], 'title': 'Author', 'type': 'object'}}, 'properties': {'title': {'description': 'The title of the scientific article', 'title': 'Title', 'type': 'string'}, 'authors': {'description': \"The list of the article's authors and their affiliation\", 'items': {'$ref': '#/$defs/Author'}, 'title': 'Authors', 'type': 'array'}, 'abstract': {'description': \"The article's abstract\", 'title': 'Abstract', 'type': 'string'}, 'key_findings': {'description': 'The key findings of the article', 'title': 'Key Findings', 'type': 'string'}, 'limitation_of_sota': {'description': 'limitation of the existing work', 'title': 'Limitation Of Sota', 'type': 'string'}, 'proposed_solution': {'description': 'the proposed solution in details', 'title': 'Proposed Solution', 'type': 'string'}, 'paper_limitations': {'description': 'The limitations of the proposed solution of the paper', 'title': 'Paper Limitations', 'type': 'string'}}, 'required': ['title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations'], 'title': 'Scientific Article', 'type': 'object'}, {'title': '', 'authors': [{'name': '', 'affiliation': ''}], 'abstract': '', 'key_findings': '', 'limitation_of_sota': '', 'proposed_solution': '', 'paper_limitations': ''}, {'$defs': {'Author': {'properties': {'name': {'description': 'The name of the author', 'title': 'Name', 'type': 'string'}, 'affiliation': {'description': 'The affiliation of the author', 'title': 'Affiliation', 'type': 'string'}}, 'required': ['name', 'affiliation'], 'title': 'Author', 'type': 'object'}}, 'properties': {'title': {'description': 'The title of the scientific article', 'title': 'Title', 'type': 'string'}, 'authors': {'description': \"The list of the article's authors and their affiliation\", 'items': {'$ref': '#/$defs/Author'}, 'title': 'Authors', 'type': 'array'}, 'abstract': {'description': \"The article's abstract\", 'title': 'Abstract', 'type': 'string'}, 'key_findings': {'description': 'The key findings of the article', 'title': 'Key Findings', 'type': 'string'}, 'limitation_of_sota': {'description': 'limitation of the existing work', 'title': 'Limitation Of Sota', 'type': 'string'}, 'proposed_solution': {'description': 'the proposed solution in details', 'title': 'Proposed Solution', 'type': 'string'}, 'paper_limitations': {'description': 'The limitations of the proposed solution of the paper', 'title': 'Paper Limitations', 'type': 'string'}}, 'required': ['title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations'], 'title': 'Scientific Article', 'type': 'object'}, {'$defs': {'Author': {'properties': {'name': {'description': 'The name of the author', 'title': 'Name', 'type': 'string'}, 'affiliation': {'description': 'The affiliation of the author', 'title': 'Affiliation', 'type': 'string'}}, 'required': ['name', 'affiliation'], 'title': 'Author', 'type': 'object'}}, 'properties': {'title': {'description': 'The title of the scientific article', 'title': 'Title', 'type': 'string'}, 'authors': {'description': \"The list of the article's authors and their affiliation\", 'items': {'$ref': '#/$defs/Author'}, 'title': 'Authors', 'type': 'array'}, 'abstract': {'description': \"The article's abstract\", 'title': 'Abstract', 'type': 'string'}, 'key_findings': {'description': 'The key findings of the article', 'title': 'Key Findings', 'type': 'string'}, 'limitation_of_sota': {'description': 'limitation of the existing work', 'title': 'Limitation Of Sota', 'type': 'string'}, 'proposed_solution': {'description': 'the proposed solution in details', 'title': 'Proposed Solution', 'type': 'string'}, 'paper_limitations': {'description': 'The limitations of the proposed solution of the paper', 'title': 'Paper Limitations', 'type': 'string'}}, 'required': ['title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations'], 'title': 'Scientific Article', 'type': 'object'}, {'title': 'C.2 Contact Maps: Statistical Significance Tests and Null Models', 'authors': [{'name': '', 'affiliation': ''}], 'abstract': '', 'key_findings': 'The paper presents a study on contact maps, statistical significance tests, and null models.', 'limitation_of_sota': '', 'proposed_solution': '', 'paper_limitations': 'The limitations of the proposed solution are not mentioned in the provided chunk.'}, {'title': 'C.3 B INDING SITES : STATISTICAL SIGNIFICANCE TESTS AND NULL MODEL', 'authors': [{'name': '', 'affiliation': ''}], 'abstract': '', 'key_findings': 'The paper presents a study on binding sites, specifically focusing on statistical significance tests and null models. It compares the performance of different models, including TapeBert, ProtAlbert, ProtBert, ProtBert-BFD, and ProtXLNet.', 'limitation_of_sota': '', 'proposed_solution': 'The paper proposes a solution for identifying binding sites using machine learning models.', 'paper_limitations': ''}, {'title': 'Post-translational modifications (PTMs)', 'authors': [{'name': '', 'affiliation': ''}], 'abstract': '', 'key_findings': 'The paper presents a comparison of different models for post-translational modification prediction.', 'limitation_of_sota': '', 'proposed_solution': 'The paper proposes the use of ProtBert-BFD, a variant of ProtBert that uses bidirectional and forward attention mechanisms.', 'paper_limitations': {'TapeBert': '24.68%', 'ProtAlbert': '2.5%', 'ProtBert': '25%', 'ProtBert-BFD': '0.5%', 'ProtXLNet': ''}}, {'$defs': {'Author': {'properties': {'name': {'description': 'The name of the author', 'title': 'Name', 'type': 'string'}, 'affiliation': {'description': 'The affiliation of the author', 'title': 'Affiliation', 'type': 'string'}}, 'required': ['name', 'affiliation'], 'title': 'Author', 'type': 'object'}}, 'properties': {'title': {'description': 'The title of the scientific article', 'title': 'Title', 'type': 'string'}, 'authors': {'description': \"The list of the article's authors and their affiliation\", 'items': {'$ref': '#/$defs/Author'}, 'title': 'Authors', 'type': 'array'}, 'abstract': {'description': \"The article's abstract\", 'title': 'Abstract', 'type': 'string'}, 'key_findings': {'description': 'The key findings of the article', 'title': 'Key Findings', 'type': 'string'}, 'limitation_of_sota': {'description': 'limitation of the existing work', 'title': 'Limitation Of Sota', 'type': 'string'}, 'proposed_solution': {'description': 'the proposed solution in details', 'title': 'Proposed Solution', 'type': 'string'}, 'paper_limitations': {'description': 'The limitations of the proposed solution of the paper', 'title': 'Paper Limitations', 'type': 'string'}}, 'required': ['title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations'], 'title': 'Scientific Article', 'type': 'object'}, {'title': 'Amino Acid Attention in Protein Sequence Modeling', 'authors': [], 'abstract': '', 'key_findings': \"The figure shows the percentage of each head's attention that is focused on the given amino acid, averaged over a dataset (TapeBert).\", 'limitation_of_sota': '', 'proposed_solution': '', 'paper_limitations': '', 'amino_acids': [{'name': 'ALA', 'attention_percentages': ['0%', '60%']}, {'name': 'ARG', 'attention_percentages': ['0%', '40%']}, {'name': 'ASN', 'attention_percentages': ['0%', '60%']}, {'name': 'ASP', 'attention_percentages': ['0%', '80%']}, {'name': 'CYS', 'attention_percentages': ['0%', '6%']}, {'name': 'GLN', 'attention_percentages': ['0%', '16%']}, {'name': 'GLU', 'attention_percentages': ['0%', '80%']}, {'name': 'GLY', 'attention_percentages': ['0%', '45%']}, {'name': 'HIS', 'attention_percentages': ['0%', '24%']}, {'name': 'ILE', 'attention_percentages': ['0%', '40%']}, {'name': 'LEU', 'attention_percentages': ['0%', '24%']}, {'name': 'LYS', 'attention_percentages': ['0%', '60%']}, {'name': 'MET', 'attention_percentages': ['0%', '20%']}, {'name': 'PHE', 'attention_percentages': ['0%', '80%']}]}, {'title': 'Published as a conference paper at ICLR 2021', 'authors': [{'name': '', 'affiliation': ''}], 'abstract': '', 'key_findings': 'The proposed solution, TapeBert, has higher attention values for certain amino acids compared to a randomized version.', 'limitation_of_sota': '', 'proposed_solution': 'The paper proposes a model called TapeBert that uses self-attention mechanisms to analyze protein sequences.', 'paper_limitations': ''}]\n"
     ]
    }
   ],
   "source": [
    "IE_query = '''\n",
    "# DIRECTIVES : \n",
    "- Act like an experienced information extractor. \n",
    "- You have a chunk of a scientific paper.\n",
    "- If you do not find the right information, keep its place empty.\n",
    "'''\n",
    "# we have replaced the curly braces with square brackets to avoid the error in the query\n",
    "distilled_doc = document_distiller.distill(documents=docs, IE_query=IE_query, output_data_structure=Article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"$defs - ['Author': ['properties': ['name': ['description': 'The name of the author', 'title': 'Name', 'type': 'string'], 'affiliation': ['description': 'The affiliation of the author', 'title': 'Affiliation', 'type': 'string']], 'required': ['name', 'affiliation'], 'title': 'Author', 'type': 'object']]\",\n",
       " 'properties - [\\'title\\': [\\'description\\': \\'The title of the scientific article\\', \\'title\\': \\'Title\\', \\'type\\': \\'string\\'], \\'authors\\': [\\'description\\': \"The list of the article\\'s authors and their affiliation\", \\'items\\': [\\'$ref\\': \\'#/$defs/Author\\'], \\'title\\': \\'Authors\\', \\'type\\': \\'array\\'], \\'abstract\\': [\\'description\\': \"The article\\'s abstract\", \\'title\\': \\'Abstract\\', \\'type\\': \\'string\\'], \\'key_findings\\': [\\'description\\': \\'The key findings of the article\\', \\'title\\': \\'Key Findings\\', \\'type\\': \\'string\\'], \\'limitation_of_sota\\': [\\'description\\': \\'limitation of the existing work\\', \\'title\\': \\'Limitation Of Sota\\', \\'type\\': \\'string\\'], \\'proposed_solution\\': [\\'description\\': \\'the proposed solution in details\\', \\'title\\': \\'Proposed Solution\\', \\'type\\': \\'string\\'], \\'paper_limitations\\': [\\'description\\': \\'The limitations of the proposed solution of the paper\\', \\'title\\': \\'Paper Limitations\\', \\'type\\': \\'string\\']]',\n",
       " \"required - ['title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations', 'title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations', 'title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations', 'title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations', 'title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations', 'title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations', 'title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations', 'title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations', 'title', 'authors', 'abstract', 'key_findings', 'limitation_of_sota', 'proposed_solution', 'paper_limitations']\",\n",
       " 'title - Protein Modeling with Transformers Context for attention weights What Does Attention Understand About Proteins? Attention to Binding Sites and Post-Translational Modifications Why does attention target binding sites? Scientific Article Our work extends these methods to protein sequence models by considering particular biophysical properties and relationships. Reconciling Attention with Known Properties of Proteins What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties. exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models Do attention heads in BERT track syntactic dependencies? Interrogating the explanatory power of attention in neural machine translation NGL Viewer: a web application for molecular visualization Attention is not not explanation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 11–20, November 2019. Scientific Article Scientific Article Scientific Article C.2 Contact Maps: Statistical Significance Tests and Null Models C.3 B INDING SITES : STATISTICAL SIGNIFICANCE TESTS AND NULL MODEL Post-translational modifications (PTMs) Scientific Article Amino Acid Attention in Protein Sequence Modeling Published as a conference paper at ICLR 2021',\n",
       " 'authors - [[\\'$ref\\': \\'#/$defs/Author\\'], [\\'$ref\\': \\'#/$defs/Author\\'], [\\'$ref\\': \\'#/$defs/Author\\'], [\\'$ref\\': \\'#/$defs/Author\\', \\'name\\': \"Your Name (assuming you\\'re the author)\", \\'affiliation\\': \\'Your Affiliation\\'], [\\'$ref\\': \\'#/$defs/Author\\', \\'name\\': \\'Unknown Author\\', \\'affiliation\\': \\'Unknown Affiliation\\'], [\\'$ref\\': \\'#/$defs/Author\\', \\'name\\': \\'Unknown Author\\', \\'affiliation\\': \\'Unknown Affiliation\\'], [\\'$ref\\': \\'#/$defs/Author\\', \\'name\\': \\'\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'Xi Victoria Lin, Stephan Zheng, Melvin Gruesbeck, and anonymous reviewers\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'Alexis Conneau\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'German Kruszewski\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'Guillaume Lample\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'Loïc Barrault\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'Marco Baroni\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'Benjamin Hoover\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'Hendrik Strobelt\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'Sebastian Gehrmann\\', \\'affiliation\\': \\'\\'], [\\'$ref\\': \\'#/$defs/Author\\'], [\\'$ref\\': \\'#/$defs/Author\\'], [\\'$ref\\': \\'#/$defs/Author\\'], [\\'$ref\\': \\'#/$defs/Author\\'], [\\'name\\': \\'Pooya Moradi\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'Nishant Kambhatla\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'Anoop Sarkar\\', \\'affiliation\\': \\'\\'], [\\'$ref\\': \\'#/$defs/Author\\', \\'name\\': \\'Alexander S. Rose\\', \\'affiliation\\': \\'\\'], [\\'$ref\\': \\'#/$defs/Author\\', \\'name\\': \\'Peter W. Hildebrand\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'Sarah Wiegreffe\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'Yuval Pinter\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'\\', \\'affiliation\\': \\'\\'], [\\'name\\': \\'\\', \\'affiliation\\': \\'\\']]',\n",
       " 'abstract - This paper presents a novel approach to protein modeling using transformers. The authors demonstrate that specialized attention heads in a transformer can recover protein structure and function, based solely on language model pre-training.',\n",
       " \"key_findings - The key findings of this study are: Attention aligns strongly with contact maps in the deepest layers. The study found that attention heads in five pre-trained models (TapeBert, ProtAlbert, ProtBert, ProtBert-BFD, and ProtXLNet) focus strongly on binding sites. Attention targets binding sites because they describe how a protein interacts with other molecules, and these external interactions ultimately define the high-level function of the protein. Our work extends these methods to protein sequence models by considering particular biophysical properties and relationships. We also present a joint cross-layer probing analysis of attention weights and layer embeddings. This paper builds on the synergy between NLP and computational biology by adapting and extending NLP interpretability methods to protein sequence modeling. The paper presents a study on contact maps, statistical significance tests, and null models. The paper presents a study on binding sites, specifically focusing on statistical significance tests and null models. It compares the performance of different models, including TapeBert, ProtAlbert, ProtBert, ProtBert-BFD, and ProtXLNet. The paper presents a comparison of different models for post-translational modification prediction. The figure shows the percentage of each head's attention that is focused on the given amino acid, averaged over a dataset (TapeBert). The proposed solution, TapeBert, has higher attention values for certain amino acids compared to a randomized version.\",\n",
       " 'limitation_of_sota - The existing work does not provide a comprehensive analysis of attention to binding sites. Depending on the task and model architecture, attention may have less or more explanatory power for model predictions (Jain & Wallace, 2019; Serrano & Smith, 2019; Pruthi et al., 2020; Moradi et al., 2019; Vashishth et al., 2019). The existing work on attention mechanisms in neural machine translation has limitations, such as not fully understanding the explanatory power of attention.',\n",
       " 'proposed_solution - The proposed solution is to use specialized attention heads in a transformer to recover protein structure and function. The study proposes that the pre-trained models have learned structurally-aware attention patterns by focusing on contacts between amino acids, which can be used for further discovery and investigation or repurposed for prediction tasks. The proposed solution is not applicable to this problem. We extend these methods to protein sequence models by considering particular biophysical properties and relationships. A Transformer language model recovers structural and functional properties of proteins and integrates this knowledge directly into its attention mechanism. The paper proposes to investigate whether attention heads in BERT can track syntactic dependencies. The proposed solution is to interrogate the explanatory power of attention in neural machine translation using a specific approach. The paper proposes a solution for identifying binding sites using machine learning models. The paper proposes the use of ProtBert-BFD, a variant of ProtBert that uses bidirectional and forward attention mechanisms. The paper proposes a model called TapeBert that uses self-attention mechanisms to analyze protein sequences.',\n",
       " 'paper_limitations - ',\n",
       " 'type - object object object object object',\n",
       " \"amino_acids - [['name': 'ALA', 'attention_percentages': ['0%', '60%']], ['name': 'ARG', 'attention_percentages': ['0%', '40%']], ['name': 'ASN', 'attention_percentages': ['0%', '60%']], ['name': 'ASP', 'attention_percentages': ['0%', '80%']], ['name': 'CYS', 'attention_percentages': ['0%', '6%']], ['name': 'GLN', 'attention_percentages': ['0%', '16%']], ['name': 'GLU', 'attention_percentages': ['0%', '80%']], ['name': 'GLY', 'attention_percentages': ['0%', '45%']], ['name': 'HIS', 'attention_percentages': ['0%', '24%']], ['name': 'ILE', 'attention_percentages': ['0%', '40%']], ['name': 'LEU', 'attention_percentages': ['0%', '24%']], ['name': 'LYS', 'attention_percentages': ['0%', '60%']], ['name': 'MET', 'attention_percentages': ['0%', '20%']], ['name': 'PHE', 'attention_percentages': ['0%', '80%']]]\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format the distilled document into semantic sections.\n",
    "semantic_blocks = [f\"{key} - {value}\".replace(\"{\", \"[\").replace(\"}\", \"]\") for key, value in distilled_doc.items()]\n",
    "semantic_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iText2KG for graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting Entities from the Document 1\n",
      "{'entities': [{'label': 'Author', 'name': 'The name of the author'}]}\n",
      "[INFO] Extracting Relations from the Document 1\n",
      "{'$defs': {'Relationship': {'properties': {'startNode': {'default': 'The starting entity, which is present in the entities list.', 'title': 'Startnode', 'type': 'string'}, 'endNode': {'default': 'The ending entity, which is present in the entities list.', 'title': 'Endnode', 'type': 'string'}, 'name': {'default': 'The predicate that defines the relationship between the two entities. This predicate should represent a single, semantically distinct relation.', 'title': 'Name', 'type': 'string'}}, 'title': 'Relationship', 'type': 'object'}}, 'properties': {'relationships': {'default': 'Based on the provided entities and context, identify the predicates that define relationships between these entities. The predicates should be chosen with precision to accurately reflect the expressed relationships.', 'items': {'$ref': '#/$defs/Relationship'}, 'title': 'Relationships', 'type': 'array'}}, 'relationships': [{'name': 'has_name', 'startNode': 'the name of the author', 'endNode': 'the name of the author'}]}\n",
      "[INFO] The isolated entities are  []\n",
      "Some isolated entities without relations were detected ... trying to solve them!\n",
      "{'$defs': {'Relationship': {'properties': {'startNode': {'default': 'The starting entity, which is present in the entities list.', 'title': 'Startnode', 'type': 'string'}, 'endNode': {'default': 'The ending entity, which is present in the entities list.', 'title': 'Endnode', 'type': 'string'}, 'name': {'default': 'The predicate that defines the relationship between the two entities. This predicate should represent a single, semantically distinct relation.', 'title': 'Name', 'type': 'string'}}, 'title': 'Relationship', 'type': 'object'}}, 'properties': {'relationships': {'default': 'Based on the provided entities and context, identify the predicates that define relationships between these entities. The predicates should be chosen with precision to accurately reflect the expressed relationships.', 'items': {'$ref': '#/$defs/Relationship'}, 'title': 'Relationships', 'type': 'array'}}, 'relationships': [{'name': 'author_of', 'startNode': 'the name of the author', 'endNode': 'Author'}]}\n",
      "[INFO] Extracting Entities from the Document 2\n",
      "{'entities': [{'label': 'The title of the scientific article', 'name': 'Title'}, {'label': 'Authors', 'name': 'List of authors and their affiliation'}, {'label': 'Abstract', 'name': \"Article's abstract\"}, {'label': 'Key Findings', 'name': 'The key findings of the article'}, {'label': 'Limitation Of Sota', 'name': 'Limitation of the existing work'}, {'label': 'Proposed Solution', 'name': 'The proposed solution in details'}, {'label': 'Paper Limitations', 'name': 'The limitations of the proposed solution of the paper'}]}\n",
      "[INFO] Extracting Relations from the Document 2\n",
      "{'$defs': {'Relationship': {'properties': {'startNode': {'default': 'The starting entity, which is present in the entities list.', 'title': 'Startnode', 'type': 'string'}, 'endNode': {'default': 'The ending entity, which is present in the entities list.', 'title': 'Endnode', 'type': 'string'}, 'name': {'default': 'The predicate that defines the relationship between the two entities. This predicate should represent a single, semantically distinct relation.', 'title': 'Name', 'type': 'string'}}, 'title': 'Relationship', 'type': 'object'}}, 'properties': {'relationships': {'default': 'Based on the provided entities and context, identify the predicates that define relationships between these entities. The predicates should be chosen with precision to accurately reflect the expressed relationships.', 'items': {'$ref': '#/$defs/Relationship'}, 'title': 'Relationships', 'type': 'array'}}, 'relationships': [{'startNode': 'The title of the scientific article', 'endNode': 'The list of authors and their affiliation', 'name': 'Written by'}, {'startNode': 'The list of authors and their affiliation', 'endNode': \"The article's abstract\", 'name': 'Related to'}, {'startNode': \"The article's abstract\", 'endNode': 'The key findings of the article', 'name': 'Describes'}, {'startNode': 'The key findings of the article', 'endNode': 'Limitation of the existing work', 'name': 'Contrasts with'}, {'startNode': 'Limitation of the existing work', 'endNode': 'The proposed solution in details', 'name': 'Solves'}, {'startNode': 'The proposed solution in details', 'endNode': 'The limitations of the proposed solution of the paper', 'name': 'Addresses'}]}\n",
      "[INFO] Wohoo ! Relation using embeddings is matched --- contrasts with -merged--> author of \n",
      "[INFO] Wohoo ! Relation using embeddings is matched --- addresses -merged--> has name \n",
      "[INFO] The isolated entities are  [{'label': 'The title of the scientific article', 'name': 'title', 'properties': {'embeddings': array([ 0.00038817, -0.01537237,  0.02617165, ...,  0.00610176,\n",
      "        0.01082225,  0.00830292])}}, {'label': 'Authors', 'name': 'list of authors and their affiliation', 'properties': {'embeddings': array([-0.01012932, -0.02165926,  0.01579848, ...,  0.01217581,\n",
      "       -0.00960065, -0.00984145])}}, {'label': 'Abstract', 'name': \"article's abstract\", 'properties': {'embeddings': array([ 4.7690207e-03, -2.5707543e-02,  9.6826180e-05, ...,\n",
      "        1.3489442e-02,  3.4747417e-03,  3.1695124e-03])}}, {'label': 'Key Findings', 'name': 'the key findings of the article', 'properties': {'embeddings': array([ 0.01059615, -0.0155891 ,  0.00875205, ...,  0.01409425,\n",
      "       -0.00611884, -0.01051767])}}, {'label': 'Limitation Of Sota', 'name': 'limitation of the existing work', 'properties': {'embeddings': array([ 0.00555676, -0.00446899,  0.01147087, ...,  0.00308466,\n",
      "       -0.01393188, -0.00958561])}}, {'label': 'Proposed Solution', 'name': 'the proposed solution in details', 'properties': {'embeddings': array([-0.01100891, -0.01865113,  0.02470583, ..., -0.00596826,\n",
      "        0.0029286 , -0.00465411])}}, {'label': 'Paper Limitations', 'name': 'the limitations of the proposed solution of the paper', 'properties': {'embeddings': array([ 0.01487406, -0.00763439,  0.02448528, ...,  0.0037551 ,\n",
      "        0.0020268 , -0.01794446])}}]\n",
      "Some isolated entities without relations were detected ... trying to solve them!\n",
      "{'$defs': {'Relationship': {'properties': {'startNode': {'default': 'The starting entity, which is present in the entities list.', 'title': 'Startnode', 'type': 'string'}, 'endNode': {'default': 'The ending entity, which is present in the entities list.', 'title': 'Endnode', 'type': 'string'}, 'name': {'default': 'The predicate that defines the relationship between the two entities. This predicate should represent a single, semantically distinct relation.', 'title': 'Name', 'type': 'string'}}, 'title': 'Relationship', 'type': 'object'}}, 'properties': {'relationships': {'default': 'Based on the provided entities and context, identify the predicates that define relationships between these entities. The predicates should be chosen with precision to accurately reflect the expressed relationships.', 'items': {'$ref': '#/$defs/Relationship'}, 'title': 'Relationships', 'type': 'array'}}, 'relationships': [{'startNode': 'The title of the scientific article', 'endNode': 'Title', 'name': 'Is a title'}, {'startNode': \"The list of the article's authors and their affiliation\", 'endNode': 'Authors', 'name': 'Are authors'}, {'startNode': \"The article's abstract\", 'endNode': 'Abstract', 'name': 'Is an abstract'}, {'startNode': 'The key findings of the article', 'endNode': 'Key Findings', 'name': 'Are key findings'}, {'startNode': 'Limitation of the existing work', 'endNode': 'Limitation Of Sota', 'name': 'Is a limitation'}, {'startNode': 'The proposed solution in details', 'endNode': 'Proposed Solution', 'name': 'Is a proposed solution'}, {'startNode': 'The limitations of the proposed solution of the paper', 'endNode': 'Paper Limitations', 'name': 'Are limitations'}]}\n",
      "[INFO] The isolated entities are  [{'label': 'The title of the scientific article', 'name': 'title', 'properties': {'embeddings': array([ 0.00038817, -0.01537237,  0.02617165, ...,  0.00610176,\n",
      "        0.01082225,  0.00830292])}}, {'label': 'Authors', 'name': 'list of authors and their affiliation', 'properties': {'embeddings': array([-0.01012932, -0.02165926,  0.01579848, ...,  0.01217581,\n",
      "       -0.00960065, -0.00984145])}}, {'label': 'Abstract', 'name': \"article's abstract\", 'properties': {'embeddings': array([ 4.7690207e-03, -2.5707543e-02,  9.6826180e-05, ...,\n",
      "        1.3489442e-02,  3.4747417e-03,  3.1695124e-03])}}, {'label': 'Key Findings', 'name': 'the key findings of the article', 'properties': {'embeddings': array([ 0.01059615, -0.0155891 ,  0.00875205, ...,  0.01409425,\n",
      "       -0.00611884, -0.01051767])}}, {'label': 'Limitation Of Sota', 'name': 'limitation of the existing work', 'properties': {'embeddings': array([ 0.00555676, -0.00446899,  0.01147087, ...,  0.00308466,\n",
      "       -0.01393188, -0.00958561])}}, {'label': 'Proposed Solution', 'name': 'the proposed solution in details', 'properties': {'embeddings': array([-0.01100891, -0.01865113,  0.02470583, ..., -0.00596826,\n",
      "        0.0029286 , -0.00465411])}}, {'label': 'Paper Limitations', 'name': 'the limitations of the proposed solution of the paper', 'properties': {'embeddings': array([ 0.01487406, -0.00763439,  0.02448528, ...,  0.0037551 ,\n",
      "        0.0020268 , -0.01794446])}}]\n",
      "Some isolated entities without relations were detected ... trying to solve them!\n",
      "{'$defs': {'Relationship': {'properties': {'startNode': {'default': 'The starting entity, which is present in the entities list.', 'title': 'Startnode', 'type': 'string'}, 'endNode': {'default': 'The ending entity, which is present in the entities list.', 'title': 'Endnode', 'type': 'string'}, 'name': {'default': 'The predicate that defines the relationship between the two entities. This predicate should represent a single, semantically distinct relation.', 'title': 'Name', 'type': 'string'}}, 'title': 'Relationship', 'type': 'object'}}, 'properties': {'relationships': {'default': 'Based on the provided entities and context, identify the predicates that define relationships between these entities. The predicates should be chosen with precision to accurately reflect the expressed relationships.', 'items': {'$ref': '#/$defs/Relationship'}, 'title': 'Relationships', 'type': 'array'}}, 'relationships': [{'startNode': 'The title of the scientific article', 'endNode': 'Title', 'name': 'Is a title'}, {'startNode': \"The list of the article's authors and their affiliation\", 'endNode': 'Authors', 'name': 'Are authors'}, {'startNode': \"The article's abstract\", 'endNode': 'Abstract', 'name': 'Is an abstract'}, {'startNode': 'The key findings of the article', 'endNode': 'Key Findings', 'name': 'Are key findings'}, {'startNode': 'Limitation of the existing work', 'endNode': 'Limitation Of Sota', 'name': 'Is a limitation'}, {'startNode': 'The proposed solution in details', 'endNode': 'Proposed Solution', 'name': 'Is a proposed solution'}, {'startNode': 'The limitations of the proposed solution of the paper', 'endNode': 'Paper Limitations', 'name': 'Are limitations'}]}\n",
      "[INFO] The isolated entities are  [{'label': 'The title of the scientific article', 'name': 'title', 'properties': {'embeddings': array([ 0.00038817, -0.01537237,  0.02617165, ...,  0.00610176,\n",
      "        0.01082225,  0.00830292])}}, {'label': 'Authors', 'name': 'list of authors and their affiliation', 'properties': {'embeddings': array([-0.01012932, -0.02165926,  0.01579848, ...,  0.01217581,\n",
      "       -0.00960065, -0.00984145])}}, {'label': 'Abstract', 'name': \"article's abstract\", 'properties': {'embeddings': array([ 4.7690207e-03, -2.5707543e-02,  9.6826180e-05, ...,\n",
      "        1.3489442e-02,  3.4747417e-03,  3.1695124e-03])}}, {'label': 'Key Findings', 'name': 'the key findings of the article', 'properties': {'embeddings': array([ 0.01059615, -0.0155891 ,  0.00875205, ...,  0.01409425,\n",
      "       -0.00611884, -0.01051767])}}, {'label': 'Limitation Of Sota', 'name': 'limitation of the existing work', 'properties': {'embeddings': array([ 0.00555676, -0.00446899,  0.01147087, ...,  0.00308466,\n",
      "       -0.01393188, -0.00958561])}}, {'label': 'Proposed Solution', 'name': 'the proposed solution in details', 'properties': {'embeddings': array([-0.01100891, -0.01865113,  0.02470583, ..., -0.00596826,\n",
      "        0.0029286 , -0.00465411])}}, {'label': 'Paper Limitations', 'name': 'the limitations of the proposed solution of the paper', 'properties': {'embeddings': array([ 0.01487406, -0.00763439,  0.02448528, ...,  0.0037551 ,\n",
      "        0.0020268 , -0.01794446])}}]\n",
      "Some isolated entities without relations were detected ... trying to solve them!\n",
      "{'$defs': {'Relationship': {'properties': {'startNode': {'default': 'The starting entity, which is present in the entities list.', 'title': 'Startnode', 'type': 'string'}, 'endNode': {'default': 'The ending entity, which is present in the entities list.', 'title': 'Endnode', 'type': 'string'}, 'name': {'default': 'The predicate that defines the relationship between the two entities. This predicate should represent a single, semantically distinct relation.', 'title': 'Name', 'type': 'string'}}, 'title': 'Relationship', 'type': 'object'}}, 'properties': {'relationships': {'default': 'Based on the provided entities and context, identify the predicates that define relationships between these entities. The predicates should be chosen with precision to accurately reflect the expressed relationships.', 'items': {'$ref': '#/$defs/Relationship'}, 'title': 'Relationships', 'type': 'array'}}, 'relationships': [{'startNode': 'The title of the scientific article', 'endNode': 'Title', 'name': 'Is a title'}, {'startNode': \"The list of the article's authors and their affiliation\", 'endNode': 'Authors', 'name': 'Are authors'}, {'startNode': \"The article's abstract\", 'endNode': 'Abstract', 'name': 'Is an abstract'}, {'startNode': 'The key findings of the article', 'endNode': 'Key Findings', 'name': 'Are key findings'}, {'startNode': 'Limitation of the existing work', 'endNode': 'Limitation Of Sota', 'name': 'Is a limitation'}, {'startNode': 'The proposed solution in details', 'endNode': 'Proposed Solution', 'name': 'Is a proposed solution'}, {'startNode': 'The limitations of the proposed solution of the paper', 'endNode': 'Paper Limitations', 'name': 'Are limitations'}]}\n",
      "[INFO] The isolated entities are  [{'label': 'The title of the scientific article', 'name': 'title', 'properties': {'embeddings': array([ 0.00038817, -0.01537237,  0.02617165, ...,  0.00610176,\n",
      "        0.01082225,  0.00830292])}}, {'label': 'Authors', 'name': 'list of authors and their affiliation', 'properties': {'embeddings': array([-0.01012932, -0.02165926,  0.01579848, ...,  0.01217581,\n",
      "       -0.00960065, -0.00984145])}}, {'label': 'Abstract', 'name': \"article's abstract\", 'properties': {'embeddings': array([ 4.7690207e-03, -2.5707543e-02,  9.6826180e-05, ...,\n",
      "        1.3489442e-02,  3.4747417e-03,  3.1695124e-03])}}, {'label': 'Key Findings', 'name': 'the key findings of the article', 'properties': {'embeddings': array([ 0.01059615, -0.0155891 ,  0.00875205, ...,  0.01409425,\n",
      "       -0.00611884, -0.01051767])}}, {'label': 'Limitation Of Sota', 'name': 'limitation of the existing work', 'properties': {'embeddings': array([ 0.00555676, -0.00446899,  0.01147087, ...,  0.00308466,\n",
      "       -0.01393188, -0.00958561])}}, {'label': 'Proposed Solution', 'name': 'the proposed solution in details', 'properties': {'embeddings': array([-0.01100891, -0.01865113,  0.02470583, ..., -0.00596826,\n",
      "        0.0029286 , -0.00465411])}}, {'label': 'Paper Limitations', 'name': 'the limitations of the proposed solution of the paper', 'properties': {'embeddings': array([ 0.01487406, -0.00763439,  0.02448528, ...,  0.0037551 ,\n",
      "        0.0020268 , -0.01794446])}}]\n",
      "Some isolated entities without relations were detected ... trying to solve them!\n",
      "{'$defs': {'Relationship': {'properties': {'startNode': {'default': 'The starting entity, which is present in the entities list.', 'title': 'Startnode', 'type': 'string'}, 'endNode': {'default': 'The ending entity, which is present in the entities list.', 'title': 'Endnode', 'type': 'string'}, 'name': {'default': 'The predicate that defines the relationship between the two entities. This predicate should represent a single, semantically distinct relation.', 'title': 'Name', 'type': 'string'}}, 'title': 'Relationship', 'type': 'object'}}, 'properties': {'relationships': {'default': 'Based on the provided entities and context, identify the predicates that define relationships between these entities. The predicates should be chosen with precision to accurately reflect the expressed relationships.', 'items': {'$ref': '#/$defs/Relationship'}, 'title': 'Relationships', 'type': 'array'}}, 'relationships': [{'startNode': 'The title of the scientific article', 'endNode': 'Title', 'name': 'Is a title'}, {'startNode': \"The list of the article's authors and their affiliation\", 'endNode': 'Authors', 'name': 'Are authors'}, {'startNode': \"The article's abstract\", 'endNode': 'Abstract', 'name': 'Is an abstract'}, {'startNode': 'The key findings of the article', 'endNode': 'Key Findings', 'name': 'Are key findings'}, {'startNode': 'Limitation of the existing work', 'endNode': 'Limitation Of Sota', 'name': 'Is a limitation'}, {'startNode': 'The proposed solution in details', 'endNode': 'Proposed Solution', 'name': 'Is a proposed solution'}, {'startNode': 'The limitations of the proposed solution of the paper', 'endNode': 'Paper Limitations', 'name': 'Are limitations'}]}\n",
      "[INFO] The isolated entities are  [{'label': 'The title of the scientific article', 'name': 'title', 'properties': {'embeddings': array([ 0.00038817, -0.01537237,  0.02617165, ...,  0.00610176,\n",
      "        0.01082225,  0.00830292])}}, {'label': 'Authors', 'name': 'list of authors and their affiliation', 'properties': {'embeddings': array([-0.01012932, -0.02165926,  0.01579848, ...,  0.01217581,\n",
      "       -0.00960065, -0.00984145])}}, {'label': 'Abstract', 'name': \"article's abstract\", 'properties': {'embeddings': array([ 4.7690207e-03, -2.5707543e-02,  9.6826180e-05, ...,\n",
      "        1.3489442e-02,  3.4747417e-03,  3.1695124e-03])}}, {'label': 'Key Findings', 'name': 'the key findings of the article', 'properties': {'embeddings': array([ 0.01059615, -0.0155891 ,  0.00875205, ...,  0.01409425,\n",
      "       -0.00611884, -0.01051767])}}, {'label': 'Limitation Of Sota', 'name': 'limitation of the existing work', 'properties': {'embeddings': array([ 0.00555676, -0.00446899,  0.01147087, ...,  0.00308466,\n",
      "       -0.01393188, -0.00958561])}}, {'label': 'Proposed Solution', 'name': 'the proposed solution in details', 'properties': {'embeddings': array([-0.01100891, -0.01865113,  0.02470583, ..., -0.00596826,\n",
      "        0.0029286 , -0.00465411])}}, {'label': 'Paper Limitations', 'name': 'the limitations of the proposed solution of the paper', 'properties': {'embeddings': array([ 0.01487406, -0.00763439,  0.02448528, ...,  0.0037551 ,\n",
      "        0.0020268 , -0.01794446])}}]\n",
      "Some isolated entities without relations were detected ... trying to solve them!\n",
      "{'$defs': {'Relationship': {'properties': {'startNode': {'default': 'The starting entity, which is present in the entities list.', 'title': 'Startnode', 'type': 'string'}, 'endNode': {'default': 'The ending entity, which is present in the entities list.', 'title': 'Endnode', 'type': 'string'}, 'name': {'default': 'The predicate that defines the relationship between the two entities. This predicate should represent a single, semantically distinct relation.', 'title': 'Name', 'type': 'string'}}, 'title': 'Relationship', 'type': 'object'}}, 'properties': {'relationships': {'default': 'Based on the provided entities and context, identify the predicates that define relationships between these entities. The predicates should be chosen with precision to accurately reflect the expressed relationships.', 'items': {'$ref': '#/$defs/Relationship'}, 'title': 'Relationships', 'type': 'array'}}, 'relationships': [{'startNode': 'The title of the scientific article', 'endNode': 'Title', 'name': 'Is a title'}, {'startNode': \"The list of the article's authors and their affiliation\", 'endNode': 'Authors', 'name': 'Are authors'}, {'startNode': \"The article's abstract\", 'endNode': 'Abstract', 'name': 'Is an abstract'}, {'startNode': 'The key findings of the article', 'endNode': 'Key Findings', 'name': 'Are key findings'}, {'startNode': 'Limitation of the existing work', 'endNode': 'Limitation Of Sota', 'name': 'Is a limitation'}, {'startNode': 'The proposed solution in details', 'endNode': 'Proposed Solution', 'name': 'Is a proposed solution'}, {'startNode': 'The limitations of the proposed solution of the paper', 'endNode': 'Paper Limitations', 'name': 'Are limitations'}]}\n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The title of the scientific article -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The list of authors and their affiliation -merged--> list of authors and their affiliation \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The list of authors and their affiliation -merged--> list of authors and their affiliation \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The article's abstract -merged--> article's abstract \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The article's abstract -merged--> article's abstract \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The key findings of the article -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The key findings of the article -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Limitation of the existing work -merged--> limitation of the existing work \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Limitation of the existing work -merged--> limitation of the existing work \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The proposed solution in details -merged--> the proposed solution in details \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The proposed solution in details -merged--> the proposed solution in details \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The limitations of the proposed solution of the paper -merged--> the limitations of the proposed solution of the paper \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The title of the scientific article -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The list of the article's authors and their affiliation -merged--> list of authors and their affiliation \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The article's abstract -merged--> article's abstract \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The key findings of the article -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Key Findings -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Limitation of the existing work -merged--> limitation of the existing work \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The proposed solution in details -merged--> the proposed solution in details \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Proposed Solution -merged--> the proposed solution in details \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The limitations of the proposed solution of the paper -merged--> the limitations of the proposed solution of the paper \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Paper Limitations -merged--> the limitations of the proposed solution of the paper \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The title of the scientific article -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The list of the article's authors and their affiliation -merged--> list of authors and their affiliation \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The article's abstract -merged--> article's abstract \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The key findings of the article -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Key Findings -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Limitation of the existing work -merged--> limitation of the existing work \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The proposed solution in details -merged--> the proposed solution in details \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Proposed Solution -merged--> the proposed solution in details \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The limitations of the proposed solution of the paper -merged--> the limitations of the proposed solution of the paper \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Paper Limitations -merged--> the limitations of the proposed solution of the paper \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The title of the scientific article -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The list of the article's authors and their affiliation -merged--> list of authors and their affiliation \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The article's abstract -merged--> article's abstract \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The key findings of the article -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Key Findings -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Limitation of the existing work -merged--> limitation of the existing work \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The proposed solution in details -merged--> the proposed solution in details \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Proposed Solution -merged--> the proposed solution in details \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The limitations of the proposed solution of the paper -merged--> the limitations of the proposed solution of the paper \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Paper Limitations -merged--> the limitations of the proposed solution of the paper \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The title of the scientific article -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The list of the article's authors and their affiliation -merged--> list of authors and their affiliation \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The article's abstract -merged--> article's abstract \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The key findings of the article -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Key Findings -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Limitation of the existing work -merged--> limitation of the existing work \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The proposed solution in details -merged--> the proposed solution in details \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Proposed Solution -merged--> the proposed solution in details \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The limitations of the proposed solution of the paper -merged--> the limitations of the proposed solution of the paper \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Paper Limitations -merged--> the limitations of the proposed solution of the paper \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The title of the scientific article -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The list of the article's authors and their affiliation -merged--> list of authors and their affiliation \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The article's abstract -merged--> article's abstract \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The key findings of the article -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Key Findings -merged--> the key findings of the article \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Limitation of the existing work -merged--> limitation of the existing work \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The proposed solution in details -merged--> the proposed solution in details \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Proposed Solution -merged--> the proposed solution in details \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- The limitations of the proposed solution of the paper -merged--> the limitations of the proposed solution of the paper \n",
      "[INFO] Wohoo ! Entity using embeddings is matched --- Paper Limitations -merged--> the limitations of the proposed solution of the paper \n",
      "[INFO] Extracting Entities from the Document 3\n",
      "{'$defs': {'Entity': {'properties': {'label': {'default': '', 'title': 'Label', 'type': 'string'}, 'name': {'default': '', 'title': 'Name', 'type': 'string'}}, 'title': 'Entity', 'type': 'object'}}, 'properties': {'entities': {'default': [], 'items': {'$ref': '#/$defs/Entity'}, 'title': 'Entities', 'type': 'array'}}, 'required': ['entities']}\n",
      "Not formatted in the desired format, we are retrying ....\n",
      "{'$defs': {'Entity': {'properties': {'label': {'default': '', 'title': 'Label', 'type': 'string'}, 'name': {'default': '', 'title': 'Name', 'type': 'string'}}, 'title': 'Entity', 'type': 'object'}}, 'properties': {'entities': {'default': [], 'items': {'$ref': '#/$defs/Entity'}, 'title': 'Entities', 'type': 'array'}}, 'required': ['entities']}\n",
      "Not formatted in the desired format, we are retrying ....\n",
      "{'$defs': {'Entity': {'properties': {'label': {'default': '', 'title': 'Label', 'type': 'string'}, 'name': {'default': '', 'title': 'Name', 'type': 'string'}}, 'title': 'Entity', 'type': 'object'}}, 'properties': {'entities': {'default': [], 'items': {'$ref': '#/$defs/Entity'}, 'title': 'Entities', 'type': 'array'}}, 'required': ['entities']}\n",
      "Not formatted in the desired format, we are retrying ....\n",
      "{'$defs': {'Entity': {'properties': {'label': {'default': '', 'title': 'Label', 'type': 'string'}, 'name': {'default': '', 'title': 'Name', 'type': 'string'}}, 'title': 'Entity', 'type': 'object'}}, 'properties': {'entities': {'default': [], 'items': {'$ref': '#/$defs/Entity'}, 'title': 'Entities', 'type': 'array'}}, 'required': ['entities']}\n",
      "Not formatted in the desired format, we are retrying ....\n",
      "{'$defs': {'Entity': {'properties': {'label': {'default': '', 'title': 'Label', 'type': 'string'}, 'name': {'default': '', 'title': 'Name', 'type': 'string'}}, 'title': 'Entity', 'type': 'object'}}, 'properties': {'entities': {'default': [], 'items': {'$ref': '#/$defs/Entity'}, 'title': 'Entities', 'type': 'array'}}, 'required': ['entities']}\n",
      "Not formatted in the desired format, we are retrying ....\n",
      "{'$defs': {'Entity': {'properties': {'label': {'default': '', 'title': 'Label', 'type': 'string'}, 'name': {'default': '', 'title': 'Name', 'type': 'string'}}, 'title': 'Entity', 'type': 'object'}}, 'properties': {'entities': {'default': [], 'items': {'$ref': '#/$defs/Entity'}, 'title': 'Entities', 'type': 'array'}}, 'required': ['entities']}\n",
      "Not formatted in the desired format, we are retrying ....\n",
      "{'$defs': {'Entity': {'properties': {'label': {'default': '', 'title': 'Label', 'type': 'string'}, 'name': {'default': '', 'title': 'Name', 'type': 'string'}}, 'title': 'Entity', 'type': 'object'}}, 'properties': {'entities': {'default': [], 'items': {'$ref': '#/$defs/Entity'}, 'title': 'Entities', 'type': 'array'}}, 'required': ['entities']}\n",
      "Not formatted in the desired format, we are retrying ....\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitext2kg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iText2KG\n\u001b[1;32m      4\u001b[0m itext2kg \u001b[38;5;241m=\u001b[39m iText2KG(llm_model \u001b[38;5;241m=\u001b[39m llm, embeddings_model \u001b[38;5;241m=\u001b[39m embeddings)\n\u001b[0;32m----> 5\u001b[0m global_ent, global_rel \u001b[38;5;241m=\u001b[39m \u001b[43mitext2kg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43msections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msemantic_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/WorkSpace/itext2kg/examples/../itext2kg/graph_integration/itext2kg.py:142\u001b[0m, in \u001b[0;36miText2KG.build_graph\u001b[0;34m(self, sections, existing_global_entities, existing_global_relationships, ent_threshold, rel_threshold, resolving_tries, entity_key)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sections)):\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Extracting Entities from the Document\u001b[39m\u001b[38;5;124m\"\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m     entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mientities_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msections\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentity_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     processed_entities, global_entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatcher\u001b[38;5;241m.\u001b[39mprocess_lists(list1 \u001b[38;5;241m=\u001b[39m entities, list2\u001b[38;5;241m=\u001b[39mglobal_entities, for_entity_or_relation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m, threshold\u001b[38;5;241m=\u001b[39ment_threshold)\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m#relationships = relationship_extraction(context= sections[i], entities=list(map(lambda w:w[\"name\"], processed_entities)))\u001b[39;00m\n",
      "File \u001b[0;32m~/WorkSpace/itext2kg/examples/../itext2kg/ientities_extraction/ientities_extractor.py:42\u001b[0m, in \u001b[0;36miEntitiesExtractor.extract_entities\u001b[0;34m(self, context, embeddings, entity_key, property_name, entity_name_key)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entities \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m entity_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m entities\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot formatted in the desired format, we are retrying ....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperty_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_name_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentity_name_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_handler\u001b[38;5;241m.\u001b[39madd_embeddings_as_property_batch(embeddings_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangchain_output_parser\u001b[38;5;241m.\u001b[39mcalculate_embeddings(x), \n\u001b[1;32m     45\u001b[0m                                                           items\u001b[38;5;241m=\u001b[39mentities[entity_key],\n\u001b[1;32m     46\u001b[0m                                                           property_name\u001b[38;5;241m=\u001b[39mproperty_name,\n\u001b[1;32m     47\u001b[0m                                                           item_name_key\u001b[38;5;241m=\u001b[39mentity_name_key,\n\u001b[1;32m     48\u001b[0m                                                           embeddings\u001b[38;5;241m=\u001b[39membeddings)\n",
      "File \u001b[0;32m~/WorkSpace/itext2kg/examples/../itext2kg/ientities_extraction/ientities_extractor.py:42\u001b[0m, in \u001b[0;36miEntitiesExtractor.extract_entities\u001b[0;34m(self, context, embeddings, entity_key, property_name, entity_name_key)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entities \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m entity_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m entities\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot formatted in the desired format, we are retrying ....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperty_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_name_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentity_name_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_handler\u001b[38;5;241m.\u001b[39madd_embeddings_as_property_batch(embeddings_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangchain_output_parser\u001b[38;5;241m.\u001b[39mcalculate_embeddings(x), \n\u001b[1;32m     45\u001b[0m                                                           items\u001b[38;5;241m=\u001b[39mentities[entity_key],\n\u001b[1;32m     46\u001b[0m                                                           property_name\u001b[38;5;241m=\u001b[39mproperty_name,\n\u001b[1;32m     47\u001b[0m                                                           item_name_key\u001b[38;5;241m=\u001b[39mentity_name_key,\n\u001b[1;32m     48\u001b[0m                                                           embeddings\u001b[38;5;241m=\u001b[39membeddings)\n",
      "    \u001b[0;31m[... skipping similar frames: iEntitiesExtractor.extract_entities at line 42 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/WorkSpace/itext2kg/examples/../itext2kg/ientities_extraction/ientities_extractor.py:42\u001b[0m, in \u001b[0;36miEntitiesExtractor.extract_entities\u001b[0;34m(self, context, embeddings, entity_key, property_name, entity_name_key)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entities \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m entity_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m entities\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot formatted in the desired format, we are retrying ....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperty_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_name_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentity_name_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_handler\u001b[38;5;241m.\u001b[39madd_embeddings_as_property_batch(embeddings_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangchain_output_parser\u001b[38;5;241m.\u001b[39mcalculate_embeddings(x), \n\u001b[1;32m     45\u001b[0m                                                           items\u001b[38;5;241m=\u001b[39mentities[entity_key],\n\u001b[1;32m     46\u001b[0m                                                           property_name\u001b[38;5;241m=\u001b[39mproperty_name,\n\u001b[1;32m     47\u001b[0m                                                           item_name_key\u001b[38;5;241m=\u001b[39mentity_name_key,\n\u001b[1;32m     48\u001b[0m                                                           embeddings\u001b[38;5;241m=\u001b[39membeddings)\n",
      "File \u001b[0;32m~/WorkSpace/itext2kg/examples/../itext2kg/ientities_extraction/ientities_extractor.py:36\u001b[0m, in \u001b[0;36miEntitiesExtractor.extract_entities\u001b[0;34m(self, context, embeddings, entity_key, property_name, entity_name_key)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_entities\u001b[39m(\u001b[38;5;28mself\u001b[39m, context: \u001b[38;5;28mstr\u001b[39m, embeddings: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, entity_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m, property_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m, entity_name_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    Extract entities from a given context and optionally add embeddings to each.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    List[dict]: A list of extracted entities with optional embeddings.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlangchain_output_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_information_as_json_for_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_data_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEntitiesExtractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(entities)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m entities \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m entity_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m entities\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/WorkSpace/itext2kg/examples/../itext2kg/utils/llm_output_parser.py:94\u001b[0m, in \u001b[0;36mLangchainOutputParser.extract_information_as_json_for_context\u001b[0;34m(self, output_data_structure, context, IE_query)\u001b[0m\n\u001b[1;32m     92\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m|\u001b[39m parser\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mIE_query\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo much requests, we are sleeping! \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m the error is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/langchain_core/runnables/base.py:3013\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3011\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3012\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3013\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3014\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3015\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:855\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/langchain_ollama/chat_models.py:644\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    639\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 644\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    648\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    649\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    650\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    655\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/langchain_ollama/chat_models.py:545\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    544\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mChatGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAIMessageChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/langchain_ollama/chat_models.py:527\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    518\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    519\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         tools\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    528\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    529\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[1;32m    530\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    531\u001b[0m         options\u001b[38;5;241m=\u001b[39mOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    532\u001b[0m         keep_alive\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    534\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/ollama/_client.py:87\u001b[0m, in \u001b[0;36mClient._stream\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     85\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m:=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/httpx/_models.py:863\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    861\u001b[0m decoder \u001b[38;5;241m=\u001b[39m LineDecoder()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 863\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/httpx/_models.py:850\u001b[0m, in \u001b[0;36mResponse.iter_text\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    848\u001b[0m chunker \u001b[38;5;241m=\u001b[39m TextChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/httpx/_models.py:831\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    829\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 831\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/httpx/_models.py:885\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    882\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 885\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/httpx/_client.py:127\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m--> 127\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/httpx/_transports/default.py:116\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:367\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:363\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/httpcore/_sync/http11.py:349\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/httpcore/_sync/http11.py:341\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 341\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/httpcore/_sync/http11.py:210\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    207\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/text/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itext2kg import iText2KG\n",
    "\n",
    "\n",
    "itext2kg = iText2KG(llm_model = llm, embeddings_model = embeddings)\n",
    "global_ent, global_rel = itext2kg.build_graph(sections=semantic_blocks, entity_key=\"entities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw the graph\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final section involves visualizing the constructed knowledge graph using GraphIntegrator. The graph database Neo4j is accessed using specified credentials, and the resulting graph is visualized to provide a visual representation of the relationships and entities extracted from the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run docker run -p7474:7474 -p7687:7687 -e NEO4J_AUTH=neo4j/secretgraph neo4j:latest in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_ent_list = []\n",
    "for ent in global_ent:\n",
    "    if ent[\"label\"] != '':\n",
    "        global_ent_list.append(ent)\n",
    "\n",
    "global_ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_rel_list = []\n",
    "for rel in global_rel:\n",
    "    if rel[\"startNode\"] != '' and rel[\"endNode\"] != '':\n",
    "        global_rel_list.append(rel)\n",
    "\n",
    "global_rel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itext2kg.graph_integration import GraphIntegrator\n",
    "\n",
    "\n",
    "URI = \"bolt://localhost:7687\"\n",
    "USERNAME = \"neo4j\"\n",
    "PASSWORD = \"secretgraph\"\n",
    "\n",
    "new_graph = {}\n",
    "new_graph[\"nodes\"] = global_ent_list\n",
    "new_graph[\"relationships\"] = global_rel_list\n",
    "GraphIntegrator(uri=URI, username=USERNAME, password=PASSWORD).visualize_graph(json_graph=new_graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
